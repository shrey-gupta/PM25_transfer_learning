{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repositories uploaded!!\n"
     ]
    }
   ],
   "source": [
    "# from two_TrAdaBoostR2 import TwoStageTrAdaBoostR2 ##STrAdaBoost.R2\n",
    "# from TwoStageTrAdaBoostR2 import TwoStageTrAdaBoostR2 ##two-stage TrAdaBoost.R2\n",
    "\n",
    "import pandas as pd\n",
    "import sys\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from keras.layers import Input, Dense, Activation, Conv2D, Dropout, Flatten\n",
    "from keras import optimizers, utils, initializers, regularizers\n",
    "import keras.backend as K\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import StandardScaler #Importing the StandardScaler\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.stats.stats import pearsonr\n",
    "from math import sqrt\n",
    "\n",
    "#Geo plotting libraries\n",
    "import geopandas as gdp\n",
    "from matplotlib.colors import ListedColormap\n",
    "# import geoplot as glpt\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn import linear_model\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.lines as mlines\n",
    "\n",
    "import statistics\n",
    "\n",
    "from scipy.stats import rv_continuous\n",
    "from scipy.stats import *\n",
    "\n",
    "from statistics import mean\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial import distance\n",
    "\n",
    "\n",
    "######### Instance Transfer repositories ####################\n",
    "from adapt.instance_based import TwoStageTrAdaBoostR2\n",
    "\n",
    "print(\"Repositories uploaded!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second Upload Completed!!\n"
     ]
    }
   ],
   "source": [
    "from adapt.instance_based import TrAdaBoost, TrAdaBoostR2, TwoStageTrAdaBoostR2\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from adapt.instance_based import KMM\n",
    "\n",
    "print(\"Second Upload Completed!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Importing libraries for map plots #######\n",
    "\n",
    "import geopandas as gpd\n",
    "import json\n",
    "import altair as alt\n",
    "import pandas as pd\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import glob\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib import cm\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Reading the US 2018 dataset\n",
    "US_new_source = pd.read_csv('US_data/new_US_dataset/source_test.csv')\n",
    "US_new_target = pd.read_csv('US_data/new_US_dataset/target_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'Date', 'PM25', 'Region', 'CF_CLDTT', 'CF_PS', 'CF_Q', 'CF_Q10M',\n",
       "       'CF_Q2M', 'CF_RH', 'CF_SLP', 'CF_T', 'CF_T10M', 'CF_T2M', 'CF_TPREC',\n",
       "       'CF_TROPPB', 'CF_TS', 'CF_U', 'CF_U10M', 'CF_U2M', 'CF_V', 'CF_V10M',\n",
       "       'CF_V2M', 'CF_ZL', 'CF_ZPBL', 'CF_PM25', 'CF_CO', 'CF_NO2', 'CF_O3',\n",
       "       'CF_SO2', 'LC_Shrubs', 'LC_Herbaceous', 'LC_Agriculture', 'LC_Urban',\n",
       "       'LC_Bare', 'LC_Snow', 'LC_Water', 'LC_Wetland', 'LC_Lichen',\n",
       "       'LC_Closed_Forest', 'LC_Open_Forest', 'LC_Ocean', 'Elevation',\n",
       "       'Dist_Primary', 'Dist_Secondary', 'Pop', 'Lon', 'Lat'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "US_new_target.shape, US_new_source.shape \n",
    "US_new_target.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CF_CLDTT', 'CF_PS', 'CF_Q', 'CF_Q10M', 'CF_Q2M', 'CF_RH', 'CF_SLP',\n",
       "       'CF_T', 'CF_T10M', 'CF_T2M', 'CF_TPREC', 'CF_TROPPB', 'CF_TS', 'CF_U',\n",
       "       'CF_U10M', 'CF_U2M', 'CF_V', 'CF_V10M', 'CF_V2M', 'CF_ZL', 'CF_ZPBL',\n",
       "       'CF_PM25', 'CF_CO', 'CF_NO2', 'CF_O3', 'CF_SO2', 'LC_Shrubs',\n",
       "       'LC_Herbaceous', 'LC_Agriculture', 'LC_Urban', 'LC_Bare', 'LC_Snow',\n",
       "       'LC_Water', 'LC_Wetland', 'LC_Lichen', 'LC_Closed_Forest',\n",
       "       'LC_Open_Forest', 'LC_Ocean', 'Elevation', 'Dist_Primary',\n",
       "       'Dist_Secondary', 'Pop'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######## Split into features and output ########\n",
    "output_US = ['PM25']\n",
    "\n",
    "US_y_source_df = US_new_source['PM25']\n",
    "US_X_source_df = US_new_source.drop(output_US, axis = 1)\n",
    "\n",
    "US_y_target_df = US_new_target['PM25']\n",
    "US_X_target_df = US_new_target.drop(output_US, axis = 1)\n",
    "\n",
    "\n",
    "######## Drop the unwanted features ########\n",
    "drop_features = ['ID', 'Date', 'Region', 'Lon', 'Lat']\n",
    "\n",
    "US_X_source_df = US_X_source_df.drop(drop_features, axis = 1)\n",
    "US_X_target_df = US_X_target_df.drop(drop_features, axis = 1)\n",
    "\n",
    "US_X_source_df.shape, US_X_target_df.shape\n",
    "US_X_source_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Normalize the dataset ########\n",
    "from sklearn import preprocessing\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "X_target = US_X_target_df.values \n",
    "X_target_scaled = min_max_scaler.fit_transform(X_target)\n",
    "US_X_target_df = pd.DataFrame(X_target_scaled)\n",
    "\n",
    "X_source = US_X_source_df.values \n",
    "X_source_scaled = min_max_scaler.fit_transform(X_source)\n",
    "US_X_source_df = pd.DataFrame(X_source_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.272812e-01</td>\n",
       "      <td>0.946823</td>\n",
       "      <td>0.103185</td>\n",
       "      <td>0.098439</td>\n",
       "      <td>0.096783</td>\n",
       "      <td>0.075007</td>\n",
       "      <td>0.723671</td>\n",
       "      <td>0.617497</td>\n",
       "      <td>0.607640</td>\n",
       "      <td>0.595801</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.296381</td>\n",
       "      <td>0.292476</td>\n",
       "      <td>0.286463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.206282e-01</td>\n",
       "      <td>0.946948</td>\n",
       "      <td>0.271847</td>\n",
       "      <td>0.262674</td>\n",
       "      <td>0.256929</td>\n",
       "      <td>0.198366</td>\n",
       "      <td>0.725258</td>\n",
       "      <td>0.626451</td>\n",
       "      <td>0.619452</td>\n",
       "      <td>0.611232</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.296381</td>\n",
       "      <td>0.292476</td>\n",
       "      <td>0.286463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.077174e-01</td>\n",
       "      <td>0.893265</td>\n",
       "      <td>0.441761</td>\n",
       "      <td>0.436748</td>\n",
       "      <td>0.427883</td>\n",
       "      <td>0.444067</td>\n",
       "      <td>0.434259</td>\n",
       "      <td>0.540808</td>\n",
       "      <td>0.536332</td>\n",
       "      <td>0.527199</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.296381</td>\n",
       "      <td>0.292476</td>\n",
       "      <td>0.286463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.961028</td>\n",
       "      <td>0.376618</td>\n",
       "      <td>0.372304</td>\n",
       "      <td>0.364236</td>\n",
       "      <td>0.369088</td>\n",
       "      <td>0.802700</td>\n",
       "      <td>0.569805</td>\n",
       "      <td>0.566485</td>\n",
       "      <td>0.562533</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.296381</td>\n",
       "      <td>0.292476</td>\n",
       "      <td>0.286463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.521268e-01</td>\n",
       "      <td>0.955045</td>\n",
       "      <td>0.194071</td>\n",
       "      <td>0.190295</td>\n",
       "      <td>0.187262</td>\n",
       "      <td>0.182382</td>\n",
       "      <td>0.770261</td>\n",
       "      <td>0.570736</td>\n",
       "      <td>0.563269</td>\n",
       "      <td>0.553933</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.296381</td>\n",
       "      <td>0.292476</td>\n",
       "      <td>0.286463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10215</th>\n",
       "      <td>3.241445e-08</td>\n",
       "      <td>0.227873</td>\n",
       "      <td>0.449790</td>\n",
       "      <td>0.458414</td>\n",
       "      <td>0.463797</td>\n",
       "      <td>0.295111</td>\n",
       "      <td>0.544184</td>\n",
       "      <td>0.623240</td>\n",
       "      <td>0.630562</td>\n",
       "      <td>0.637230</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.907152</td>\n",
       "      <td>0.103218</td>\n",
       "      <td>0.093607</td>\n",
       "      <td>0.000554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10216</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.221229</td>\n",
       "      <td>0.184650</td>\n",
       "      <td>0.188425</td>\n",
       "      <td>0.193010</td>\n",
       "      <td>0.241111</td>\n",
       "      <td>0.581110</td>\n",
       "      <td>0.430094</td>\n",
       "      <td>0.442945</td>\n",
       "      <td>0.457304</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.907152</td>\n",
       "      <td>0.103218</td>\n",
       "      <td>0.093607</td>\n",
       "      <td>0.000554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10217</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.234674</td>\n",
       "      <td>0.249304</td>\n",
       "      <td>0.255928</td>\n",
       "      <td>0.262718</td>\n",
       "      <td>0.201952</td>\n",
       "      <td>0.616677</td>\n",
       "      <td>0.558672</td>\n",
       "      <td>0.567174</td>\n",
       "      <td>0.576144</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.907152</td>\n",
       "      <td>0.103218</td>\n",
       "      <td>0.093607</td>\n",
       "      <td>0.000554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10218</th>\n",
       "      <td>2.769444e-02</td>\n",
       "      <td>0.199898</td>\n",
       "      <td>0.293804</td>\n",
       "      <td>0.296983</td>\n",
       "      <td>0.299266</td>\n",
       "      <td>0.319077</td>\n",
       "      <td>0.485674</td>\n",
       "      <td>0.471915</td>\n",
       "      <td>0.484383</td>\n",
       "      <td>0.495415</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.907152</td>\n",
       "      <td>0.103218</td>\n",
       "      <td>0.093607</td>\n",
       "      <td>0.000554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10219</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.229526</td>\n",
       "      <td>0.232174</td>\n",
       "      <td>0.236804</td>\n",
       "      <td>0.242707</td>\n",
       "      <td>0.195863</td>\n",
       "      <td>0.628753</td>\n",
       "      <td>0.546372</td>\n",
       "      <td>0.556716</td>\n",
       "      <td>0.566964</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.907152</td>\n",
       "      <td>0.103218</td>\n",
       "      <td>0.093607</td>\n",
       "      <td>0.000554</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10220 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0         1         2         3         4         5   \\\n",
       "0      8.272812e-01  0.946823  0.103185  0.098439  0.096783  0.075007   \n",
       "1      5.206282e-01  0.946948  0.271847  0.262674  0.256929  0.198366   \n",
       "2      6.077174e-01  0.893265  0.441761  0.436748  0.427883  0.444067   \n",
       "3      0.000000e+00  0.961028  0.376618  0.372304  0.364236  0.369088   \n",
       "4      9.521268e-01  0.955045  0.194071  0.190295  0.187262  0.182382   \n",
       "...             ...       ...       ...       ...       ...       ...   \n",
       "10215  3.241445e-08  0.227873  0.449790  0.458414  0.463797  0.295111   \n",
       "10216  0.000000e+00  0.221229  0.184650  0.188425  0.193010  0.241111   \n",
       "10217  0.000000e+00  0.234674  0.249304  0.255928  0.262718  0.201952   \n",
       "10218  2.769444e-02  0.199898  0.293804  0.296983  0.299266  0.319077   \n",
       "10219  0.000000e+00  0.229526  0.232174  0.236804  0.242707  0.195863   \n",
       "\n",
       "             6         7         8         9   ...   32   33   34   35   36  \\\n",
       "0      0.723671  0.617497  0.607640  0.595801  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "1      0.725258  0.626451  0.619452  0.611232  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "2      0.434259  0.540808  0.536332  0.527199  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "3      0.802700  0.569805  0.566485  0.562533  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "4      0.770261  0.570736  0.563269  0.553933  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "...         ...       ...       ...       ...  ...  ...  ...  ...  ...  ...   \n",
       "10215  0.544184  0.623240  0.630562  0.637230  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "10216  0.581110  0.430094  0.442945  0.457304  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "10217  0.616677  0.558672  0.567174  0.576144  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "10218  0.485674  0.471915  0.484383  0.495415  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "10219  0.628753  0.546372  0.556716  0.566964  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "        37        38        39        40        41  \n",
       "0      0.0  0.000000  0.296381  0.292476  0.286463  \n",
       "1      0.0  0.000000  0.296381  0.292476  0.286463  \n",
       "2      0.0  0.000000  0.296381  0.292476  0.286463  \n",
       "3      0.0  0.000000  0.296381  0.292476  0.286463  \n",
       "4      0.0  0.000000  0.296381  0.292476  0.286463  \n",
       "...    ...       ...       ...       ...       ...  \n",
       "10215  0.0  0.907152  0.103218  0.093607  0.000554  \n",
       "10216  0.0  0.907152  0.103218  0.093607  0.000554  \n",
       "10217  0.0  0.907152  0.103218  0.093607  0.000554  \n",
       "10218  0.0  0.907152  0.103218  0.093607  0.000554  \n",
       "10219  0.0  0.907152  0.103218  0.093607  0.000554  \n",
       "\n",
       "[10220 rows x 42 columns]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "US_X_target_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------\n"
     ]
    }
   ],
   "source": [
    "######## Split into Train-Test ########\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "US_X_target_df, US_X_test_df, US_y_target_df, US_y_test_df = train_test_split(US_X_target_df, US_y_target_df, test_size = 0.30)\n",
    "\n",
    "######## Merging the datasets ########\n",
    "US_X_df = pd.concat([US_X_target_df, US_X_source_df], ignore_index=True)\n",
    "US_y_df = pd.concat([US_y_target_df, US_y_source_df], ignore_index=True)\n",
    "\n",
    "US_np_train_X = US_X_df.to_numpy()\n",
    "US_np_train_y = US_y_df.to_numpy()\n",
    "\n",
    "US_np_test_X = US_X_test_df.to_numpy()\n",
    "US_np_test_y = US_y_test_df.to_numpy()\n",
    "\n",
    "US_np_train_y_list = US_np_train_y.ravel()\n",
    "US_np_test_y_list = US_np_test_y.ravel()\n",
    "\n",
    "src_size_US = len(US_y_source_df)\n",
    "tgt_size_US = len(US_y_target_df)\n",
    "\n",
    "print(\"---------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elastic Net Regression Transfer Learning\n",
      "-------------------------------------------\n",
      "Mean RMSE of Elastic N: 11.459042742133393\n",
      "Mean R-squared of Elastic N: 0.5263031480122707\n",
      "\n",
      "\n",
      "RMSE of Elastic N: [11.459042742133393, 11.459042742133393, 11.459042742133393, 11.459042742133393, 11.459042742133393, 11.459042742133393, 11.459042742133393, 11.459042742133393, 11.459042742133393, 11.459042742133393]\n",
      "R-squared of Elastic N: [0.5263031480122707, 0.5263031480122707, 0.5263031480122707, 0.5263031480122707, 0.5263031480122707, 0.5263031480122707, 0.5263031480122707, 0.5263031480122707, 0.5263031480122707, 0.5263031480122707]\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "####################### Elastic Net Regression #########################################################\n",
    "\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "print(\"Elastic Net Regression Transfer Learning\")\n",
    "print(\"-------------------------------------------\")\n",
    "\n",
    "r2scorelist_ENTL_us = []\n",
    "rmselist_ENTL_us = []\n",
    "\n",
    "for x in range(0, 10):\n",
    "\n",
    "    model_ENTL_us = ElasticNet(normalize = True, alpha = 0.01, l1_ratio = 0.75)\n",
    "    model_ENTL_us.fit(US_np_train_X, US_np_train_y_list)\n",
    "\n",
    "\n",
    "    y_pred_ENTL_us = model_ENTL_us.predict(US_np_test_X) \n",
    "\n",
    "    rmse_ENTL_us = sqrt(mean_squared_error(US_np_test_y_list, y_pred_ENTL_us))\n",
    "    rmselist_ENTL_us.append(rmse_ENTL_us)\n",
    "    \n",
    "    r2_score_ENTL_us = pearsonr(US_np_test_y_list, y_pred_ENTL_us)\n",
    "    r2_score_ENTL_us = (r2_score_ENTL_us[0])**2\n",
    "    r2scorelist_ENTL_us.append(r2_score_ENTL_us)\n",
    "    \n",
    "print(\"Mean RMSE of Elastic N:\", statistics.mean(rmselist_ENTL_us))\n",
    "print(\"Mean R-squared of Elastic N:\", statistics.mean(r2scorelist_ENTL_us))\n",
    "print(\"\\n\")\n",
    "print(\"RMSE of Elastic N:\", rmselist_ENTL_us)\n",
    "print(\"R-squared of Elastic N:\", r2scorelist_ENTL_us)\n",
    "\n",
    "print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Regression\n",
      "-------------------------------------------\n",
      "Mean RMSE of GBR: 6.1237041643921994\n",
      "Mean R-squared of GBR: 0.691797141193723\n",
      "\n",
      "\n",
      "RMSE of GBR: [6.242791736154706, 5.967406971288434, 6.103326661391169, 6.07966037305247, 6.4169278145635475, 6.0571821464155065, 5.9666186528394425, 5.980383869431792, 6.331659419282624, 6.0910839995023]\n",
      "R^2 of GBR: [0.6803614576376373, 0.7066887760393229, 0.69999203769556, 0.6945941560052241, 0.659883312133618, 0.6985052523902867, 0.7057963218332743, 0.7047052579294206, 0.6746246477627736, 0.6928201925101125]\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "####################### GBR Transfer Learning #########################################################\n",
    "print(\"Gradient Boosting Regression Transfer Learning\")\n",
    "print(\"-------------------------------------------\")\n",
    "\n",
    "r2scorelist_GBRTL_us = []\n",
    "rmselist_GBRTL_us = []\n",
    "\n",
    "for x in range(0, 10):\n",
    "\n",
    "    model_GBRTL_us = GradientBoostingRegressor(learning_rate = 0.1, max_depth = 6, n_estimators = 100, subsample=0.5)\n",
    "    model_GBRTL_us.fit(US_np_train_X, US_np_train_y_list)\n",
    "\n",
    "\n",
    "    y_pred_GBRTL_us = model_GBRTL_us.predict(US_np_test_X) \n",
    "\n",
    "    rmse_GBRTL_us = sqrt(mean_squared_error(US_np_test_y_list, y_pred_GBRTL_us))\n",
    "    rmselist_GBRTL_us.append(rmse_GBRTL_us)\n",
    "    \n",
    "    r2_score_GBRTL_us = pearsonr(US_np_test_y_list, y_pred_GBRTL_us)\n",
    "    r2_score_GBRTL_us = (r2_score_GBRTL_us[0])**2\n",
    "    r2scorelist_GBRTL_us.append(r2_score_GBRTL_us)\n",
    "    \n",
    "print(\"Mean RMSE of GBR:\", statistics.mean(rmselist_GBRTL_us))\n",
    "print(\"Mean R-squared of GBR:\", statistics.mean(r2scorelist_GBRTL_us))\n",
    "print(\"\\n\")\n",
    "print(\"RMSE of GBR:\", rmselist_GBRTL_us)\n",
    "print(\"R^2 of GBR:\", r2scorelist_GBRTL_us)\n",
    "\n",
    "print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaboost.R2 Transfer Learning\n",
      "-------------------------------------------\n",
      "Mean RMSE of Adaboost.R2: 8.243143411630145\n",
      "Mean R-squared of AdaboostR2: 0.39924065062108927\n",
      "\n",
      "\n",
      "RMSE of Adaboost.R2: [8.232257146636876, 8.25346584333486, 8.21037591599147, 8.278125021853175, 8.272184512267058, 8.263867952900434, 8.22618438442368, 8.196550708025356, 8.257250780880515, 8.241171849988028]\n",
      "R^2 of AdaboostR2: [0.4001945712140022, 0.3977397596397475, 0.40406787813022266, 0.39390530366111676, 0.394781601034896, 0.3955751039758742, 0.4028969521057108, 0.4060709892725128, 0.3975578204080337, 0.3996165267687761]\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "########################### AdaBoost.R2 Transfer Learning #####################################################\n",
    "\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "\n",
    "print(\"Adaboost.R2 Transfer Learning\")\n",
    "print(\"-------------------------------------------\")\n",
    "\n",
    "r2scorelist_AdaTL_US = []\n",
    "rmselist_AdaTL_US = []\n",
    "\n",
    "\n",
    "for x in range(0, 10):\n",
    "\n",
    "    model_AdaTL_US = AdaBoostRegressor(DecisionTreeRegressor(max_depth = 1), learning_rate = 0.01, n_estimators = 500) \n",
    "    model_AdaTL_US.fit(US_np_train_X, US_np_train_y_list)\n",
    "    \n",
    "    y_pred_AdaTL_US = model_AdaTL_US.predict(US_np_test_X) \n",
    "    \n",
    "    rmse_AdaTL_US = sqrt(mean_squared_error(US_np_test_y_list, y_pred_AdaTL_US))\n",
    "    rmselist_AdaTL_US.append(rmse_AdaTL_US)\n",
    "        \n",
    "    r2_score_AdaTL_US = pearsonr(US_np_test_y_list, y_pred_AdaTL_US)\n",
    "    r2_score_AdaTL_US = (r2_score_AdaTL_US[0])**2\n",
    "    r2scorelist_AdaTL_US.append(r2_score_AdaTL_US)\n",
    "    \n",
    "\n",
    "print(\"Mean RMSE of Adaboost.R2:\", statistics.mean(rmselist_AdaTL_US))\n",
    "print(\"Mean R-squared of AdaboostR2:\", statistics.mean(r2scorelist_AdaTL_US))\n",
    "print(\"\\n\")\n",
    "print(\"RMSE of Adaboost.R2:\", rmselist_AdaTL_US)\n",
    "print(\"R^2 of AdaboostR2:\", r2scorelist_AdaTL_US)\n",
    "\n",
    "\n",
    "print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB.R2 Transfer Learning\n",
      "-------------------------------------------\n",
      "Mean RMSE of XGB.R2: 5.824769191814529\n",
      "Mean R-squared of XGB.R2: 0.7341415650936336\n",
      "\n",
      "\n",
      "RMSE of XGB.R2: [5.824769191814529, 5.824769191814529, 5.824769191814529, 5.824769191814529, 5.824769191814529, 5.824769191814529, 5.824769191814529, 5.824769191814529, 5.824769191814529, 5.824769191814529]\n",
      "R^2 of XGB.R2: [0.7341415650936336, 0.7341415650936336, 0.7341415650936336, 0.7341415650936336, 0.7341415650936336, 0.7341415650936336, 0.7341415650936336, 0.7341415650936336, 0.7341415650936336, 0.7341415650936336]\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "####################### XGBoost Transfer Learning #########################################################\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "\n",
    "print(\"XGB.R2 Transfer Learning\")\n",
    "print(\"-------------------------------------------\")\n",
    "\n",
    "r2scorelist_XGBTL_US = []\n",
    "rmselist_XGBTL_US = []\n",
    "\n",
    "# DecisionTreeRegressor(max_depth = 6), learning_rate=0.1, n_estimators = 100\n",
    "\n",
    "for x in range(0, 10):\n",
    "\n",
    "    model_XGBTL_US = XGBRegressor(n_estimators = 100, max_depth = 6, eta = 0.1, subsample = 0.5, colsample_bytree = 0.8, n_jobs = 8)\n",
    "    model_XGBTL_US.fit(US_np_train_X, US_np_train_y_list)\n",
    "    \n",
    "    y_pred_XGBTL_US = model_XGBTL_US.predict(US_np_test_X) \n",
    "    \n",
    "    rmse_XGBTL_US = sqrt(mean_squared_error(US_np_test_y_list, y_pred_XGBTL_US))\n",
    "    rmselist_XGBTL_US.append(rmse_XGBTL_US)\n",
    "        \n",
    "    r2_score_XGBTL_US = pearsonr(US_np_test_y_list, y_pred_XGBTL_US)\n",
    "    r2_score_XGBTL_US = (r2_score_XGBTL_US[0])**2\n",
    "    r2scorelist_XGBTL_US.append(r2_score_XGBTL_US)\n",
    "    \n",
    "\n",
    "print(\"Mean RMSE of XGB.R2:\", statistics.mean(rmselist_XGBTL_US))\n",
    "print(\"Mean R-squared of XGB.R2:\", statistics.mean(r2scorelist_XGBTL_US))\n",
    "print(\"\\n\")\n",
    "print(\"RMSE of XGB.R2:\", rmselist_XGBTL_US)\n",
    "print(\"R^2 of XGB.R2:\", r2scorelist_XGBTL_US)\n",
    "\n",
    "\n",
    "print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBRF.R2 Transfer Learning\n",
      "-------------------------------------------\n",
      "Mean RMSE of XGB.R2: 6.536998744850794\n",
      "Mean R-squared of XGB.R2: 0.6760191028799771\n",
      "\n",
      "\n",
      "RMSE of XGB.R2: [6.536998744850794, 6.536998744850794, 6.536998744850794, 6.536998744850794, 6.536998744850794, 6.536998744850794, 6.536998744850794, 6.536998744850794, 6.536998744850794, 6.536998744850794]\n",
      "R^2 of XGB.R2: [0.6760191028799771, 0.6760191028799771, 0.6760191028799771, 0.6760191028799771, 0.6760191028799771, 0.6760191028799771, 0.6760191028799771, 0.6760191028799771, 0.6760191028799771, 0.6760191028799771]\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "####################### XGBoost Random Forest Regression Transfer Learning #########################################################\n",
    "from xgboost import XGBRFRegressor\n",
    "\n",
    "print(\"XGBRF.R2 Transfer Learning\")\n",
    "print(\"-------------------------------------------\")\n",
    "\n",
    "r2scorelist_XGBRFTL_US = []\n",
    "rmselist_XGBRFTL_US = []\n",
    "\n",
    "# DecisionTreeRegressor(max_depth = 6), learning_rate=0.1, n_estimators = 100\n",
    "\n",
    "for x in range(0, 10):\n",
    "\n",
    "    model_XGBRFTL_US = XGBRFRegressor(n_estimators = 100, max_depth = 6, eta = 0.1, subsample = 0.5, colsample_bytree = 0.8)\n",
    "    model_XGBRFTL_US.fit(US_np_train_X, US_np_train_y_list)\n",
    "    \n",
    "    y_pred_XGBRFTL_US = model_XGBRFTL_US.predict(US_np_test_X) \n",
    "    \n",
    "    rmse_XGBRFTL_US = sqrt(mean_squared_error(US_np_test_y_list, y_pred_XGBRFTL_US))\n",
    "    rmselist_XGBRFTL_US.append(rmse_XGBRFTL_US)\n",
    "        \n",
    "    r2_score_XGBRFTL_US = pearsonr(US_np_test_y_list, y_pred_XGBRFTL_US)\n",
    "    r2_score_XGBRFTL_US = (r2_score_XGBRFTL_US[0])**2\n",
    "    r2scorelist_XGBRFTL_US.append(r2_score_XGBRFTL_US)\n",
    "    \n",
    "\n",
    "print(\"Mean RMSE of XGB.R2:\", statistics.mean(rmselist_XGBRFTL_US))\n",
    "print(\"Mean R-squared of XGB.R2:\", statistics.mean(r2scorelist_XGBRFTL_US))\n",
    "print(\"\\n\")\n",
    "print(\"RMSE of XGB.R2:\", rmselist_XGBRFTL_US)\n",
    "print(\"R^2 of XGB.R2:\", r2scorelist_XGBRFTL_US)\n",
    "\n",
    "\n",
    "print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Transfer Learning\n",
      "-------------------------------------------\n",
      "Mean RMSE of RF.TL: 6.547669379971239\n",
      "Mean R-squared of RF.TL: 0.6312631412354593\n",
      "\n",
      "\n",
      "RMSE of RF.TL: [6.547669379971239, 6.547669379971239, 6.547669379971239, 6.547669379971239, 6.547669379971239, 6.547669379971239, 6.547669379971239, 6.547669379971239, 6.547669379971239, 6.547669379971239]\n",
      "R^2 of RF.TL: [0.6312631412354593, 0.6312631412354593, 0.6312631412354593, 0.6312631412354593, 0.6312631412354593, 0.6312631412354593, 0.6312631412354593, 0.6312631412354593, 0.6312631412354593, 0.6312631412354593]\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "####################### Random Forest Regression Transfer Learning #########################################################\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "print(\"Random Forest Transfer Learning\")\n",
    "print(\"-------------------------------------------\")\n",
    "\n",
    "r2scorelist_RFTL_US = []\n",
    "rmselist_RFTL_US = []\n",
    "\n",
    "#base_estimator_ = ElasticNet()\n",
    "\n",
    "for x in range(0, 10):\n",
    "\n",
    "    model_RFTL_US = RandomForestRegressor(max_depth = 6, random_state= 0, n_estimators =  100)\n",
    "    model_RFTL_US.fit(US_np_train_X, US_np_train_y_list)\n",
    "    \n",
    "    y_pred_RFTL_US = model_RFTL_US.predict(US_np_test_X) \n",
    "    \n",
    "    rmse_RFTL_US = sqrt(mean_squared_error(US_np_test_y_list, y_pred_RFTL_US))\n",
    "    rmselist_RFTL_US.append(rmse_RFTL_US)\n",
    "        \n",
    "    r2_score_RFTL_US = pearsonr(US_np_test_y_list, y_pred_RFTL_US)\n",
    "    r2_score_RFTL_US = (r2_score_RFTL_US[0])**2\n",
    "    r2scorelist_RFTL_US.append(r2_score_RFTL_US)\n",
    "    \n",
    "\n",
    "print(\"Mean RMSE of RF.TL:\", statistics.mean(rmselist_RFTL_US))\n",
    "print(\"Mean R-squared of RF.TL:\", statistics.mean(r2scorelist_RFTL_US))\n",
    "print(\"\\n\")\n",
    "print(\"RMSE of RF.TL:\", rmselist_RFTL_US)\n",
    "print(\"R^2 of RF.TL:\", r2scorelist_RFTL_US)\n",
    "\n",
    "print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataset extracted: \n",
      "(150, 43)\n",
      "----------------------------------------------\n",
      "----------------------------------------------\n",
      "Shape of source before : (53357, 43)\n",
      "Shape of source after : (53206, 43)\n",
      "----------------------------------------------\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'US_df_new_source' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-2d02249daaac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"----------------------------------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Shape of new source before :\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUS_df_new_source\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0mUS_df_new_source\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUS_df_new_source\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUS_final_df\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Shape of new source after :\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUS_df_new_source\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'US_df_new_source' is not defined"
     ]
    }
   ],
   "source": [
    "######################################## Phase 1: Seeding Technique (US) ###################################################\n",
    "\n",
    "## Make a copy of the source dataframe\n",
    "US_df_source = US_new_source.copy()\n",
    "\n",
    "drop_features = ['ID', 'Date', 'Region', 'Lon', 'Lat']\n",
    "US_df_source = US_df_source.drop(drop_features, axis = 1)\n",
    "\n",
    "\n",
    "kmeans = KMeans(n_clusters = 150, random_state=0).fit(US_df_source)\n",
    "\n",
    "US_alternate_df = US_df_source.copy()\n",
    "US_alternate_df_np = US_df_source.to_numpy()\n",
    "\n",
    "idxlist = []\n",
    "US_new_df_list = []\n",
    "\n",
    "for rowkm in kmeans.cluster_centers_:\n",
    "    mindist = -99\n",
    "    rowidx = 0\n",
    "    idx = 0\n",
    "    for row in US_alternate_df_np:\n",
    "        dst = distance.cosine(row, rowkm)\n",
    "\n",
    "        if(dst >= mindist):\n",
    "            mindist = dst\n",
    "            rowidx = idx\n",
    "            rowval = row\n",
    "\n",
    "        idx = idx + 1\n",
    "        #print(idx)\n",
    "\n",
    "    US_new_df_list.append(rowval)\n",
    "    US_alternate_df = np.delete(US_alternate_df_np, rowidx, 0)\n",
    "    idxlist.append(rowidx)\n",
    "\n",
    "\n",
    "US_new_df = pd.DataFrame(np.vstack(US_new_df_list))\n",
    "\n",
    "print(\"Shape of dataset extracted: \")\n",
    "print(US_new_df.shape)\n",
    "print(\"----------------------------------------------\")\n",
    "\n",
    "######################################## Phase 2: Seeding Technique (US) ###################################################\n",
    "\n",
    "US_alternate_source_df = US_df_source[1:].copy()\n",
    "US_alternate_source_df_np = US_alternate_source_df.to_numpy()\n",
    "\n",
    "idxlist2 = []\n",
    "US_final_df_list = []\n",
    "\n",
    "for row_nm in US_new_df_list:\n",
    "    min_dist = -99\n",
    "    row_idx = 0\n",
    "    idx_val = 0\n",
    "    for row_alt in US_alternate_source_df_np:\n",
    "        dst = distance.cosine(row_alt, row_nm)\n",
    "        if(dst >= mindist):\n",
    "            min_dist = dst\n",
    "            row_idx = idx_val\n",
    "            row_val = row_alt\n",
    "\n",
    "        idx_val = idx_val + 1\n",
    "\n",
    "    US_final_df_list.append(row_val)\n",
    "    US_alternate_source_df_np = np.delete(US_alternate_source_df_np, row_idx, 0)\n",
    "    idxlist2.append(row_idx)\n",
    "\n",
    "\n",
    "US_final_df = pd.DataFrame(np.vstack(US_final_df_list), columns = US_df_source.columns)\n",
    "\n",
    "\n",
    "print(\"----------------------------------------------\")\n",
    "print(\"Shape of source before :\",US_df_source.shape)\n",
    "US_df_source = pd.DataFrame(np.vstack(US_alternate_source_df_np), columns= US_df_source.columns)\n",
    "print(\"Shape of source after :\", US_df_source.shape)\n",
    "\n",
    "# US_df_new_source = US_final_df\n",
    "# print(\"----------------------------------------------\")\n",
    "# print(\"Shape of new source :\", US_df_new_source.shape)\n",
    "\n",
    "\n",
    "print(\"----------------------------------------------\")\n",
    "print(\"Shape of new source before :\", US_df_new_source.shape)\n",
    "US_df_new_source = pd.concat([US_df_new_source, US_final_df], ignore_index=True)\n",
    "print(\"Shape of new source after :\", US_df_new_source.shape)\n",
    "\n",
    "# print(\"----------------------------------------------\")\n",
    "# print(\"Shape of target before :\", US_df_train.shape)\n",
    "# US_df_train = pd.concat([US_df_train, US_final_df], ignore_index=True)\n",
    "# print(\"Shape of target after :\", US_df_train.shape)\n",
    "\n",
    "print(\"----------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Set:  (10220, 48)\n",
      "Source Set:  (10000, 48)\n",
      "---------------------------\n"
     ]
    }
   ],
   "source": [
    "################################## Finding best instances from the source dataset (US) ######################################################\n",
    "\n",
    "# US_df_source = US_new_source.copy()\n",
    "# US_df_target = US_new_target.copy()\n",
    "\n",
    "US_new_source[\"ManDis\"] = \"\"\n",
    "\n",
    "US_new_target_mean = []\n",
    "prow = US_new_target.mean()\n",
    "\n",
    "US_new_target_mean = [prow.CF_CLDTT, prow.CF_PS, prow.CF_Q, prow.CF_Q10M, prow.CF_Q2M, prow.CF_RH, prow.CF_SLP,\n",
    "        prow.CF_T, prow.CF_T10M, prow.CF_T2M, prow.CF_TPREC, prow.CF_TROPPB, prow.CF_TS, prow.CF_U,\n",
    "        prow.CF_U10M, prow.CF_U2M, prow.CF_V, prow.CF_V10M, prow.CF_V2M, prow.CF_ZL, prow.CF_ZPBL,\n",
    "        prow.CF_PM25, prow.CF_CO, prow.CF_NO2, prow.CF_O3, prow.CF_SO2, prow.LC_Shrubs,\n",
    "        prow.LC_Herbaceous, prow.LC_Agriculture, prow.LC_Urban, prow.LC_Bare, prow.LC_Snow,\n",
    "        prow.LC_Water, prow.LC_Wetland, prow.LC_Lichen, prow.LC_Closed_Forest,\n",
    "        prow.LC_Open_Forest, prow.LC_Ocean, prow.Elevation, prow.Dist_Primary,\n",
    "        prow.Dist_Secondary, prow.Pop]\n",
    "\n",
    "rowidx = 0\n",
    "\n",
    "for row in US_new_source.itertuples():\n",
    "    row_list =[row.CF_CLDTT, row.CF_PS, row.CF_Q, row.CF_Q10M, row.CF_Q2M, row.CF_RH, row.CF_SLP,\n",
    "               row.CF_T, row.CF_T10M, row.CF_T2M, row.CF_TPREC, row.CF_TROPPB, row.CF_TS, row.CF_U,\n",
    "               row.CF_U10M, row.CF_U2M, row.CF_V, row.CF_V10M, row.CF_V2M, row.CF_ZL, row.CF_ZPBL,\n",
    "               row.CF_PM25, row.CF_CO, row.CF_NO2, row.CF_O3, row.CF_SO2, row.LC_Shrubs,\n",
    "               row.LC_Herbaceous, row.LC_Agriculture, row.LC_Urban, row.LC_Bare, row.LC_Snow,\n",
    "               row.LC_Water, row.LC_Wetland, row.LC_Lichen, row.LC_Closed_Forest,\n",
    "               row.LC_Open_Forest, row.LC_Ocean, row.Elevation, row.Dist_Primary,\n",
    "               row.Dist_Secondary, row.Pop]\n",
    "\n",
    "    man_dis = 0\n",
    "    for i in range(0, len(row_list)):\n",
    "        tempval = US_new_target_mean[i] - row_list[i]\n",
    "        man_dis = man_dis + abs(tempval)\n",
    "\n",
    "    US_new_source.loc[rowidx,\"ManDis\"] = man_dis\n",
    "    rowidx = rowidx + 1\n",
    "\n",
    "US_new_source = US_new_source.sort_values(by =['ManDis'])\n",
    "US_new_source = US_new_source.head(10000) \n",
    "US_new_source = US_new_source.drop(['ManDis'], axis =1)\n",
    "US_new_source = US_new_source.reset_index(drop=True)\n",
    "\n",
    "#################### Splitting with small target set and large source and test set #############\n",
    "print(\"Target Set: \", US_new_target.shape)\n",
    "print(\"Source Set: \", US_new_source.shape)\n",
    "\n",
    "######## Split into features and output ########\n",
    "output_US = ['PM25']\n",
    "\n",
    "US_y_source_df = US_new_source['PM25']\n",
    "US_X_source_df = US_new_source.drop(output_US, axis = 1)\n",
    "\n",
    "US_y_target_df = US_new_target['PM25']\n",
    "US_X_target_df = US_new_target.drop(output_US, axis = 1)\n",
    "\n",
    "\n",
    "######## Drop the unwanted features ########\n",
    "drop_features = ['ID', 'Date', 'Region', 'Lon', 'Lat']\n",
    "\n",
    "US_X_source_df = US_X_source_df.drop(drop_features, axis = 1)\n",
    "US_X_target_df = US_X_target_df.drop(drop_features, axis = 1)\n",
    "\n",
    "US_X_source_df.shape, US_X_target_df.shape\n",
    "US_X_source_df.columns\n",
    "\n",
    "######## Normalize the dataset ########\n",
    "from sklearn import preprocessing\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "X_target = US_X_target_df.values \n",
    "X_target_scaled = min_max_scaler.fit_transform(X_target)\n",
    "US_X_target_df = pd.DataFrame(X_target_scaled)\n",
    "\n",
    "X_source = US_X_source_df.values \n",
    "X_source_scaled = min_max_scaler.fit_transform(X_source)\n",
    "US_X_source_df = pd.DataFrame(X_source_scaled)\n",
    "\n",
    "\n",
    "######## Split into Train-Test ########\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "US_X_target_df, US_X_test_df, US_y_target_df, US_y_test_df = train_test_split(US_X_target_df, US_y_target_df, test_size = 0.50)\n",
    "\n",
    "######## Merging the datasets ########\n",
    "US_X_df = pd.concat([US_X_target_df, US_X_source_df], ignore_index=True)\n",
    "US_y_df = pd.concat([US_y_target_df, US_y_source_df], ignore_index=True)\n",
    "\n",
    "US_np_train_X = US_X_df.to_numpy()\n",
    "US_np_train_y = US_y_df.to_numpy()\n",
    "\n",
    "US_np_test_X = US_X_test_df.to_numpy()\n",
    "US_np_test_y = US_y_test_df.to_numpy()\n",
    "\n",
    "US_np_train_y_list = US_np_train_y.ravel()\n",
    "US_np_test_y_list = US_np_test_y.ravel()\n",
    "\n",
    "src_size_US = len(US_y_source_df)\n",
    "tgt_size_US = len(US_y_target_df)\n",
    "\n",
    "print(\"---------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOTRBoost.R2 AS\n",
      "-------------------------------------------\n",
      "Inside STrAdaBoost.R2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-23f853b90053>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m                           random_state = random_state)\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mmodel_lotrboost_us\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mUS_np_train_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUS_np_train_y_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0my_pred_us\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_lotrboost_us\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mUS_np_test_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/PM25/PM_25_TL/two_TrAdaBoostR2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    286\u001b[0m                 \u001b[0;31m## Make this model learn.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m                 \u001b[0;31m## source_weight remains the same over all the CV iterations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_weight_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m                 \u001b[0;31m## Get the predictions for the model fitted on the test set.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m                 \u001b[0my_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_weight_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m         \u001b[0;31m# Fit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1007\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate_estimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_weight_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0miboost\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0;31m# Boosting step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             sample_weight, estimator_weight, estimator_error = self._boost(\n\u001b[0m\u001b[1;32m    131\u001b[0m                 \u001b[0miboost\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_weight_boosting.py\u001b[0m in \u001b[0;36m_boost\u001b[0;34m(self, iboost, X, y, sample_weight, random_state)\u001b[0m\n\u001b[1;32m   1066\u001b[0m         \u001b[0mX_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_safe_indexing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbootstrap_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m         \u001b[0my_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_safe_indexing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbootstrap_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1068\u001b[0;31m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1069\u001b[0m         \u001b[0my_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, callbacks)\u001b[0m\n\u001b[1;32m    537\u001b[0m                 \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'eval_metric'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0meval_metric\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 539\u001b[0;31m         self._Booster = train(params, train_dmatrix,\n\u001b[0m\u001b[1;32m    540\u001b[0m                               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_num_boosting_rounds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m                               \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m     return _train_internal(params, dtrain,\n\u001b[0m\u001b[1;32m    209\u001b[0m                            \u001b[0mnum_boost_round\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_boost_round\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,\n\u001b[0m\u001b[1;32m   1368\u001b[0m                                                     \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m                                                     dtrain.handle))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "####################### STrAdaBoost.R2 #########################################################\n",
    "from two_TrAdaBoostR2 import TwoStageTrAdaBoostR2\n",
    "\n",
    "sample_size = [len(US_X_target_df), len(US_X_source_df)]\n",
    "# sample_size = [153, 900]\n",
    "\n",
    "n_estimators = 50\n",
    "steps = 20\n",
    "fold = 10\n",
    "random_state = np.random.RandomState(1)\n",
    "\n",
    "print(\"LOTRBoost.R2 AS\")\n",
    "print(\"-------------------------------------------\")\n",
    "\n",
    "r2scorelist_stradaboost_us = []\n",
    "rmselist_stradaboost_us = []\n",
    "\n",
    "#DecisionTreeRegressor(max_depth=6)\n",
    "\n",
    "for x in range(0, 10):\n",
    "\n",
    "    model_lotrboost_us = TwoStageTrAdaBoostR2(XGBRegressor(max_depth = 6),\n",
    "                          n_estimators = n_estimators, sample_size = sample_size,\n",
    "                          steps = steps, fold = fold,\n",
    "                          random_state = random_state)\n",
    "\n",
    "    model_lotrboost_us.fit(US_np_train_X, US_np_train_y_list)\n",
    "    y_pred_us = model_lotrboost_us.predict(US_np_test_X) \n",
    "\n",
    "    rmse_lotrboost_us = sqrt(mean_squared_error(US_np_test_y_list, y_pred_lotrboost_US))\n",
    "    rmselist_lotrboost_us.append(rmse_lotrboost_US)\n",
    "    print(\"RMSE:\", rmse_lotrboost_us)\n",
    "\n",
    "    r2_score_lotrboost_us = pearsonr(US_np_test_y_list, y_pred_lotrboost_US)\n",
    "    r2_score_lotrboost_us = (r2_score_lotrboost_us[0])**2\n",
    "    r2scorelist_lotrboost_us.append(r2_score_lotrboost_us)\n",
    "    print(\"R-squared Score:\", r2_score_lotrboost_us)\n",
    "    \n",
    "    \n",
    "print(\"Mean RMSE of LOTRBoost:\", statistics.mean(rmselist_lotrboost_us))\n",
    "print(\"Mean R-squared of LOTRBoost:\", statistics.mean(r2scorelist_lotrboost_us))\n",
    "print(\"\\n\")\n",
    "print(\"RMSE of LOTRBoost:\", rmselist_lotrboost_us)\n",
    "print(\"R-squared of LOTRBoost:\", r2scorelist_lotrboost_us)\n",
    "\n",
    "\n",
    "print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_ENTL = [11.459042742133393, 11.459042742133393, 11.459042742133393, 11.459042742133393, 11.459042742133393, 11.459042742133393, 11.459042742133393, 11.459042742133393, 11.459042742133393, 11.459042742133393]\n",
    "r2_ENTL = [0.5263031480122707, 0.5263031480122707, 0.5263031480122707, 0.5263031480122707, 0.5263031480122707, 0.5263031480122707, 0.5263031480122707, 0.5263031480122707, 0.5263031480122707, 0.5263031480122707]\n",
    "\n",
    "rmse_GBRTL = [6.242791736154706, 5.967406971288434, 6.103326661391169, 6.07966037305247, 6.4169278145635475, 6.0571821464155065, 5.9666186528394425, 5.980383869431792, 6.331659419282624, 6.0910839995023]\n",
    "r2_GBRTL = [0.6803614576376373, 0.7066887760393229, 0.69999203769556, 0.6945941560052241, 0.659883312133618, 0.6985052523902867, 0.7057963218332743, 0.7047052579294206, 0.6746246477627736, 0.6928201925101125]\n",
    "\n",
    "rmse_AdaBoostTL = [6.513838707632526, 6.507590223702059, 6.55160350375954, 6.599943793994746, 6.573921853611853, 6.611650583448354, 6.646095874239225, 6.740173209131736, 6.586374086165526, 6.557380568200348]\n",
    "r2_AdaBoostTL = [0.639587617398082, 0.6383348179277775, 0.6338738352675425, 0.6298573545575309, 0.6336110481214641, 0.6254952052983551, 0.6222174153130634, 0.6108497440065302, 0.630458479203877, 0.6335778226077935]\n",
    "\n",
    "rmse_XGBTL = [5.824769191814529, 5.824769191814529, 5.824769191814529, 5.824769191814529, 5.824769191814529, 5.824769191814529, 5.824769191814529, 5.824769191814529, 5.824769191814529, 5.824769191814529]\n",
    "r2_XGBTL = [0.7341415650936336, 0.7341415650936336, 0.7341415650936336, 0.7341415650936336, 0.7341415650936336, 0.7341415650936336, 0.7341415650936336, 0.7341415650936336, 0.7341415650936336, 0.7341415650936336]\n",
    "\n",
    "rmse_XGBRFTL = [6.536998744850794, 6.536998744850794, 6.536998744850794, 6.536998744850794, 6.536998744850794, 6.536998744850794, 6.536998744850794, 6.536998744850794, 6.536998744850794, 6.536998744850794]\n",
    "r2_XGBRFTL = [0.6760191028799771, 0.6760191028799771, 0.6760191028799771, 0.6760191028799771, 0.6760191028799771, 0.6760191028799771, 0.6760191028799771, 0.6760191028799771, 0.6760191028799771, 0.6760191028799771]\n",
    "\n",
    "rmse_RFTL = [6.547669379971239, 6.547669379971239, 6.547669379971239, 6.547669379971239, 6.547669379971239, 6.547669379971239, 6.547669379971239, 6.547669379971239, 6.547669379971239, 6.547669379971239]\n",
    "r2_RFTL = [0.6312631412354593, 0.6312631412354593, 0.6312631412354593, 0.6312631412354593, 0.6312631412354593, 0.6312631412354593, 0.6312631412354593, 0.6312631412354593, 0.6312631412354593, 0.6312631412354593]\n",
    "\n",
    "\n",
    "# Calculate the average\n",
    "rmse_ENTL_mean = np.mean(rmse_ENTL)\n",
    "rmse_GBRTL_mean = np.mean(rmse_GBRTL)\n",
    "rmse_AdaBoostTL_mean = np.mean(rmse_AdaBoostTL)\n",
    "rmse_XGBTL_mean = np.mean(rmse_XGBTL)\n",
    "rmse_XGBRFTL_mean = np.mean(rmse_XGBRFTL)\n",
    "rmse_RFTL_mean = np.mean(rmse_RFTL)\n",
    "\n",
    "r2_ENTL_mean = np.mean(r2_ENTL)\n",
    "r2_GBRTL_mean = np.mean(r2_GBRTL)\n",
    "r2_AdaBoostTL_mean = np.mean(r2_AdaBoostTL)\n",
    "r2_XGBTL_mean = np.mean(r2_XGBTL)\n",
    "r2_XGBRFTL_mean = np.mean(r2_XGBRFTL)\n",
    "r2_RFTL_mean = np.mean(r2_RFTL)\n",
    "\n",
    "\n",
    "# Calculate the standard deviation\n",
    "rmse_ENTL_std = np.mean(rmse_ENTL)\n",
    "rmse_GBRTL_std = np.mean(rmse_GBRTL)\n",
    "rmse_AdaBoostTL_std = np.mean(rmse_AdaBoostTL)\n",
    "rmse_XGBTL_std = np.mean(rmse_XGBTL)\n",
    "rmse_XGBRFTL_std = np.mean(rmse_XGBRFTL)\n",
    "rmse_RFTL_std = np.mean(rmse_RFTL)\n",
    "\n",
    "r2_ENTL_std = np.mean(r2_ENTL)\n",
    "r2_GBRTL_std = np.mean(r2_GBRTL)\n",
    "r2_AdaBoostTL_std = np.mean(r2_AdaBoostTL)\n",
    "r2_XGBTL_std = np.mean(r2_XGBTL)\n",
    "r2_XGBRFTL_std = np.mean(r2_XGBRFTL)\n",
    "r2_RFTL_std = np.mean(r2_RFTL)\n",
    "\n",
    "\n",
    "# Define labels, positions, bar heights and error bar heights\n",
    "labels = ['ENTL', 'GBRTL', 'AdaBoostTL', 'XGBTL', 'XGBRFTL', 'RFTL']\n",
    "x_pos = np.arange(len(labels))\n",
    "\n",
    "CTE_rmse = [rmse_ENTL_mean, rmse_GBRTL_mean, rmse_AdaBoostTL_mean, rmse_XGBTL_mean, rmse_XGBRFTL_mean, rmse_RFTL_mean]\n",
    "error_rmse = [rmse_ENTL_std, rmse_GBRTL_std, rmse_AdaBoostTL_std, rmse_XGBTL_std, rmse_XGBRFTL_std, rmse_RFTL_std]\n",
    "\n",
    "CTE_r2 = [r2_ENTL_mean, r2_GBRTL_mean, r2_AdaBoostTL_mean, r2_XGBTL_mean, r2_XGBRFTL_mean, r2_RFTL_mean]\n",
    "error_r2 = [r2_ENTL_std, r2_GBRTL_std, r2_AdaBoostTL_std, r2_XGBTL_std, r2_XGBRFTL_std, r2_RFTL_std]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsgAAALICAYAAABiqwZ2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdeZgtV1kv/u9LQhhkOGDgCAkYELgYkckThuuQBkWCA1FAICACggEU9CJcyHWABtQfkwQHMEbEEAUiCjLdSIhgg4pAghcRIomRRDlhJgOQkZD1+6PqJCs73X26c/rsvU/35/M8++muqlW1367Vu/e3q9auqtZaAACAwQ1mXQAAAMwTARkAADoCMgAAdARkAADoCMgAANDZf9YFbIQDDzywHXLIIbMuAwCAfcjHPvaxr7TWbjM5f1ME5EMOOSSnn376rMsAAGAfUlX/tdx8QywAAKAjIAMAQEdABgCAjoAMAAAdARkAADoCMgAAdARkAADoCMgAANARkAEAoCMgAwBAR0AGAICOgAwAAB0BGQAAOgIyAAB0BGQAAOgIyAAA0BGQAQCgIyADAEBHQAYAgI6ADAAAHQEZAAA6AjIAAHQEZAAA6AjIAADQEZABAKAjIAMAQEdABgCAjoAMAAAdARkAADoCMgAzt7i4mKrasMfi4uKsfyRgH1attVnXsMd27NjRTj/99FmXAcBetLCwkCRZWlqaaR3A5lFVH2ut7Zic7wgyAAB0BGQAAOgIyAAA0BGQAQCgIyADAEBHQAYAgI6ADAAAHQEZAAA6AjIAAHQEZAAA6AjIAADQEZABAKAjIAMAQEdABgCAjoAMAAAdARkAADoCMgAAdARkAADoCMgAANARkAEAoCMgAwBAR0AGAICOgAwAAB0BGQAAOlMNyFX1+qr6UlV9cjftDquqb1XVo6ZVGwAAJNM/gnxCkiNWa1BV+yV5WZJTplEQAAD0phqQW2sfTHL+bpo9K8lbk3xp71cEAADXtv+sC+hV1UFJfjrJg5Mctpu2Ryc5Okm2b9+epaWlvV4fALNz4YUXJom/98BeN1cBOcmrkzy/tfatqlq1YWvt+CTHJ8mOHTvawsLC3q8OgJnZtm1bksTfe2Bvm7eAvCPJSWM4PjDJj1XVla21t8+2LAAAtoq5CsittTvt+r6qTkjybuEYAIBpmmpArqo3J1lIcmBV7UzywiQ3TJLW2nHTrAUAAJYz1YDcWjtqHW2ftBdLAQCAZbmTHgAAdARk2GCLi4upqg17LC4uzvpHAoAtZa4+pAebweLi4m5D7a7LVLmeKwDMH0eQAQCgIyADAEBHQAYAgI6ADAAAHQEZAAA6AjIAAHQEZAAA6AjIAADQEZABAKAjIAMAQEdABgCAjoAMAKxocXExVbVhj8XFxVn/SLBb+8+6AABgfi0uLu421C4sLCRJlpaW9no9MA2OIAMAQEdABgCAjoAMAAAdARkAADoCMgAAdARkAADoCMgAANARkAEAoCMgAwBAR0AGAICOgAwAAB0BGQAAOgIyAAB0BGQAAOgIyAAA0BGQAQCgIyADAEBHQAYAgI6ADAAAHQEZAAA6AjIAAHQEZAAA6AjIAADQEZABAKAjIAMAQEdABgCAjoAMAAAdARkAADoCMgAAdARkAADoCMgAANARkAEAoCMgAwBAR0AGAICOgAwAAB0BGQAAOgIyAMA+aHFxMVW1YY/FxcVZ/0hzQ0AGtgxvJsBmsri4mNbaqo/DDz88hx9++G7btdb8TevsP+sCAKZlcXFxt28ACwsLSZKlpaW9Xg8A88kRZAAA6AjIAADQEZABAKAjIAMAQEdABgCAjoAMAAAdARkAADoCMgAAdARkAADoCMgAANARkAEAoCMgAwBAR0AGAICOgAwAAB0BGQAAOgIyAAB0BGQAAOgIyAAA0BGQAQCgIyADAEBnqgG5ql5fVV+qqk+usPzxVfWJ8fGhqrrXNOsDAIBpH0E+IckRqyw/J8nhrbV7JnlJkuOnURQAAOyy/zSfrLX2wao6ZJXlH+omP5zk4L1dEwAA9KYakNfpKUn+dqWFVXV0kqOTZPv27VlaWppSWbDnLrzwwiTxezuH9M380jfzS9/ML31z/cxlQK6qB2UIyD+wUpvW2vEZh2Ds2LGjLSwsTKc42ADbtm1Lkvi9nT/6Zn7pm/mlb+aXvrl+5i4gV9U9k7wuycNaa1+ddT0AAGwtc3WZt6q6Y5K3JXlCa+2sWdcDAMDWM9UjyFX15iQLSQ6sqp1JXpjkhknSWjsuyQuSfHuS11ZVklzZWtsxzRoBANjapn0Vi6N2s/ypSZ46pXIAAOA65mqIBQAAzJqADAAAHQEZAAA6AjIAAHQEZAAA6AjIAADQEZABAKAjIAMAQEdABgCAjoAMAAAdARkAADoCMgAAdARkAADoCMgAANARkAEAoCMgAwBAR0AGAICOgAwAAB0BGQAAOgIyAAB0BGQAAOgIyAAA0BGQAQCgIyADAEBHQAYAgI6ADAAAHQEZAAA6AjIAAHQEZAAA6AjIAADQEZABAKAjIAMAQEdABgCAjoAMAAAdARkAADoCMgAAdARkAADoCMgAANARkAEAoCMgAwBAR0AGAICOgAwAAB0BGQAAOgIyAAB0BGQAAOgIyAAA0BGQAQCgIyADAEBHQAYAgI6ADAAAHQEZAAA6AjIAAHQEZAAA6AjIAADQEZABAKAjIAMAQEdA3kctLi6mqjbssbi4OOsfCQBgLuw/6wK4fhYXF3cbahcWFpIkS0tLe70eAIDNwhFkAADoCMgAANARkAEAoCMgAwBAR0AGAICOgAwAAB0BGQAAOgIyAAB0BGQAAOgIyAAA0BGQAQCgIyADAEBHQAYAgI6ADAAAHQEZAAA6AjIAAHQEZAAA6Ew1IFfV66vqS1X1yRWWV1X9flWdXVWfqKr7TrM+AACY9hHkE5IcscryhyW56/g4OskfTaEmAAC42lQDcmvtg0nOX6XJkUlObIMPJ9lWVbebTnUAAJDsP+sCJhyU5LPd9M5x3ucnG1bV0RmOMmf79u1ZWlqaRn37lAsvvDBJ7Js5pG/ml76ZX/pmfumb+aVvrp95C8i1zLy2XMPW2vFJjk+SHTt2tIWFhb1Y1r5p27ZtSRL7Zv7om/mlb+aXvplf+mZ+6ZvrZ96uYrEzyR266YOTfG5GtQAAsAXNW0B+Z5KfG69m8YAkF7XWrjO8AgAA9papDrGoqjcnWUhyYFXtTPLCJDdMktbacUlOTvJjSc5OckmSJ0+zPgAAmGpAbq0dtZvlLckvTakcAAC4jnkbYgEAADMlIAMAQEdABgCAjoAMAAAdARkAADoCMgAAdARkAADoCMgAANARkAEAoCMgAwBAR0AGAICOgAwAAB0BGQAAOgIyAAB0BGQAAOgIyAAA0BGQAQCgIyADAEBHQAYAgI6ADAAAHQEZAAA6AjIAAHQEZAAA6AjIAADQEZABAKAjIAMAQEdABgCAjoAMAAAdARkAADoCMgAAdARkAADoCMgAANARkAEAoCMgAwBAR0AGAICOgAwAAB0BGQAAOgIyAAB0BGQAAOgIyAAA0BGQAQCgIyADAEBHQAYAgI6ADAAAHQEZAAA6AjIAAHQEZAAA6AjIAADQEZABAKAjIAMAQEdABgCAjoAMAAAdARkAADoCMgAAdARkAADoCMgAANARkAEAoCMgAwBAZ//dNaiqc5K0tW6wtXbnPaoIAABmaLcBOclbc+2A/NgkN01yapIvJbltkockuTjJSRtdIAAATNNuA3Jr7bm7vq+qX0vyn0l+vLV2cTf/ZkneneRre6NIAACYlvWOQf6lJK/ow3GStNa+keSV43IAANhnrTcg3zLJ9hWWfUeSm+1ZOQAAMFtrGYPce2eSV1TV15K8q7V2eVXdKMnDk7wsybs2ukAAAJim9QbkZyQ5IclbkrSq+nqSmyepDOH5GRtaHQAATNm6AnJr7aIkP11V35PksAzDLb6Q5LTW2hl7oT4AAJiq9R5BTpK01j6V5FMbXAsAAMzcuu+kV1W3raqXVdX7qurM8WhyqupXquqBG18iAABMz7oCclXdL8l/JHlkknOT3CXJjcbFt0vynI0sDgAApm29R5CPTfL3Se6W5GkZPpy3y0eT3G+D6gIAgJlY7xjk+yY5srV2VVXVxLKvZrjtNAAA7LPWewT5oiS3WWHZnZN8cc/KAQCA2VpvQH5HkhdV1Z27ea2qDkzy3CRv27DKAABgBtYbkI9J8rUkZyT54DjvuCRnJrk0yQs2rjQAAJi+9d4o5IKqekCSJyT54SQXJzk/yeuSnNhau3zjSwQAgOlZc0CuqhtnuJ3077TW/jTJn16fJ6yqI5L8XpL9kryutfbSieW3TPIXSe441vfK1tqfXZ/nAgCA9VrzEIvW2mUZbi+93/V9sqraL8lrkjwsyaFJjqqqQyea/VKSM1pr90qykOR3q+qA6/ucAACwHusdg/zOJD+1B893vyRnt9Y+01q7IslJSY6caNOS3Hy8jNzNMgzhuHIPnhMAANZsvddBPiXJK6rqdklOznBZt9Y3aK2dvMr6ByX5bDe9M8n9J9r8YYYg/rkkN0/ymNbaVZMbqqqjkxydJNu3b8/S0tK6fpCt4MILL0wS+2YO6Zv5pW/ml76ZX/pmfumb62e9Afkvxq+PGB+TWlYfgjF5c5Fd6/QemuTjSR6c5LuSnFpV/9Ba+9q1Vmrt+CTHJ8mOHTvawsLCbovfarZt25YksW/mj76ZX/pmfumb+aVv5pe+uX7WG5DvtIfPtzPJHbrpgzMcKe49OclLW2stydlVdU6Su2e4lTUAAOxV673M23/t4fOdluSuVXWnJOcleWySx020+e8Ml5D7h6ranuR/JPnMHj4vAACsyXqPICdJqmr/DJdhu/HkstbaGSut11q7sqqemWEs835JXt9a+1RVPX1cflySlyQ5oar+LcOQjOe31r5yfeoEAID1WldArqobJvn9JE9McqMVmq16GbjxQ3wnT8w7rvv+c0l+dD11AQDARlnvZd5ekOQnkjwlw9HdZ2YYM/y+JOcm+cmNLA4AAKZtvQH50UkWk7xlnP5oa+3E1tqPJvnHXPeaxgAAsE9Zb0C+Q5KzWmvfSnJZklt1y96Y5JEbVRgAAMzCegPy55NsG78/J8kPdcu+a0MqAgCAGVrvVSyWkvxgkncl+ZMkr6yquyS5PMljkrx5Q6sDAIApW29A/vUkByZJa+3VVVVJHpXkJkn+IMmLN7Y8AACYrvXeKOQLSb7QTR+b5NiNLgoAAGZlvWOQAQBgU1vvjUK+nKSt1qa1dts9qggAAGZovWOQX5PrBuRbJ3lwklsk+dONKAoAAGZlvWOQF5ebP35Y7y1JrtyAmgAAYGY2ZAxya60leV2GW08DAMA+ayM/pHfnJAds4PYAAGDq1vshvV9cZvYBSb47yeOT/NVGFAUAALOy3g/p/eEy8y5PsjPJa5O8aI8rAgCAGVrvh/RcNxkAgE1N4AUAgM56xyD/3Hrat9ZOXF85AAAwW+sdg3xCrrlRSHXzV5onIAMAsE9Z7xCLw5Kcm+Q3kxya5MDx6wvG+Yclufn4uMVGFQkAANOy3iPIv5vkNa21V3Xzzk/y21V1WZJXtdYO37DqAABgytZ7BPl+ST61wrJPZjiCDAAA+6z1HkH+bJInJzllmWVPyXA9ZAD2QceeetasS1jVzgsuTTL/dT77IXebdQnAHlpvQP61JCdV1SeTvDPJl5LcNsnDk9w9yWM2tjwAAJiu9d4o5K1Vdf8kxyQ5Ksl3JPlCktOSPLG19rGNLxEAAKZnvUeQ01r7lySP3gu1AADAzO3xnfSq6u5V9VNVdfuNKAgAAGZpXQG5qv64qo7rph+T4eoVb0vy6ar6nxtcHwAATNV6jyAfkeSD3fRLkrwpye0zXNniJRtUFwAAzMR6A/JtM1zqLVV11yR3SfLy1toXkhyf5D4bWx4AAEzXegPy+Um2j9//SJIvtNY+OU5Xkv02qjAAAJiF9V7F4m+TvLiqtid5XpK3dMvukeTcDaoLAABmYr1HkJ+T5MNJnp5hLPILu2U/neQ9G1QXAADMxHpvFHJRkp9fYdkPbkhFAAAwQ9f7Osg1eEFVfcdGFgQAALO0JzcKuUGGIRZuEAIAwKaxp3fSqw2pAgAA5sQe32oaAAA2kz0JyFcleVGSzyVJVT2oqv52Q6oCAIAZWdNVLKpqW4bbTN8hyWeSvLO19s0kL6qqn6mq5ye5b5Kz9lqlAAAwBbsNyFX1vUnem2vuoJck/1JVj0zypiQPSHJGkscn+cu9USSs17Gnzvf/ajsvuDTJ/Nf57IfcbdYlAMDUrWWIxe8k+VqSBya5aZLvznDL6dMy3D3via21722tvbm1dtVeqxQAAKZgLUMsdiT5ldbaR8bpM6vqGUn+I8nRrbW/2GvVAQDAlK3lCPL2JOdOzNs1/a8bWQwAAMzaWq9i0VaYf+VGFQIAAPNgTVexSHJKVS0Xht83Ob+1dts9LwsAAGZjLQH5RXu9CgBgRfN+xRtX5mGz2W1Abq0JyAAAbBluNQ0AAB0BGQAAOgIyAAB0BGQAAOgIyAAA0BGQAQCgIyADAEBHQAYAgI6ADAAAHQEZAAA6AjIAAHQEZAAA6AjIAADQEZABAKAjIAMAQEdABgCAjoAMAACd/WddAADAvujYU8+adQm7tfOCS5PMf63PfsjdZl3CtQjIwFTN+x9pbyYAGGIBAAAdARkAADoCMgAAdARkAADoCMgAANARkAEAoCMgAwBAR0AGAIDO1ANyVR1RVWdW1dlVdcwKbRaq6uNV9amq+sC0awQAYOua6p30qmq/JK9J8pAkO5OcVlXvbK2d0bXZluS1SY5orf13Vd12mjUCALC1TfsI8v2SnN1a+0xr7YokJyU5cqLN45K8rbX230nSWvvSlGsEAGALm+oR5CQHJflsN70zyf0n2twtyQ2rainJzZP8XmvtxMkNVdXRSY5Oku3bt2dpaWlv1LtPu/DCC5NkS+6bgy67fNYlrOpGV12aJDnosnNmXMnqlpY+t+Hb1DcbQ9/ML30zvza6b+a9X5Kt2zd7atoBuZaZ1yam90/yfUl+OMlNkvxzVX24tXbWtVZq7fgkxyfJjh072sLCwsZXu4/btm1bkmQr7ptjTz1r941m6PIb3CRJct6N7zTjSlb36IW7bfg29c3G0DfzS9/Mr43um3nvl2Tr9s2emnZA3pnkDt30wUkm/2XYmeQrrbWLk1xcVR9Mcq8k8/9bCADAPm/aY5BPS3LXqrpTVR2Q5LFJ3jnR5h1JfrCq9q+qm2YYgvHvU64TAIAtaqpHkFtrV1bVM5OckmS/JK9vrX2qqp4+Lj+utfbvVfWeJJ9IclWS17XWPjnNOgEA2LqmPcQirbWTk5w8Me+4ielXJHnFNOsCAIDEnfQAAOBaBGQAAOgIyAAA0BGQAQCgIyADAEBHQAYAgI6ADAAAHQEZAAA6AjIAAHQEZAAA6AjIAADQEZABAKAjIAMAQEdABgCAjoAMAAAdARkAADoCMgAAdARkAADoCMgAANARkAEAoCMgAwBAR0AGAICOgAwAAB0BGQAAOgIyAAB09p91AfuyY089a9YlrGrnBZcmmf86n/2Qu826BACAqzmCDAAAHQEZAAA6AjIAAHQEZAAA6AjIAADQEZABAKAjIAMAQEdABgCAjoAMAAAdARkAADoCMgAAdARkAADoCMgAANARkAEAoCMgAwBAR0AGAICOgAwAAB0BGQAAOgIyAAB0BGQAAOgIyAAA0BGQAQCgIyADAEBHQAYAgI6ADAAAHQEZAAA6AjIAAHQEZAAA6AjIAADQEZABAKAjIAMAQEdABgCAjoAMAAAdARkAADoCMgAAdARkAADoCMgAANARkAEAoCMgAwBAR0AGAICOgAwAAB0BGQAAOgIyAAB0BGQAAOgIyAAA0BGQAQCgIyADAEBHQAYAgI6ADAAAnakH5Ko6oqrOrKqzq+qYVdodVlXfqqpHTbM+AAC2tqkG5KraL8lrkjwsyaFJjqqqQ1do97Ikp0yzPgAAmPYR5PslObu19pnW2hVJTkpy5DLtnpXkrUm+NM3iAABg2gH5oCSf7aZ3jvOuVlUHJfnpJMdNsS4AAEiS7D/l56tl5rWJ6VcneX5r7VtVyzUfN1R1dJKjk2T79u1ZWlraqBrX7KDLLp/6c67Hja66NEly0GXnzLiS1S0tfW7Dt6lvNoa+mV/6Zn7pm/m10X0z7/2SbN2+2VPTDsg7k9yhmz44yeQe2ZHkpDEcH5jkx6rqytba2/tGrbXjkxyfJDt27GgLCwt7q+YVHXvqWVN/zvW4/AY3SZKcd+M7zbiS1T164W4bvk19szH0zfzSN/NL38yvje6bee+XZOv2zZ6adkA+Lcldq+pOSc5L8tgkj+sbtNau7sGqOiHJuyfDMQAA7C1TDcittSur6pkZrk6xX5LXt9Y+VVVPH5cbdwwAwExN+whyWmsnJzl5Yt6ywbi19qRp1AQAALu4kx4AAHQEZAAA6AjIAADQEZABAKAjIAMAQEdABgCAjoAMAAAdARkAADoCMgAAdARkAADoCMgAANARkAEAoCMgAwBAR0AGAICOgAwAAB0BGQAAOgIyAAB0BGQAAOgIyAAA0BGQAQCgIyADAEBHQAYAgI6ADAAAHQEZAAA6AjIAAHQEZAAA6AjIAADQEZABAKAjIAMAQEdABgCAjoAMAAAdARkAADoCMgAAdARkAADoCMgAANARkAEAoCMgAwBAR0AGAICOgAwAAB0BGQAAOgIyAAB0BGQAAOgIyAAA0BGQAQCgIyADAEBHQAYAgI6ADAAAHQEZAAA6AjIAAHQEZAAA6AjIAADQEZABAKAjIAMAQEdABgCAjoAMAAAdARkAADoCMgAAdARkAADoCMgAANARkAEAoCMgAwBAR0AGAICOgAwAAB0BGQAAOgIyAAB0BGQAAOgIyAAA0BGQAQCgIyADAEBHQAYAgI6ADAAAHQEZAAA6AjIAAHQEZAAA6AjIAADQmXpArqojqurMqjq7qo5ZZvnjq+oT4+NDVXWvadcIAMDWNdWAXFX7JXlNkoclOTTJUVV16ESzc5Ic3lq7Z5KXJDl+mjUCALC1TfsI8v2SnN1a+0xr7YokJyU5sm/QWvtQa+2CcfLDSQ6eco0AAGxh0w7IByX5bDe9c5y3kqck+du9WhEAAHT2n/Lz1TLz2rINqx6UISD/wArLj05ydJJs3749S0tLG1Ti2h102eVTf871uNFVlyZJDrrsnBlXsrqlpc9t+Db1zcbQN/NL38wvfTO/Nrpv5r1fkq3bN3tq2gF5Z5I7dNMHJ7nOHqmqeyZ5XZKHtda+utyGWmvHZxyfvGPHjrawsLDhxe7OsaeeNfXnXI/Lb3CTJMl5N77TjCtZ3aMX7rbh29Q3G0PfzC99M7/0zfza6L6Z935Jtm7f7KlpD7E4Lcldq+pOVXVAkscmeWffoKrumORtSZ7QWpv/3zwAADaVqR5Bbq1dWVXPTHJKkv2SvL619qmqevq4/LgkL0jy7UleW1VJcmVrbcc06wQAYOua9hCLtNZOTnLyxLzjuu+fmuSp064LAAASd9IDAIBrEZABAKAjIAMAQEdABgCAjoAMAAAdARkAADoCMgAAdARkAADoCMgAANARkAEAoCMgAwBAR0AGAICOgAwAAB0BGQAAOgIyAAB0BGQAAOgIyAAA0BGQAQCgIyADAEBHQAYAgI6ADAAAHQEZAAA6AjIAAHQEZAAA6AjIAADQEZABAKAjIAMAQEdABgCAjoAMAAAdARkAADoCMgAAdARkAADoCMgAANARkAEAoCMgAwBAR0AGAICOgAwAAB0BGQAAOgIyAAB0BGQAAOgIyAAA0BGQAQCgIyADAEBHQAYAgI6ADAAAHQEZAAA6AjIAAHQEZAAA6AjIAADQEZABAKAjIAMAQEdABgCAjoAMAAAdARkAADoCMgAAdARkAADoCMgAANARkAEAoCMgAwBAR0AGAICOgAwAAB0BGQAAOgIyAAB0BGQAAOgIyAAA0BGQAQCgIyADAEBHQAYAgI6ADAAAHQEZAAA6AjIAAHQEZAAA6AjIAADQEZABAKAjIAMAQEdABgCAztQDclUdUVVnVtXZVXXMMsurqn5/XP6JqrrvtGsEAGDrmmpArqr9krwmycOSHJrkqKo6dKLZw5LcdXwcneSPplkjAABb27SPIN8vydmttc+01q5IclKSIyfaHJnkxDb4cJJtVXW7KdcJAMAWVa216T1Z1aOSHNFae+o4/YQk92+tPbNr8+4kL22t/eM4/b4kz2+tnT6xraMzHGFOkv+R5Mwp/Ajz5PZJNvIfh88n+dwGbm8r0zfzS9/ML30zv/TN/NI3e+47W2u3mZy5/5SLqGXmTSb0tbRJa+34JMdvRFGbWVWd3lrbMes6uC59M7/0zfzSN/NL38wvfbN+0x5isTPJHbrpg3Pd/1TW0gYAAPaKaQfk05LctaruVFUHJHlskndOtHlnkp8br2bxgCQXtdY+P+U6AQDYoqY6xKK1dmVVPTPJKUn2S/L61tqnqurp4/Ljkpyc5MeSnJ3kkiRPnmaNm5BhKPNL38wvfTO/9M380jfzS9+s01Q/pAcAAPPOnfQAAKAjIAMAQEdAnmNVtVhVbYXHz45tdk0/cGLde4zzF8bHStu5+tE951dm8fPOu6r6qap6b1V9taquqKrzquqkqvr+rs25E/v1y1V1clXda2Jbk337hap6d1Xdc1y+tIY+WxzbtnFs/z5h/ADuOWPdd1lD+6t/l9f5PCdM7K+vV9VpVfWI6138HqiqA8Z+v3c3b7XX+K7H0th2qar+eha1r0dVvb+q/rWq9p+Y/8jx53lIN+9eVfWm8bV0RVWdX1V/V1WP6ddfS19u5tfMWsxqv4/tJt9jLqyqj1TVTy1T5+TfyKvf09bQf5PvaffYG/ty3izzd+Ja7xdjm0NW2Wdr2rfdc8kAmf51kFm/i5Icscz8syemfyPJj6+wjX9J0gfo70/yyiSPyHBRcHajqo5N8stJTsxw+/OvJvnODFdi+cequktr7T/H5m9K8gfj97dP8n+SnFJV391au6DbbN+3hyR5cZJTq+q7k/xiklt0bfmiWoMAABOVSURBVP8syWeSvKSbt3Njfrqpe2CGnzcZ9t9v7cXn+nSu+aDvLZI8KclfVdXhu25GNEUHJHlhknOTfHyc97ok7+naPCvJg5P8dDfva9MobgP9YpJPZHi9vCpJqupmSV6d5C2ttVPHeY/K8Fr5pwyvkXOT3DrDh7RPTHKzJH/abXd3fbmZXzNrMav93nt8hn1+6yTPTPK2qlporX1wol3/N3KXs3Pt96mbJHl/hr8P/7ebf0aS+668GzatFd8vWmvnd+2em6Fve2dEBlg3AXn+XTnecns1S0l+rKru01r7f5MLW2tfS3L1NqrqwPHb/9daO3ejCt2squrIJP8ryZNbaydMLP7zqvrJJJd28z7f91lVfTrJpzL8gTq5a9f37Yer6twk/5zhbpNvmqjh4iRfXsPvwr7gqCQXJ/nk+P3eDMgXT/TF3yV5UJKHJ5l2QL6O1trOdKFtDC+X78v93Fr7dFX9bpIXVdVfttbOS/KiJLdM8uwkqaqDkpyQ5I1Jfr5d+9Pibx/Xn7w72Kp92Vo7o2+8yV4zuzWr/T7R9hOttU+O7ZaSfDbJzyaZDMifX6Ffrj5yOYb7JPnPybZVy91PbNNb8f0iwz8cu5y5wr6VAdbJEIvN4W0Z/kP89VkXskn9rySnLROOkySttXe11la7mc3Xx6833M3z/Ov49Q6rttqHVdV+SX4mw/XOX5/k0P404djmF6vqs1V1cVW9K8vcRrWqnjOe6r2oqr5YVe+qNQzXaK1dleHykdfqi6q6d1W9r6ouqaoLquqNVbV9os2BVfWGGobYXFLDKf0dE20eXlUfG2u/YDzNfPi4eNfvwZ91pzUP2V3N+6iXZDjLcuzYv7+c5IXd6+SpGQ7QPGcipCVJWmtnttaWVnuClfpyi5ub/d5auyTDUeFN+/dsxjb9+8WsCcj7gKraf/Ix0aQl+Z0kj6iqQ2dQ4qY17usHJnnv+la7uq/ukOTlSc5P8oHdrHfH8es56690n/HgJNuTnJTkr5N8M8NR5CRXH61/TZJ3Zzj9928ZgvSkg5P8YZIjk/xChuuq/1NV3XKyYdcXt66q52Y4PfmObvltMpyFuWmSx2UY5nB4htOXB3SbenuSh2Y4hfmYDH8//35XMK+q7xp/pvcn+ckMp5vfneF0866fPRmOmD9wfGzK05tjOPqVDP8MvSPDP/D9KfUfSnL6xKnh3dpdX25187Tfq+oGGV6ny/09q4n3tP3WUw9JVn6/uMHEvpXzridDLObft2cIEddSVXeaODVyUobTaf8nyROmU9qW8O1JbpThVOHVajjH1/9R/1Z3ROZXx8cuFyZ5RGvtwsmNd//sfGeGwPfxbO43/KMy7I/3tNauqKpTkzy2qn5t3H+/Pi57xtj+lDHAPrXfSGvt2bu+H99cT03ypQyB+cSu6ffl2q+fq5I8b+Io2XPGrw8dhyOlqs5K8pEkj0zy5qo6IsO4vYXW2gfGNu/PMH7zfyd5WpL7JPl6a+1/d9vuh9ScNn69zinjzai19o6q+liGPnhwa+3KbvHtc8047KtN/PN/1Xi0cpe19OWWN+P9vt+4rVsleV6Sb0vye8u0m/wbeV6GMM0q1vh+MTn9xgzDXFgn/1nMv4uSHLbM41qn9Ftr30ry0iRHjUey2Bi7BrtNno58ToY3jV2PX+qW/UWu6aeHZviD9TeTQwlyzT8/38xwKvI+GYL05Rv5A8yLqrpRhg+f/U1r7Ypx9pszHI16wBh075Pr/oF/2zLbekBVnVpVX01yZYZTvjdLcreJpv+ea/ri8CQvSPLbVfWkrs39krx3VzhOktbaRzOE3x/o2nx5Vzge21yc4Qjxrjb/luSW4zCMH62qb1t1h2xy4/CT+2R47SxMLs7Ea2ps37+m3jKxzlr6csub8X7/+LiNL2UIwE9qrZ25TLv+b+RhGT4gyOrW+n7x7Fx73/7mNIvcTBxBnn9XttZOX2PbEzP88Xp+kt/feyVtKV9Jcnmue3TjzzOclk+uOTK4yxf7PhuPkt4nQ988qmt3UZIfyXAk+l4ZPlX8pqr6/okjOJvFw5JsS3JyVW0b5y1l2L9HZThVuH+GN9fetaar6o4Zhrx8NMOR288luSLDJ91vPLHuJROvnw9W1XckeXlVvWE8an27DB+inPTFXDM84nbj9IptWmtnjkNEjslw5PibVfU3SX6ltfblZdbdtMbTun+U4UNEpyY5ZtzfnxmbLHfE8IwMb+hJ8sfLbHYtfbmlzcF+f2yS/8xw+v+3Moy3/+gyn9H44jre1xis9f3ibPt2YziCvImMR+VekeSJcbpqQ4ynJ/85yY9OzP9ia+30tfwhGt9APp3kuycWXTlu4yOtteMzHIV+QIbxg5vRrrHGf5XkgvHx2QxDWB6d5MsZjgbfdmK9yekjMowXPrK19tettQ9lOHJ166zNGUluk2TXJ7k/v8xzJMNY6fPX0Sattf/bWvvBDEd7npLhDW3yclZbwdMz/FP4ixnObJ2Xa//T/sEkh1XVrXbNaK1d0r2mvp61mezLrW7W+/1T47belmEc/k3jCOZG2WrvFzMnIG8+f5IheDxv1oVsIq9Ocv+qul5ju8fxyodmYhzzMv4iw5HM51+f55lnNVyy6ScyDKl40MTjVzMEzQdlCLpHTqw+eWOPm2QYB9mPrXx01n5G7B4ZLsv31XH6I0keWlU37+o9LMPQj3/s2ty2qn6oa3PTDNcev87l4lprF42X6vubDH2fDEe5k+se5d5Uquq2SX47yR+01j4xngL+5SQ/Ph5hT4brP38rwz/0e2KyL7esedvvbbgu/OuSPGmsjY21ad8v5oUhFvNv/6p6wDLzPzte5/JaWmuXVdWrkrxsD57zgBquxzrpA1vtVHFy9YdeXp3khKp6UJJ3ZRh68e1Jdt2d6hvdKrfr+uxWGa6McI8MQyxWe55WVb+T5I1V9cOttfeto8x7L9Nn1xozO2NHZjia9HuttY/0C6rqnzJ8OO+oDFdjeVtV/VGGcHl4rnujnPdnOM34Z1X1p0m+J8OVJa7zIcgk39b1xU2S/GCGq168tjst+aokz8jwgcCXZRjL/NIMY4rfmiSttVPGOv+yqo7JEAyeO27zFePP8bQMV6Z4T4ZhH3fNcHTnxHEbV1TVOUkeXVWfTHJZhuvG7grOa3HQcq/N1to83WHvlRnC0wt3zWitnVxV70jy6qp6b2vtvKp6cobf9TtnuKnHuRn2/Y4k98xwKcDeWvpyPeb9NbNe87jfXz62fVb2zpHkh1TV3SfmndEmrom9GU2+X2QY2rJRZIAkaa15zOkjyWKGD1Qs9/iNsU1L8syJ9W6W4Q28ZfjU/eR2f2Jcdsg6n/M629pKjwwfMDs1wyn1b2YIQW9N8rCuzbkT++zCDBdof+Qy+/kryzzHfknOSnLKxPzTk5ywQl0r9dfSrPdZV+O7k5y1yvLXZjjzcaMMd+DameGDdydnGN5yrd+/JD+X4Q3h0nH/3n/c96/s2pwwsT8uzXBq+JgkB0w8/30yBO9Lxj57U5LtE21ukyHsXjBu6wNJDuuWPzDDOOjPZQi/52T4R/VGXZsfzXC3s8uWew1mCDnnrrCPllbq61n3b1fjD401HbXMsu/McIOYl3Tz7p3hrMLnxtfU+WM/PC3JDa9PX26W18y+tN8zfBiwJbnHMs//+gzvR982Tl/rdbrKz3SzcZtPWmbZQlbuw8VZ98de6N/F7Ob9IsMZr5bkJ9awPRlgDY8adwgAABBjkAEA4FoEZAAA6AjIAADQEZABAKAjIAMAQEdABgCAjoAMsIKqWqyqVlVLs67l+hrrb1W1cD3Xf9K4/rkbWxnA/BKQgS2jqm5VVZd1ofGus65pVqpq2/gPwGJVbZt1PQDzREAGtpLHZ7hb3y4/P6tCpujM8XHJxPxtGW5L/MLxewBGAjKwlTxl/PoH49cnVtV+sypmGlprdx8fH511LQD7CgEZ2BKq6r5J7p3kwiTPS/KZJLdL8rBZ1gXA/BGQga1i19Hjv2ytXZbkzyfmXy9VdWRVva+qLqyqb1TVv1bV86rqhmv5kF9VLVTVX1XVeVV1eVV9Zdzek1c6uj253ap6ZFW9t6q+VFVXVdVi1/Y6H9Ib1zun2+Q5Xbvd1ft9VfWWqvr8WO9nqupVVXWrNdb68PHn+2pVfa2qPlRVPzWxzhOq6p+q6oJxn36wqn54lZpuVVUvrqp/Gbd5RVV9oao+UVXHrbYuwHIEZGDTq6obJ3ncOHli97Ul+Ymq2n49t/vKJG9P8uAkt0zyzSSHJnlZkr9LcsPdrP+qJH+f5FEZjmZfkmE88IOTvD7Je6vq5rvZxu8m+eskP5Jk/yRXraH085N8pZv+SpIvdo/zV3iuxyX55yQ/k+Qm4/PdKcmzk/xDVd1sN7W+KMk7kiwk2S/JzZM8MMnfVNXTa3BChr6539jm25L8YJJTqurHl9nmwUk+nuQ3k9xnbP+NJAcm+d4kTxuXAayZgAxsBY/MEDzPbq19KElaa59J8o8ZQt4T1rvBqnpskueMk29KcnBr7VYZQt/RGQLeM1ZZ/5kZgmWSHJ/k9uP6txznX5khKP/JKmV8X5JfTfLyJNtba7fOEBD/bLXaW2uPSHJYN+uw1tp3dI9HLLPabTKE9jckuWNrbdv4sz4zwz8G35Nh6MpK7p3k15P8RpJbj+sfnOSUcfnLkywmeUySpye5ZWvtFknuluT0DGH5tVU1+b61mOSOSc7N8E/CAeN+uFGSQzL0wYdXqQvgOgRkYCvYNYzixIn5J04sX5OqqiQvHidPTfKzrbXzkqS1dllr7U8yBLOVhh3cJMmLxsk3t9ae1lr7wrj+xa21V2cIvknymKrasUIpN0vyqtba81trXx7Xv7y19l/r+XnW6KZJTmqt/UJr7bPjc13SWntNrvnQ41GrrH/LJC9srf12a+2icf3zkjw6ycUZwvYLkjy1tfbHrbVLxjb/keSx4zbumOR/Tmx31/Svtdbe11r71rjet1pr/9VaO661dswe/NzAFiQgA5taVd05wyn9lmvGHe/yliSXJrl7VU0Gr9XcO8muayj/TmutLdPmDUn+e4X1H5Lk1uP3iyu0eW2Sz4/frxQ8r8ownGNafmuF+e8Yv96lqm66QpvLkrx6cmZr7WsZhm0kw/560zJt/jPJ2ePkPScWXzh+vd0KzwuwbgIysNn9fJJK8g+ttXP7BWM4e3vXbq3uO379ZpIPLddgDM0fWGH9XUeEP9taO2uF9b+V5P0T7Sed3Vr70u7L3RDnt9bOXmHZ57rvlz1qnuSM1trFKyz74vj19BX+2ejbTG7/3ePXl1bV8VV1RFXdYoVtAKyJgAxsWuN41SeOk5PDK3Z5w/j1Mbv7kFnnNuPXr7bWrlil3XkrzL/tbpbvsnOi/aRpheMk+foqy67svl/pg4lrWX8tbSa3/4oMZwJumOQXkvxtkgur6t+q6hVVdbdVtgmwLAEZ2MwemuGDYEnyuolLmbWqakneMy6/WYbxsGtR49eVjnZOtlvJ7tbfXbtvrXH9Tau19s3W2mMyDHt5cYaj7pckuUeS5yY5o6qes8omAK5DQAY2s/Ve43itwyx2Hbk9sKoOWKXd7Xez/h128zy7wv2X11jXltVa+9fW2gtbaz+c4YolP5LkgxmufvGKqrrXTAsE9ikCMrApVdVtkjx8nHxUhqskrPS439ju+6vq7mvY/L+MX2+Y615VYdfzV5IfWmH908evB680BGC8SciDxsnT1lDTevXXS97dke59Smvtytba+5L8eJLLM/x8PzLbqoB9iYAMbFZPyBBgL0ryrtbaN1Z5nJbk0+N6azmK/PFcc1WFY8YwPOlnk3znCuufmuSr4/eLK7R5Wq45Av3mNdS0Xl/rvt+2F7Y/FVV1o1UWX55rhqFs+eEowNoJyMBmtSvovmM3H6Tb5a/Grz9XVfuv1nC80sILx8mHJnlDVd0+Ge7aV1VPSfLHSS5YYf1Lc00wPmq8HfL2cf2bVtWzcs0l0f6ytfaxNdS/Lq21C3PNhwSfvLufeY79V1X9f1X1gD4sV9Vdkrwxw/Wbr8o1NyQB2C0BGdh0quoBGe7sllwTfHdnV7vtGU7Nr6q19qZcE2KfkGRnVZ2f4cjs6zJc2/e4cflly6z/h0mOHSefluTz4/oXJfn9DEe//z7DlRn2ll31PSvJN6rqv6vq3Ko6aS8+50bbnuSYDPv7kqo6v6ouTfIfGW6J3ZI8p7X27zOsEfj/27tjlQaCKArD/61EFExnOp/C2sqUgj6BnaUo2NoI+gQ2voOCnVioIPaCaGXnC2gjiMW1mA3ZJjFsDEnk/8owu9nyMMycO2MMyJL+o+7lvA/gepgHMvMJ6IaooS73ZeYesAXcUSrK5qp3HFB2lheqpe99nt+njJM+p/T8LlbvuaXsgK9n5qDqs1EdA7uUM9HflEuBK0B7jP/51zrACXAPvAHz1e+vlJHbq9VkQkkaWvTvZJckjSIiHiiX+A4z82jS3yNJGo47yJI0BhGxRq/h4mrQWknSdDEgS1JDEXEaEdsR0e42WUREKyJ2gMtq2U3VkiFJmhEesZCkhiLiEegOoPiiTHBr0esVfgE6mfnbSGlJ0hQxIEtSQxGxAWxSBo0sA0uUFotn4AI4y8zPyX2hJKkJA7IkSZJU4xlkSZIkqcaALEmSJNUYkCVJkqQaA7IkSZJUY0CWJEmSan4AjWgIjYTvM30AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build the plot\n",
    "# plt.figure()\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (10,10))\n",
    "ax.bar(x_pos, CTE_r2,\n",
    "       yerr = error_r2,\n",
    "       align='center',\n",
    "       alpha=0.5,\n",
    "       ecolor='black',\n",
    "       capsize=10)\n",
    "ax.set_ylabel('R-squared', fontsize = 15)\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(labels, fontsize = 15)\n",
    "ax.set_xlabel('Algorithms', fontsize = 25)\n",
    "# ax.set_title('Algorithms')\n",
    "ax.yaxis.grid(True)\n",
    "\n",
    "#Save the figure and show\n",
    "plt.tight_layout()\n",
    "plt.savefig('plot_r2_USnew.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
