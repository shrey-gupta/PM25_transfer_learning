{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repositories uploaded!!\n"
     ]
    }
   ],
   "source": [
    "# from two_TrAdaBoostR2 import TwoStageTrAdaBoostR2 ##STrAdaBoost.R2\n",
    "# from TwoStageTrAdaBoostR2 import TwoStageTrAdaBoostR2 ##two-stage TrAdaBoost.R2\n",
    "\n",
    "import pandas as pd\n",
    "import sys\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.stats.stats import pearsonr\n",
    "from math import sqrt\n",
    "import statistics \n",
    "from scipy.stats import *\n",
    "from scipy.spatial import distance\n",
    "\n",
    "######### Instance Transfer repositories ####################\n",
    "from adapt.instance_based import TrAdaBoost, TrAdaBoostR2, TwoStageTrAdaBoostR2\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from adapt.instance_based import KMM\n",
    "\n",
    "print(\"Repositories uploaded!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ailerons Data\n",
      "Training Set:  (7154, 41)\n",
      "Testing Set:  (6596, 41)\n",
      "-------------------------------------------\n",
      "Target Set:  (358, 41)\n",
      "Source Set:  (6796, 41)\n",
      "Test Set:  (6596, 41)\n"
     ]
    }
   ],
   "source": [
    "################################################################ Ailerons ###############################################\n",
    "target_ailerons = ['goal']\n",
    "colnames_ailerons = ['climbRate', 'Sgz', 'p', 'q', 'curPitch', 'curRoll', 'absRoll', 'diffClb', 'diffRollRate', 'diffDiffClb', 'SeTime1',\n",
    "            'SeTime2', 'SeTime3', 'SeTime4', 'SeTime5', 'SeTime6', 'SeTime7', 'SeTime8', 'SeTime9', 'SeTime10', 'SeTime11', 'SeTime12', \n",
    "            'SeTime13', 'SeTime14', 'diffSeTime1', 'diffSeTime2', 'diffSeTime3', 'diffSeTime4', 'diffSeTime5', 'diffSeTime6', 'diffSeTime7',\n",
    "            'diffSeTime8', 'diffSeTime9', 'diffSeTime10', 'diffSeTime11', 'diffSeTime12', 'diffSeTime13', 'diffSeTime14', 'alpha', 'Se', 'goal']\n",
    "AileronsData_train_df = pd.read_csv('UCI_regression/Ailerons/ailerons.data', header = None, names = colnames_ailerons) \n",
    "print(\"Ailerons Data\")\n",
    "print(\"Training Set: \", AileronsData_train_df.shape)\n",
    "\n",
    "AileronsData_test_df = pd.read_csv('UCI_regression/Ailerons/ailerons.test', header = None, names = colnames_ailerons) \n",
    "print(\"Testing Set: \", AileronsData_test_df.shape)\n",
    "\n",
    "print(\"-------------------------------------------\")\n",
    "\n",
    "############################################### Standardization ###############################################\n",
    "ailerons_cols = AileronsData_train_df.columns.difference(['goal'])\n",
    "ss = StandardScaler()\n",
    "AileronsData_train_df[ailerons_cols] = ss.fit_transform(AileronsData_train_df[ailerons_cols])\n",
    "AileronsData_test_df[ailerons_cols] = ss.transform(AileronsData_test_df[ailerons_cols]) ## Use the same scale as the training data\n",
    "\n",
    "\n",
    "#################### Splitting with small target set and large source and test set #############\n",
    "AileronsData_source_df, AileronsData_tgt_df = train_test_split(AileronsData_train_df, test_size = 0.05) ## test_size = tgt size\n",
    "# print(AileronsData_df_tgt.shape, AileronsData_df_source.shape, AileronsData_df_test.shape)\n",
    "\n",
    "AileronsData_train_df = AileronsData_train_df.reset_index(drop = True)\n",
    "AileronsData_tgt_df = AileronsData_tgt_df.reset_index(drop = True)\n",
    "AileronsData_source_df = AileronsData_source_df.reset_index(drop = True)\n",
    "print(\"Target Set: \", AileronsData_tgt_df.shape)\n",
    "print(\"Source Set: \", AileronsData_source_df.shape)\n",
    "print(\"Test Set: \", AileronsData_test_df.shape)\n",
    "\n",
    "\n",
    "# ################################# Splitting into predictors and target data #################################\n",
    "# AileronsData_df_test_y = AileronsData_test_df[target_ailerons]\n",
    "# AileronsData_df_test_X = AileronsData_test_df.drop(target_ailerons, axis = 1)\n",
    "\n",
    "# AileronsData_df_tgt_y = AileronsData_tgt_df[target_ailerons]\n",
    "# AileronsData_df_tgt_X = AileronsData_tgt_df.drop(target_ailerons, axis = 1)\n",
    "\n",
    "# AileronsData_df_source_y = AileronsData_source_df[target_ailerons]\n",
    "# AileronsData_df_source_X = AileronsData_source_df.drop(target_ailerons, axis = 1)\n",
    "\n",
    "\n",
    "# ############## Merging the datasets #################\n",
    "# ailerons_X_df = pd.concat([AileronsData_df_source_X, AileronsData_df_tgt_X], ignore_index=True)\n",
    "# ailerons_y_df = pd.concat([AileronsData_df_source_y, AileronsData_df_tgt_y], ignore_index=True)\n",
    "\n",
    "# ailerons_np_train_X = ailerons_X_df.to_numpy()\n",
    "# ailerons_np_train_y = ailerons_y_df.to_numpy()\n",
    "\n",
    "# ailerons_np_test_X = AileronsData_df_test_X.to_numpy()\n",
    "# ailerons_np_test_y = AileronsData_df_test_y.to_numpy()\n",
    "\n",
    "# ailerons_np_train_y_list = ailerons_np_train_y.ravel()\n",
    "# ailerons_np_test_y_list = ailerons_np_test_y.ravel()\n",
    "\n",
    "# src_size_ailerons = len(AileronsData_df_source_y)\n",
    "# tgt_size_ailerons = len(AileronsData_df_tgt_y)\n",
    "\n",
    "# print(\"-------------------------------------------\")\n",
    "###############################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 41)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######################################## Variance Sampling: Phase 1###################################################\n",
    "kmeans = KMeans(n_clusters = 15, random_state=0).fit(AileronsData_tgt_df)\n",
    "\n",
    "AileronsData_alternate_df = AileronsData_tgt_df.copy()\n",
    "AileronsData_alternate_df_np = AileronsData_tgt_df.to_numpy()\n",
    "\n",
    "idxlist = []\n",
    "AileronsData_new_df_list = []\n",
    "\n",
    "for rowkm in kmeans.cluster_centers_:\n",
    "    mindist = -99\n",
    "    rowidx = 0\n",
    "    idx = 0\n",
    "    for row in AileronsData_alternate_df_np:\n",
    "        dst = distance.euclidean(row, rowkm)\n",
    "\n",
    "        if(dst >= mindist):\n",
    "            mindist = dst\n",
    "            rowidx = idx\n",
    "            rowval = row\n",
    "\n",
    "        idx = idx + 1\n",
    "\n",
    "#     print(\"Row selected: \", rowidx) #Alternate_df.loc[rowidx,:]\\\n",
    "#     print(\"Min. distance: \", mindist)\n",
    "#     print(\"Matrix shape: \", AileronsData_alternate_df_np.shape)\n",
    "    AileronsData_new_df_list.append(rowval)\n",
    "    AileronsData_alternate_df = np.delete(AileronsData_alternate_df_np, rowidx, 0)\n",
    "    idxlist.append(rowidx)\n",
    "\n",
    "\n",
    "AileronsData_new_df = pd.DataFrame(np.vstack(AileronsData_new_df_list))\n",
    "AileronsData_new_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of source dataset before Variance Sampling:  (6796, 41)\n",
      "Shape of source dataset after Variance Sampling:  (6781, 41)\n",
      "----------------------------------------------\n",
      "Shape of target before Variance Sampling:  (358, 41)\n",
      "Shape of target after Variance Sampling:  (373, 41)\n",
      "----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "##################################################### Variance Sampling: Phase 2 ################################################\n",
    "\n",
    "AileronsData_alternate_source_df = AileronsData_source_df.copy()\n",
    "AileronsData_alternate_source_df_np = AileronsData_alternate_source_df.to_numpy()\n",
    "\n",
    "idxlist2 = []\n",
    "AileronsData_final_df_list = []\n",
    "\n",
    "for row_nm in AileronsData_new_df_list:\n",
    "    min_dist = -99\n",
    "    row_idx = 0\n",
    "    idx_val = 0\n",
    "    for row_alt in AileronsData_alternate_df_np:\n",
    "        dst = distance.euclidean(row_alt, row_nm)\n",
    "        if(dst >= mindist):\n",
    "            min_dist = dst\n",
    "            row_idx = idx_val\n",
    "            row_val = row_alt\n",
    "\n",
    "        idx_val = idx_val + 1\n",
    "\n",
    "#     print(\"Row selected: \", row_idx) #Alternate_df.loc[rowidx,:]\\\n",
    "#     print(\"Min. distance: \", min_dist)\n",
    "#     print(\"Matrix shape: \", AileronsData_alternate_source_df_np.shape)\n",
    "    AileronsData_final_df_list.append(row_val)\n",
    "    AileronsData_alternate_source_df_np = np.delete(AileronsData_alternate_source_df_np, row_idx, 0)\n",
    "    idxlist2.append(row_idx)\n",
    "\n",
    "\n",
    "AileronsData_final_df = pd.DataFrame(np.vstack(AileronsData_final_df_list), columns= AileronsData_alternate_source_df.columns)\n",
    "\n",
    "\n",
    "print(\"Shape of source dataset before Variance Sampling: \",AileronsData_source_df.shape)\n",
    "AileronsData_source_df = pd.DataFrame(np.vstack(AileronsData_alternate_source_df_np), columns= AileronsData_source_df.columns)\n",
    "print(\"Shape of source dataset after Variance Sampling: \", AileronsData_source_df.shape)\n",
    "\n",
    "print(\"----------------------------------------------\")\n",
    "\n",
    "print(\"Shape of target before Variance Sampling: \", AileronsData_tgt_df.shape)\n",
    "AileronsData_tgt_df = pd.concat([AileronsData_tgt_df, AileronsData_final_df], ignore_index=True)\n",
    "print(\"Shape of target after Variance Sampling: \", AileronsData_tgt_df.shape)\n",
    "\n",
    "print(\"----------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Set:  (373, 41)\n",
      "Source Set:  (1500, 41)\n",
      "Test Set:  (6596, 41)\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "################################## Importance Sampling: Manhattan Distance ######################################################\n",
    "\n",
    "AileronsData_source_df[\"ManDis\"] = \"\"\n",
    "\n",
    "AileronsData_tgt_df_mean = []\n",
    "prow = AileronsData_tgt_df.mean()\n",
    "AileronsData_tgt_df_mean = [prow.climbRate, prow.Sgz, prow.p, prow.q, prow.curPitch, prow.curRoll, prow.absRoll,\n",
    "       prow.diffClb, prow.diffRollRate, prow.diffDiffClb, prow.SeTime1, prow.SeTime2, prow.SeTime3, prow.SeTime4, \n",
    "        prow.SeTime5, prow.SeTime6, prow.SeTime7, prow.SeTime8, prow.SeTime9, prow.SeTime10, prow.SeTime11, prow.SeTime12, \n",
    "        prow.SeTime13, prow.SeTime14, prow.diffSeTime1, prow.diffSeTime2, prow.diffSeTime3, prow.diffSeTime4, prow.diffSeTime5, \n",
    "        prow.diffSeTime6, prow.diffSeTime7, prow.diffSeTime8, prow.diffSeTime9, prow.diffSeTime10, prow.diffSeTime11, prow.diffSeTime12,\n",
    "       prow.diffSeTime13, prow.diffSeTime14, prow.alpha, prow.Se, prow.goal]\n",
    "\n",
    "rowidx = 0\n",
    "\n",
    "for row in AileronsData_source_df.itertuples():\n",
    "    row_list =[row.climbRate, row.Sgz, row.p, row.q, row.curPitch, row.curRoll, row.absRoll,\n",
    "       row.diffClb, row.diffRollRate, row.diffDiffClb, row.SeTime1, row.SeTime2, row.SeTime3, row.SeTime4, \n",
    "        row.SeTime5, row.SeTime6, row.SeTime7, row.SeTime8, row.SeTime9, row.SeTime10, row.SeTime11, row.SeTime12, \n",
    "        row.SeTime13, row.SeTime14, row.diffSeTime1, row.diffSeTime2, row.diffSeTime3, row.diffSeTime4, row.diffSeTime5, \n",
    "        row.diffSeTime6, row.diffSeTime7, row.diffSeTime8, row.diffSeTime9, row.diffSeTime10, row.diffSeTime11, row.diffSeTime12,\n",
    "       row.diffSeTime13, row.diffSeTime14, row.alpha, row.Se, row.goal]\n",
    "\n",
    "    man_dis = 0\n",
    "    for i in range(0, len(row_list)):\n",
    "        tempval = AileronsData_tgt_df_mean[i] - row_list[i]\n",
    "        man_dis = man_dis + abs(tempval)\n",
    "\n",
    "    AileronsData_source_df.loc[rowidx,\"ManDis\"] = man_dis\n",
    "    rowidx = rowidx + 1\n",
    "\n",
    "AileronsData_source_df = AileronsData_source_df.sort_values(by =['ManDis'])\n",
    "AileronsData_source_df = AileronsData_source_df.head(1500) \n",
    "AileronsData_source_df = AileronsData_source_df.drop(['ManDis'], axis =1)\n",
    "AileronsData_source_df = AileronsData_source_df.reset_index(drop=True)\n",
    "\n",
    "# print(\"Target Set: \",AileronsData_tgt_df.shape)\n",
    "# print(\"Source Set: \",AileronsData_source_df.shape)\n",
    "# print(\"Test Set: \",AileronsData_test_df.shape)\n",
    "\n",
    "\n",
    "################################# Splitting into predictors and target data #################################\n",
    "print(\"Target Set: \", AileronsData_tgt_df.shape)\n",
    "print(\"Source Set: \", AileronsData_source_df.shape)\n",
    "print(\"Test Set: \", AileronsData_test_df.shape)\n",
    "\n",
    "\n",
    "AileronsData_df_test_y = AileronsData_test_df[target_ailerons]\n",
    "AileronsData_df_test_X = AileronsData_test_df.drop(target_ailerons, axis = 1)\n",
    "\n",
    "AileronsData_df_tgt_y = AileronsData_tgt_df[target_ailerons]\n",
    "AileronsData_df_tgt_X = AileronsData_tgt_df.drop(target_ailerons, axis = 1)\n",
    "\n",
    "AileronsData_df_source_y = AileronsData_source_df[target_ailerons]\n",
    "AileronsData_df_source_X = AileronsData_source_df.drop(target_ailerons, axis = 1)\n",
    "\n",
    "\n",
    "############## Merging the datasets #################\n",
    "ailerons_X_df = pd.concat([AileronsData_df_tgt_X, AileronsData_df_source_X], ignore_index=True)\n",
    "ailerons_y_df = pd.concat([AileronsData_df_tgt_y, AileronsData_df_source_y], ignore_index=True)\n",
    "\n",
    "ailerons_np_train_X = ailerons_X_df.to_numpy()\n",
    "ailerons_np_train_y = ailerons_y_df.to_numpy()\n",
    "\n",
    "ailerons_np_test_X = AileronsData_df_test_X.to_numpy()\n",
    "ailerons_np_test_y = AileronsData_df_test_y.to_numpy()\n",
    "\n",
    "ailerons_np_train_y_list = ailerons_np_train_y.ravel()\n",
    "ailerons_np_test_y_list = ailerons_np_test_y.ravel()\n",
    "\n",
    "src_size_ailerons = len(AileronsData_df_source_y)\n",
    "tgt_size_ailerons = len(AileronsData_df_tgt_y)\n",
    "\n",
    "print(\"-------------------------------------------\")\n",
    "\n",
    "###############################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################### STrAdaBoost.R2 Active Sampling Ailerons ################################################################\n",
    "from two_TrAdaBoostR2 import TwoStageTrAdaBoostR2\n",
    "\n",
    "sample_size = [len(AileronsData_df_tgt_X), len(AileronsData_df_source_X)]\n",
    "n_estimators = 100\n",
    "steps = 30\n",
    "fold = 10\n",
    "random_state = np.random.RandomState(1)\n",
    "\n",
    "r2scorelist_stradaboost_ailerons = []\n",
    "rmselist_stradaboost_ailerons = []\n",
    "\n",
    "for x in range(0, 10):\n",
    "\n",
    "    model_stradaboost_ailerons = TwoStageTrAdaBoostR2(DecisionTreeRegressor(max_depth=6),\n",
    "                        n_estimators = n_estimators, sample_size = sample_size,\n",
    "                        steps = steps, fold = fold, random_state = random_state)\n",
    "\n",
    "\n",
    "    model_stradaboost_ailerons.fit(ailerons_np_train_X, ailerons_np_train_y_list)\n",
    "\n",
    "\n",
    "    y_pred_stradaboost_ailerons = model_stradaboost_ailerons.predict(ailerons_np_test_X)\n",
    "\n",
    "    mse_stradaboost_ailerons = sqrt(mean_squared_error(ailerons_np_test_y, y_pred_stradaboost_ailerons))\n",
    "    rmselist_stradaboost_ailerons.append(mse_stradaboost_ailerons)\n",
    "        \n",
    "    r2_score_stradaboost_ailerons = pearsonr(ailerons_np_test_y_list, y_pred_stradaboost_ailerons)\n",
    "    r2_score_stradaboost_ailerons = (r2_score_stradaboost_ailerons[0])**2\n",
    "    r2scorelist_stradaboost_ailerons.append(r2_score_stradaboost_ailerons)\n",
    "\n",
    "print(\"RMSE of STrAdaboostR2:\", rmselist_stradaboost_ailerons)\n",
    "print(\"R^2 of STrAdaboostR2:\", r2scorelist_stradaboost_ailerons)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"RMSE of STrAdaboostR2:\", statistics.mean(rmselist_stradaboost_ailerons), statistics.stdev(rmselist_stradaboost_ailerons))\n",
    "print(\"R^2 of STrAdaboostR2:\", statistics.mean(r2scorelist_stradaboost_ailerons), statistics.stdev(r2scorelist_stradaboost_ailerons))\n",
    "\n",
    "print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Two-Stage TrAdaBoost.R2 Ailerons #######################################\n",
    "\n",
    "src_idx_ailerons = np.arange(start = 0, stop = (src_size_ailerons - 1), step = 1)\n",
    "tgt_idx_ailerons = np.arange(start = src_size_ailerons, stop = ((src_size_ailerons + tgt_size_ailerons)-1), step = 1)\n",
    "\n",
    "def get_estimator(**kwargs):\n",
    "    return DecisionTreeRegressor(max_depth = 6)\n",
    "\n",
    "kwargs_TwoTrAda = {'steps': 30,\n",
    "                    'fold': 10,\n",
    "                  'learning_rate': 0.1}\n",
    "\n",
    "\n",
    "from adapt.instance_based import TrAdaBoost, TrAdaBoostR2, TwoStageTrAdaBoostR2\n",
    "\n",
    "print(\"Two-Stage TrAdaboost.R2\")\n",
    "print(\"-------------------------------------------\")\n",
    "\n",
    "r2scorelist_TwoTrAda_ailerons = []\n",
    "rmselist_TwoTrAda_ailerons = []\n",
    "\n",
    "for x in range(0, 20):\n",
    "\n",
    "    model_TwoTrAda_ailerons = TwoStageTrAdaBoostR2(get_estimator = get_estimator, n_estimators = 100, cv = 10) \n",
    "    model_TwoTrAda_ailerons.fit(ailerons_np_train_X, ailerons_np_train_y_list, src_idx_ailerons, tgt_idx_ailerons)\n",
    "\n",
    "    y_pred_TwoTrAda_ailerons = model_TwoTrAda_ailerons.predict(ailerons_np_test_X)\n",
    "\n",
    "    mse_TwoTrAda_ailerons = sqrt(mean_squared_error(ailerons_np_test_y, y_pred_TwoTrAda_ailerons))\n",
    "    rmselist_TwoTrAda_ailerons.append(mse_TwoTrAda_ailerons)\n",
    "        \n",
    "    r2_score_TwoTrAda_ailerons = pearsonr(ailerons_np_test_y_list, y_pred_TwoTrAda_ailerons)\n",
    "    r2_score_TwoTrAda_ailerons = (r2_score_TwoTrAda_ailerons[0])**2\n",
    "    r2scorelist_TwoTrAda_ailerons.append(r2_score_TwoTrAda_ailerons)\n",
    "\n",
    "\n",
    "print(\"RMSE of TrAdaboost.R2:\", statistics.mean(rmselist_TwoTrAda_ailerons))\n",
    "print(\"R^2 of TrAdaboost.R2:\", statistics.mean(r2scorelist_TwoTrAda_ailerons))\n",
    "print(\"\\n\")\n",
    "print(\"RMSE of TrAdaboost.R2:\", rmselist_TwoTrAda_ailerons)\n",
    "print(\"R^2 of TrAdaboostR2:\", r2scorelist_TwoTrAda_ailerons)\n",
    "\n",
    "\n",
    "print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### AdaBoostR2 Transfer Learning Ailerons #####################################################\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "print(\"Adaboost.R2 Transfer Learning\")\n",
    "print(\"-------------------------------------------\")\n",
    "\n",
    "r2scorelist_AdaTL_ailerons = []\n",
    "rmselist_AdaTL_ailerons = []\n",
    "\n",
    "for x in range(0, 30):\n",
    "\n",
    "    model_AdaTL_ailerons = AdaBoostRegressor(DecisionTreeRegressor(max_depth = 6), learning_rate = 0.1, n_estimators = 100) \n",
    "    model_AdaTL_ailerons.fit(ailerons_np_train_X, ailerons_np_train_y_list)\n",
    "\n",
    "    y_pred_AdaTL_ailerons = model_AdaTL_ailerons.predict(ailerons_np_test_X)\n",
    "\n",
    "    mse_AdaTL_ailerons = sqrt(mean_squared_error(ailerons_np_test_y, y_pred_AdaTL_ailerons))\n",
    "    rmselist_AdaTL_ailerons.append(mse_AdaTL_ailerons)\n",
    "        \n",
    "    r2_score_AdaTL_ailerons = pearsonr(ailerons_np_test_y_list, y_pred_AdaTL_ailerons)\n",
    "    r2_score_AdaTL_ailerons = (r2_score_AdaTL_ailerons[0])**2\n",
    "    r2scorelist_AdaTL_ailerons.append(r2_score_AdaTL_ailerons)\n",
    "\n",
    "print(\"RMSE of Adaboost.R2:\", statistics.mean(rmselist_AdaTL_ailerons))\n",
    "print(\"R^2 of Adaboost.R2:\", statistics.mean(r2scorelist_AdaTL_ailerons))\n",
    "print(\"\\n\")\n",
    "print(\"RMSE of Adaboost.R2:\", rmselist_AdaTL_ailerons)\n",
    "print(\"R^2 of Adaboost.R2:\", r2scorelist_AdaTL_ailerons)\n",
    "\n",
    "\n",
    "print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### Gradient Boosting Regression Transfer Learning Ailerons #######################################\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "\n",
    "print(\"Gradient Boosting Regression Transfer Learning\")\n",
    "print(\"-------------------------------------------\")\n",
    "\n",
    "r2scorelist_GBRTL_ailerons = []\n",
    "rmselist_GBRTL_ailerons = []\n",
    "\n",
    "for x in range(0, 20):\n",
    "\n",
    "    model_GBRTL_ailerons = GradientBoostingRegressor(learning_rate = 0.1, max_depth = 6, n_estimators = 100)\n",
    "    model_GBRTL_ailerons.fit(ailerons_np_train_X, ailerons_np_train_y_list)\n",
    "\n",
    "    y_pred_GBRTL_ailerons = model_GBRTL_ailerons.predict(AileronsData_df_test_X) ##Using dataframe instead of the numpy matrix\n",
    "\n",
    "    mse_GBRTL_ailerons = sqrt(mean_squared_error(ailerons_np_test_y, y_pred_GBRTL_ailerons))\n",
    "    rmselist_GBRTL_ailerons.append(mse_GBRTL_ailerons)\n",
    "        \n",
    "    r2_score_GBRTL_ailerons = pearsonr(ailerons_np_test_y_list, y_pred_GBRTL_ailerons)\n",
    "    r2_score_GBRTL_ailerons = (r2_score_GBRTL_ailerons[0])**2\n",
    "    r2scorelist_GBRTL_ailerons.append(r2_score_GBRTL_ailerons)\n",
    "\n",
    "    \n",
    "print(\"RMSE of GBRTL:\", statistics.mean(rmselist_GBRTL_ailerons))\n",
    "print(\"R^2 of GBRTL:\", statistics.mean(r2scorelist_GBRTL_ailerons))\n",
    "print(\"\\n\")\n",
    "print(\"RMSE of GBRTL:\", rmselist_GBRTL_ailerons)\n",
    "print(\"R^2 of GBRTL:\", r2scorelist_GBRTL_ailerons)\n",
    "\n",
    "\n",
    "print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elevators Data\n",
      "(8752, 19)\n",
      "(7847, 19)\n",
      "Target Set:  (438, 19)\n",
      "Source Set:  (8314, 19)\n",
      "Test Set:  (7847, 19)\n"
     ]
    }
   ],
   "source": [
    "####################################### Elevators #########################################################\n",
    "target_elevators = ['Goal']\n",
    "colnames_elevators = ['climbRate', 'Sgz', 'p', 'q', 'curRoll', 'absRoll', 'diffClb', 'diffRollRate', 'diffDiffClb', 'SaTime1', 'SaTime2', \n",
    "                      'SaTime3', 'SaTime4', 'diffSaTime1', 'diffSaTime2', 'diffSaTime3', 'diffSaTime4', 'Sa', 'Goal']\n",
    "Elevators_train_df = pd.read_csv('UCI_regression/Elevators/elevators.data', header = None, names = colnames_elevators)\n",
    "print(\"Elevators Data\")\n",
    "print(Elevators_train_df.shape)\n",
    "\n",
    "Elevators_test_df = pd.read_csv('UCI_regression/Elevators/elevators.test', header = None, names = colnames_elevators)\n",
    "print(Elevators_test_df.shape)\n",
    "\n",
    "############################################### Standardization ###############################################\n",
    "elevators_cols = Elevators_train_df.columns.difference(['Goal'])\n",
    "ss = StandardScaler()\n",
    "Elevators_train_df[elevators_cols] = ss.fit_transform(Elevators_train_df[elevators_cols])\n",
    "Elevators_test_df[elevators_cols] = ss.transform(Elevators_test_df[elevators_cols]) ## Use the same scale as the training data\n",
    "\n",
    "#################### Splitting with small target set and large source and test set #############\n",
    "Elevators_source_df, Elevators_tgt_df = train_test_split(Elevators_train_df, test_size = 0.05) ## test_size = tgt size\n",
    "# print(Elevators_df_tgt.shape, Elevators_df_source.shape, Elevators_df_test.shape)\n",
    "\n",
    "Elevators_tgt_df = Elevators_tgt_df.reset_index(drop = True)\n",
    "Elevators_source_df = Elevators_source_df.reset_index(drop = True)\n",
    "print(\"Target Set: \", Elevators_tgt_df.shape)\n",
    "print(\"Source Set: \", Elevators_source_df.shape)\n",
    "print(\"Test Set: \", Elevators_test_df.shape)\n",
    "\n",
    "# ################################# Splitting into predictors and target data #################################\n",
    "# Elevators_test_df_y = Elevators_test_df[target_elevators]\n",
    "# Elevators_test_df_X = Elevators_test_df.drop(target_elevators, axis = 1)\n",
    "\n",
    "# Elevators_tgt_df_y = Elevators_tgt_df[target_elevators]\n",
    "# Elevators_tgt_df_X = Elevators_tgt_df.drop(target_elevators, axis = 1)\n",
    "\n",
    "# Elevators_source_df_y = Elevators_source_df[target_elevators]\n",
    "# Elevators_source_df_X = Elevators_source_df.drop(target_elevators, axis = 1)\n",
    "\n",
    "\n",
    "# ############## Merging the datasets #################\n",
    "# elevators_X_df = pd.concat([Elevators_source_df_X, Elevators_tgt_df_X], ignore_index=True)\n",
    "# elevators_y_df = pd.concat([Elevators_source_df_y, Elevators_tgt_df_y], ignore_index=True)\n",
    "\n",
    "# elevators_np_train_X = elevators_X_df.to_numpy()\n",
    "# elevators_np_train_y = elevators_y_df.to_numpy()\n",
    "\n",
    "# elevators_np_test_X = Elevators_test_df_X.to_numpy()\n",
    "# elevators_np_test_y = Elevators_test_df_y.to_numpy()\n",
    "\n",
    "# elevators_np_train_y_list = elevators_np_train_y.ravel()\n",
    "# elevators_np_test_y_list = elevators_np_test_y.ravel()\n",
    "\n",
    "# src_size_elevators = len(Elevators_source_df_y)\n",
    "# tgt_size_elevators = len(Elevators_tgt_df_y)\n",
    "\n",
    "# print(\"---------------------------\")\n",
    "\n",
    "# ###############################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------\n",
      "Shape of source before : (8314, 19)\n",
      "Shape of source after : (8294, 19)\n",
      "----------------------------------------------\n",
      "Shape of target before : (438, 19)\n",
      "Shape of target after : (458, 19)\n",
      "----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "######################################## Variance Sampling: Phase 1 ###################################################\n",
    "kmeans = KMeans(n_clusters = 20, random_state=0).fit(Elevators_tgt_df)\n",
    "\n",
    "Elevators_alternate_df = Elevators_tgt_df.copy()\n",
    "Elevators_alternate_df_np = Elevators_tgt_df.to_numpy()\n",
    "\n",
    "idxlist = []\n",
    "Elevators_new_df_list = []\n",
    "\n",
    "for rowkm in kmeans.cluster_centers_:\n",
    "    mindist = -99\n",
    "    rowidx = 0\n",
    "    idx = 0\n",
    "    for row in Elevators_alternate_df_np:\n",
    "        dst = distance.euclidean(row, rowkm)\n",
    "\n",
    "        if(dst >= mindist):\n",
    "            mindist = dst\n",
    "            rowidx = idx\n",
    "            rowval = row\n",
    "\n",
    "        idx = idx + 1\n",
    "\n",
    "#     print(\"Row selected: \", rowidx) #Alternate_df.loc[rowidx,:]\\\n",
    "#     print(\"Min. distance: \", mindist)\n",
    "#     print(\"Matrix shape: \", Elevators_alternate_df_np.shape)\n",
    "    Elevators_new_df_list.append(rowval)\n",
    "    Elevators_alternate_df = np.delete(Elevators_alternate_df_np, rowidx, 0)\n",
    "    idxlist.append(rowidx)\n",
    "\n",
    "\n",
    "Elevators_new_df = pd.DataFrame(np.vstack(Elevators_new_df_list))\n",
    "Elevators_new_df.shape\n",
    "\n",
    "##################################################### Variance Sampling: Phase 2 ################################################\n",
    "\n",
    "Elevators_alternate_source_df = Elevators_source_df.copy()\n",
    "Elevators_alternate_source_df_np = Elevators_alternate_source_df.to_numpy()\n",
    "\n",
    "idxlist2 = []\n",
    "Elevators_final_df_list = []\n",
    "\n",
    "for row_nm in Elevators_new_df_list:\n",
    "    min_dist = -99\n",
    "    row_idx = 0\n",
    "    idx_val = 0\n",
    "    for row_alt in Elevators_alternate_df_np:\n",
    "        dst = distance.euclidean(row_alt, row_nm)\n",
    "        if(dst >= mindist):\n",
    "            min_dist = dst\n",
    "            row_idx = idx_val\n",
    "            row_val = row_alt\n",
    "\n",
    "        idx_val = idx_val + 1\n",
    "\n",
    "#     print(\"Row selected: \", row_idx) #Alternate_df.loc[rowidx,:]\\\n",
    "#     print(\"Min. distance: \", min_dist)\n",
    "#     print(\"Matrix shape: \", Elevators_alternate_source_df_np.shape)\n",
    "    Elevators_final_df_list.append(row_val)\n",
    "    Elevators_alternate_source_df_np = np.delete(Elevators_alternate_source_df_np, row_idx, 0)\n",
    "    idxlist2.append(row_idx)\n",
    "\n",
    "\n",
    "Elevators_final_df = pd.DataFrame(np.vstack(Elevators_final_df_list), columns= Elevators_alternate_source_df.columns)\n",
    "\n",
    "print(\"Shape of source before :\",Elevators_source_df.shape)\n",
    "Elevators_source_df = pd.DataFrame(np.vstack(Elevators_alternate_source_df_np), columns= Elevators_source_df.columns)\n",
    "print(\"Shape of source after :\", Elevators_source_df.shape)\n",
    "\n",
    "print(\"----------------------------------------------\")\n",
    "print(\"Shape of target before :\", Elevators_tgt_df.shape)\n",
    "Elevators_tgt_df = pd.concat([Elevators_tgt_df, Elevators_final_df], ignore_index=True)\n",
    "print(\"Shape of target after :\", Elevators_tgt_df.shape)\n",
    "\n",
    "print(\"----------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Set:  (1958, 19)\n",
      "Source Set:  (6794, 19)\n",
      "Test Set:  (7847, 19)\n",
      "---------------------------\n"
     ]
    }
   ],
   "source": [
    "################################## Importance Sampling: Manhattan Distance ######################################################\n",
    "\n",
    "Elevators_source_df[\"ManDis\"] = \"\"\n",
    "\n",
    "Elevators_tgt_df_mean = []\n",
    "prow = Elevators_tgt_df.mean()\n",
    "Elevators_tgt_df_mean = [prow.climbRate, prow.Sgz, prow.p, prow.q, prow.curRoll, prow.absRoll,\n",
    "       prow.diffClb, prow.diffRollRate, prow.diffDiffClb, prow.SaTime1, prow.SaTime2, prow.SaTime3, prow.SaTime4, \n",
    "        prow.diffSaTime1, prow.diffSaTime2, prow.diffSaTime3, prow.diffSaTime4, prow.Sa, prow.Goal]\n",
    "\n",
    "rowidx = 0\n",
    "\n",
    "for row in Elevators_source_df.itertuples():\n",
    "    row_list =[row.climbRate, row.Sgz, row.p, row.q, row.curRoll, row.absRoll, row.diffClb, \n",
    "            row.diffRollRate, row.diffDiffClb, row.SaTime1, row.SaTime2, row.SaTime3, row.SaTime4, \n",
    "               row.diffSaTime1, row.diffSaTime2, row.diffSaTime3, row.diffSaTime4, row.Sa, row.Goal]\n",
    "\n",
    "    man_dis = 0\n",
    "    for i in range(0, len(row_list)):\n",
    "        tempval = Elevators_tgt_df_mean[i] - row_list[i]\n",
    "        man_dis = man_dis + abs(tempval)\n",
    "\n",
    "    Elevators_source_df.loc[rowidx,\"ManDis\"] = man_dis\n",
    "    rowidx = rowidx + 1\n",
    "\n",
    "Elevators_source_df = Elevators_source_df.sort_values(by =['ManDis'])\n",
    "Elevators_tgt_source_df = Elevators_source_df.head(2078) ## 600 instances chosen for Kinematics Dataset\n",
    "Elevators_source_df = Elevators_source_df.iloc[2078:]\n",
    "\n",
    "Elevators_source_df = Elevators_source_df.drop(['ManDis'], axis =1)\n",
    "Elevators_tgt_source_df = Elevators_tgt_source_df.drop(['ManDis'], axis =1)\n",
    "\n",
    "Elevators_tgt_df = pd.concat([Elevators_tgt_df, Elevators_tgt_source_df], ignore_index=True) ### This line is used only for STrAdaBoost.R2 and not for TrAdaBoost.R2\n",
    "\n",
    "Elevators_tgt_df = Elevators_tgt_df.reset_index(drop=True)\n",
    "Elevators_source_df = Elevators_source_df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "#################### Splitting with small target set and large source and test set #############\n",
    "print(\"Target Set: \", Elevators_tgt_df.shape)\n",
    "print(\"Source Set: \", Elevators_source_df.shape)\n",
    "print(\"Test Set: \", Elevators_test_df.shape)\n",
    "\n",
    "\n",
    "Elevators_test_df_y = Elevators_test_df[target_elevators]\n",
    "Elevators_test_df_X = Elevators_test_df.drop(target_elevators, axis = 1)\n",
    "\n",
    "Elevators_tgt_df_y = Elevators_tgt_df[target_elevators]\n",
    "Elevators_tgt_df_X = Elevators_tgt_df.drop(target_elevators, axis = 1)\n",
    "\n",
    "Elevators_source_df_y = Elevators_source_df[target_elevators]\n",
    "Elevators_source_df_X = Elevators_source_df.drop(target_elevators, axis = 1)\n",
    "\n",
    "\n",
    "############## Merging the datasets #################\n",
    "elevators_X_df = pd.concat([Elevators_tgt_df_X, Elevators_source_df_X], ignore_index=True)\n",
    "elevators_y_df = pd.concat([Elevators_tgt_df_y, Elevators_source_df_y], ignore_index=True)\n",
    "\n",
    "elevators_np_train_X = elevators_X_df.to_numpy()\n",
    "elevators_np_train_y = elevators_y_df.to_numpy()\n",
    "\n",
    "elevators_np_test_X = Elevators_tgt_df_X.to_numpy()\n",
    "elevators_np_test_y = Elevators_tgt_df_y.to_numpy()\n",
    "\n",
    "elevators_np_train_y_list = elevators_np_train_y.ravel()\n",
    "elevators_np_test_y_list = elevators_np_test_y.ravel()\n",
    "\n",
    "src_size_elevators = len(Elevators_source_df_y)\n",
    "tgt_size_elevators = len(Elevators_tgt_df_y)\n",
    "\n",
    "print(\"---------------------------\")\n",
    "\n",
    "###############################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################### STrAdaBoost.R2 Active Sampling Elevators ################################################################\n",
    "from two_TrAdaBoostR2 import TwoStageTrAdaBoostR2\n",
    "\n",
    "sample_size = [len(Elevators_tgt_df_X), len(Elevators_source_df_X)]\n",
    "n_estimators = 100\n",
    "steps = 30\n",
    "fold = 10\n",
    "random_state = np.random.RandomState(1)\n",
    "\n",
    "r2scorelist_stradaboost_elevators = []\n",
    "rmselist_stradaboost_elevators = []\n",
    "\n",
    "for x in range(0, 50):\n",
    "\n",
    "    model_stradaboost_elevators = TwoStageTrAdaBoostR2(DecisionTreeRegressor(max_depth=6),\n",
    "                        n_estimators = n_estimators, sample_size = sample_size,\n",
    "                        steps = steps, fold = fold, random_state = random_state)\n",
    "\n",
    "\n",
    "    model_stradaboost_elevators.fit(elevators_np_train_X, elevators_np_train_y_list)\n",
    "\n",
    "\n",
    "    y_pred_stradaboost_elevators = model_stradaboost_elevators.predict(elevators_np_test_X)\n",
    "\n",
    "    mse_stradaboost_elevators = sqrt(mean_squared_error(elevators_np_test_y, y_pred_stradaboost_elevators))\n",
    "    rmselist_stradaboost_elevators.append(mse_stradaboost_elevators)\n",
    "        \n",
    "    r2_score_stradaboost_elevators = pearsonr(elevators_np_test_y_list, y_pred_stradaboost_elevators)\n",
    "    r2_score_stradaboost_elevators = (r2_score_stradaboost_elevators[0])**2\n",
    "    r2scorelist_stradaboost_elevators.append(r2_score_stradaboost_elevators)\n",
    "\n",
    "\n",
    "print(\"RMSE of STrAdaboostR2:\", statistics.mean(rmselist_stradaboost_elevators))\n",
    "print(\"R^2 of STrAdaboostR2:\", statistics.mean(r2scorelist_stradaboost_elevators))\n",
    "print(\"\\n\")\n",
    "print(\"RMSE of STrAdaboostR2:\", rmselist_stradaboost_elevators)\n",
    "print(\"R^2 of STrAdaboostR2:\", r2scorelist_stradaboost_elevators)\n",
    "\n",
    "\n",
    "print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### TwoStagetrAdaBoostR2 Elevators #######################################\n",
    "from adapt.instance_based import TrAdaBoost, TrAdaBoostR2, TwoStageTrAdaBoostR2\n",
    "\n",
    "############# Transfer Learning specifications #########################\n",
    "src_idx_elevators = np.arange(start=0, stop=(src_size_elevators - 1), step=1)\n",
    "tgt_idx_elevators = np.arange(start=src_size_elevators, stop=((src_size_elevators + tgt_size_elevators)-1), step=1)\n",
    "\n",
    "def get_estimator(**kwargs):\n",
    "    return DecisionTreeRegressor(max_depth = 6)\n",
    "\n",
    "kwargs_TwoTrAda = {'steps': 30,\n",
    "                    'fold': 10,\n",
    "                  'learning_rate': 0.1}\n",
    "\n",
    "\n",
    "\n",
    "print(\"Two-Stage TrAdaboost.R2\")\n",
    "print(\"-------------------------------------------\")\n",
    "\n",
    "r2scorelist_TwoTrAda_elevators = []\n",
    "rmselist_TwoTrAda_elevators = []\n",
    "\n",
    "for x in range(0, 10):\n",
    "\n",
    "    model_TwoTrAda_elevators = TwoStageTrAdaBoostR2(get_estimator = get_estimator, n_estimators = 100, cv = 10) \n",
    "    model_TwoTrAda_elevators.fit(elevators_np_train_X, elevators_np_train_y_list, src_idx_elevators, tgt_idx_elevators)\n",
    "\n",
    "    y_pred_TwoTrAda_elevators = model_TwoTrAda_elevators.predict(elevators_np_test_X)\n",
    "\n",
    "    mse_TwoTrAda_elevators = sqrt(mean_squared_error(elevators_np_test_y, y_pred_TwoTrAda_elevators))\n",
    "    rmselist_TwoTrAda_elevators.append(mse_TwoTrAda_elevators)\n",
    "        \n",
    "    r2_score_TwoTrAda_elevators = pearsonr(elevators_np_test_y_list, y_pred_TwoTrAda_elevators)\n",
    "    r2_score_TwoTrAda_elevators = (r2_score_TwoTrAda_elevators[0])**2\n",
    "\n",
    "    r2scorelist_TwoTrAda_elevators.append(r2_score_TwoTrAda_elevators)\n",
    "\n",
    "\n",
    "print(\"RMSE of TrAdaboost.R2:\", statistics.mean(rmselist_TwoTrAda_elevators))\n",
    "print(\"R^2 of TrAdaboostR2:\", statistics.mean(r2scorelist_TwoTrAda_elevators))\n",
    "print(\"\\n\")\n",
    "print(\"RMSE of TrAdaboost.R2:\", rmselist_TwoTrAda_elevators)\n",
    "print(\"R^2 of TrAdaboostR2:\", r2scorelist_TwoTrAda_elevators)\n",
    "\n",
    "\n",
    "print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### Gradient Boosting Regression Transfer Learning Elevators #######################################\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "\n",
    "print(\"Gradient Boosting Regression Transfer Learning\")\n",
    "print(\"-------------------------------------------\")\n",
    "\n",
    "r2scorelist_GBRTL_elevators = []\n",
    "rmselist_GBRTL_elevators = []\n",
    "\n",
    "for x in range(0, 10):\n",
    "\n",
    "    model_GBRTL_elevators = GradientBoostingRegressor(learning_rate = 0.1, max_depth = 6, n_estimators = 100)\n",
    "    model_GBRTL_elevators.fit(elevators_np_train_X, elevators_np_train_y_list)\n",
    "\n",
    "    y_pred_GBRTL_elevators = model_GBRTL_elevators.predict(elevators_np_test_X) ##Using dataframe instead of the numpy matrix\n",
    "\n",
    "    mse_GBRTL_elevators = sqrt(mean_squared_error(elevators_np_test_y, y_pred_GBRTL_elevators))\n",
    "    rmselist_GBRTL_elevators.append(mse_GBRTL_elevators)\n",
    "        \n",
    "    r2_score_GBRTL_elevators = pearsonr(elevators_np_test_y_list, y_pred_GBRTL_elevators)\n",
    "    r2_score_GBRTL_elevators = (r2_score_GBRTL_elevators[0])**2\n",
    "    r2scorelist_GBRTL_elevators.append(r2_score_GBRTL_elevators)\n",
    "\n",
    "\n",
    "print(\"RMSE of GBRTL:\", statistics.mean(rmselist_GBRTL_elevators))\n",
    "print(\"R^2 of GBRTL:\", statistics.mean(r2scorelist_GBRTL_elevators))\n",
    "print(\"\\n\")\n",
    "print(\"RMSE of GBRTL:\", rmselist_GBRTL_elevators)\n",
    "print(\"R^2 of GBRTL:\", r2scorelist_GBRTL_elevators)\n",
    "\n",
    "\n",
    "print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abalone Data\n",
      "(4177, 9)\n",
      "Target Set:  (1241, 8)\n",
      "Source Set 1:  (1465, 8)\n",
      "Source Set 2:  (1471, 8)\n",
      "Final Source Set:  (2936, 8)\n"
     ]
    }
   ],
   "source": [
    "################################### Abalone ###########################################\n",
    "#### range: [0.0 - 1.130]\n",
    "#### Mid of correlation variable: Whole_weight\n",
    "#### [0, 0.12] [0.12, 0.15], [0.15, 1.130]\n",
    "#### Target variable: Rings\n",
    "#######################################################################################\n",
    "target_var_abalone = ['Rings']\n",
    "colnames_abalone = ['Sex', 'Length', 'Diameter', 'Height', 'Whole_weight', 'Shucked_weight', 'Viscera_weight', 'Shell_weight', 'Rings']\n",
    "AbaloneData_df = pd.read_csv('UCI_regression/Abalone/abalone.data', header = None, names = colnames_abalone)\n",
    "\n",
    "gender = {'M': 1,'F': 2, 'I': 3} \n",
    "AbaloneData_df.Sex = [gender[item] for item in AbaloneData_df.Sex] \n",
    "\n",
    "print(\"Abalone Data\")\n",
    "print(AbaloneData_df.shape)\n",
    "\n",
    "########## Corr Abalone ################\n",
    "# print(\"The correlation matrix is: \")\n",
    "# print(AbaloneData_df.corr()['Rings'])\n",
    "\n",
    "##################### Splitting in 3 equal parts #######################################\n",
    "# print(AbaloneData_df.sort_values('Whole_weight')['Whole_weight'])\n",
    "# print(len(AbaloneData_df.loc[(AbaloneData_df['Whole_weight'] <= 0.5)]))\n",
    "# print(len(AbaloneData_df.loc[(AbaloneData_df['Whole_weight'] > 0.5) & (AbaloneData_df['Whole_weight'] <= 1.0)]))\n",
    "# print(len(AbaloneData_df.loc[(AbaloneData_df['Whole_weight'] > 1.0)]))\n",
    "\n",
    "#################### Splitting with small target set and large source and test set #############\n",
    "# AbaloneData_df_remain, AbaloneData_df_tgt = train_test_split(AbaloneData_df, test_size = 0.05) ## test_size = tgt size\n",
    "# AbaloneData_df_source, AbaloneData_df_test = train_test_split(AbaloneData_df_remain, test_size = 0.3) ## test_size = tgt size\n",
    "# print(AbaloneData_df_tgt.shape, AbaloneData_df_source.shape, AbaloneData_df_test.shape)\n",
    "\n",
    "\n",
    "drop_col_abalone = ['Whole_weight']\n",
    "\n",
    "abalone_tgt_df = AbaloneData_df.loc[(AbaloneData_df['Whole_weight'] <= 0.5)]\n",
    "abalone_tgt_df = abalone_tgt_df.drop(drop_col_abalone, axis = 1)\n",
    "abalone_tgt_df = abalone_tgt_df.reset_index(drop=True)\n",
    "print(\"Target Set: \",abalone_tgt_df.shape)\n",
    "\n",
    "abalone_source1_df = AbaloneData_df.loc[(AbaloneData_df['Whole_weight'] > 0.5) & (AbaloneData_df['Whole_weight'] <= 1.0)]\n",
    "abalone_source1_df = abalone_source1_df.drop(drop_col_abalone, axis = 1)\n",
    "abalone_source1_df = abalone_source1_df.reset_index(drop=True)\n",
    "print(\"Source Set 1: \",abalone_source1_df.shape)\n",
    "\n",
    "abalone_source2_df = AbaloneData_df.loc[(AbaloneData_df['Whole_weight'] > 1.0)]\n",
    "abalone_source2_df = abalone_source2_df.drop(drop_col_abalone, axis = 1)\n",
    "abalone_source2_df = abalone_source2_df.reset_index(drop=True)\n",
    "print(\"Source Set 2: \",abalone_source2_df.shape)\n",
    "\n",
    "\n",
    "################################# Standardization #################################\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "abalone_cols = abalone_tgt_df.columns.difference(['Rings'])\n",
    "\n",
    "ss = StandardScaler()\n",
    "abalone_tgt_df[abalone_cols] = ss.fit_transform(abalone_tgt_df[abalone_cols])\n",
    "abalone_source1_df[abalone_cols] = ss.fit_transform(abalone_source1_df[abalone_cols])\n",
    "abalone_source2_df[abalone_cols] = ss.fit_transform(abalone_source2_df[abalone_cols])\n",
    "\n",
    "\n",
    "################################# Concatenating the source datasets #################################\n",
    "abalone_source_df = pd.concat([abalone_source1_df, abalone_source2_df], ignore_index = True)\n",
    "abalone_source_df = abalone_source_df.reset_index(drop = True)\n",
    "print(\"Final Source Set: \",abalone_source_df.shape)\n",
    "\n",
    "\n",
    "# #################### Splitting into features and target ####################\n",
    "# target_abalone = ['Rings']\n",
    "\n",
    "# abalone_source_df_y = abalone_source_df[target_abalone]\n",
    "# abalone_source_df_X = abalone_source_df.drop(target_abalone, axis = 1)\n",
    "\n",
    "# features_abalone = abalone_source_df_X.columns\n",
    "# print(\"---------------------------\")\n",
    "###############################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the extracted data is: \n",
      "(60, 8)\n",
      "-------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "######################################## Variance Sampling: Phase 1 ###################################################\n",
    "kmeans = KMeans(n_clusters = 60, random_state=0).fit(abalone_tgt_df)\n",
    "\n",
    "abalone_alternate_df = abalone_tgt_df.copy()\n",
    "abalone_alternate_df_np = abalone_tgt_df.to_numpy()\n",
    "\n",
    "idxlist = []\n",
    "abalone_new_df_list = []\n",
    "\n",
    "for rowkm in kmeans.cluster_centers_:\n",
    "    mindist = -99\n",
    "    rowidx = 0\n",
    "    idx = 0\n",
    "    for row in abalone_alternate_df_np:\n",
    "        dst = distance.euclidean(row, rowkm)\n",
    "\n",
    "        if(dst >= mindist):\n",
    "            mindist = dst\n",
    "            rowidx = idx\n",
    "            rowval = row\n",
    "\n",
    "        idx = idx + 1\n",
    "\n",
    "#     print(\"Row selected: \", rowidx) #Alternate_df.loc[rowidx,:]\\\n",
    "#     print(\"Min. distance: \", mindist)\n",
    "#     print(\"Matrix shape: \", abalone_alternate_df_np.shape)\n",
    "    abalone_new_df_list.append(rowval)\n",
    "    abalone_alternate_df = np.delete(abalone_alternate_df_np, rowidx, 0)\n",
    "    idxlist.append(rowidx)\n",
    "\n",
    "\n",
    "abalone_new_df = pd.DataFrame(np.vstack(abalone_new_df_list))\n",
    "\n",
    "print(\"The shape of the extracted data is: \")\n",
    "print(abalone_new_df.shape)\n",
    "print(\"-------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of source before : (2936, 8)\n",
      "Shape of source after : (2876, 8)\n",
      "----------------------------------------------\n",
      "Shape of target before : (1241, 8)\n",
      "Shape of target after : (1301, 8)\n",
      "----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "##################################################### Variance Sampling: Phase 2 ################################################\n",
    "\n",
    "abalone_alternate_source_df = abalone_source_df.copy()\n",
    "abalone_alternate_source_df_np = abalone_alternate_source_df.to_numpy()\n",
    "\n",
    "idxlist2 = []\n",
    "abalone_final_df_list = []\n",
    "\n",
    "for row_nm in abalone_new_df_list:\n",
    "    min_dist = -99\n",
    "    row_idx = 0\n",
    "    idx_val = 0\n",
    "    for row_alt in abalone_alternate_source_df_np:\n",
    "        dst = distance.euclidean(row_alt, row_nm)\n",
    "        if(dst >= mindist):\n",
    "            min_dist = dst\n",
    "            row_idx = idx_val\n",
    "            row_val = row_alt\n",
    "\n",
    "        idx_val = idx_val + 1\n",
    "\n",
    "#     print(\"Row selected: \", row_idx) #Alternate_df.loc[rowidx,:]\\\n",
    "#     print(\"Min. distance: \", min_dist)\n",
    "#     print(\"Matrix shape: \", abalone_alternate_source_df_np.shape)\n",
    "    abalone_final_df_list.append(row_val)\n",
    "    abalone_alternate_source_df_np = np.delete(abalone_alternate_source_df_np, row_idx, 0)\n",
    "    idxlist2.append(row_idx)\n",
    "\n",
    "\n",
    "abalone_final_df = pd.DataFrame(np.vstack(abalone_final_df_list), columns= abalone_alternate_source_df.columns)\n",
    "\n",
    "print(\"Shape of source before :\",abalone_source_df.shape)\n",
    "abalone_source_df = pd.DataFrame(np.vstack(abalone_alternate_source_df_np), columns= abalone_source_df.columns)\n",
    "print(\"Shape of source after :\", abalone_source_df.shape)\n",
    "\n",
    "print(\"----------------------------------------------\")\n",
    "print(\"Shape of target before :\", abalone_tgt_df.shape)\n",
    "abalone_tgt_df = pd.concat([abalone_tgt_df, abalone_final_df], ignore_index=True)\n",
    "print(\"Shape of target after :\", abalone_tgt_df.shape)\n",
    "\n",
    "print(\"----------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Set:  (1301, 8)\n",
      "Source Set:  (600, 8)\n",
      "---------------------------\n"
     ]
    }
   ],
   "source": [
    "################################## Importance Sampling: Manhattan Distance ######################################################\n",
    "\n",
    "abalone_source_df[\"ManDis\"] = \"\"\n",
    "\n",
    "abalone_tgt_df_mean = []\n",
    "prow = abalone_tgt_df.mean()\n",
    "abalone_tgt_df_mean = [prow.Sex, prow.Length, prow.Diameter, prow.Height, prow.Shucked_weight,\n",
    "       prow.Viscera_weight, prow.Shell_weight, prow.Rings]\n",
    "\n",
    "rowidx = 0\n",
    "\n",
    "for row in abalone_source_df.itertuples():\n",
    "    row_list =[row.Sex, row.Length, row.Diameter, row.Height, row.Shucked_weight,\n",
    "       row.Viscera_weight, row.Shell_weight, row.Rings]\n",
    "\n",
    "    man_dis = 0\n",
    "    for i in range(0, len(row_list)):\n",
    "        tempval = abalone_tgt_df_mean[i] - row_list[i]\n",
    "        man_dis = man_dis + abs(tempval)\n",
    "\n",
    "    abalone_source_df.loc[rowidx,\"ManDis\"] = man_dis\n",
    "    rowidx = rowidx + 1\n",
    "\n",
    "abalone_source_df = abalone_source_df.sort_values(by =['ManDis'])\n",
    "abalone_source_df = abalone_source_df.head(734) \n",
    "abalone_source_df = abalone_source_df.drop(['ManDis'], axis =1)\n",
    "abalone_source_df = abalone_source_df.reset_index(drop=True)\n",
    "\n",
    "print(\"Target Set: \", abalone_tgt_df.shape)\n",
    "print(\"Source Set: \", abalone_source_df.shape)\n",
    "\n",
    "#################### Splitting into features and target ####################\n",
    "target_abalone = ['Rings']\n",
    "\n",
    "abalone_source_df_y = abalone_source_df[target_abalone]\n",
    "abalone_source_df_X = abalone_source_df.drop(target_abalone, axis = 1)\n",
    "\n",
    "features_abalone = abalone_source_df_X.columns\n",
    "print(\"---------------------------\")\n",
    "\n",
    "###############################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside Updated Sampling TrAdaBoost.R2\n",
      "Inside Updated Sampling TrAdaBoost.R2\n",
      "Inside Updated Sampling TrAdaBoost.R2\n",
      "Inside Updated Sampling TrAdaBoost.R2\n",
      "Inside Updated Sampling TrAdaBoost.R2\n",
      "Inside Updated Sampling TrAdaBoost.R2\n",
      "Inside Updated Sampling TrAdaBoost.R2\n",
      "Inside Updated Sampling TrAdaBoost.R2\n",
      "Inside Updated Sampling TrAdaBoost.R2\n",
      "Inside Updated Sampling TrAdaBoost.R2\n",
      "Inside Updated Sampling TrAdaBoost.R2\n",
      "Inside Updated Sampling TrAdaBoost.R2\n",
      "Inside Updated Sampling TrAdaBoost.R2\n",
      "Inside Updated Sampling TrAdaBoost.R2\n",
      "Inside Updated Sampling TrAdaBoost.R2\n",
      "Inside Updated Sampling TrAdaBoost.R2\n",
      "Inside Updated Sampling TrAdaBoost.R2\n",
      "Inside Updated Sampling TrAdaBoost.R2\n",
      "Inside Updated Sampling TrAdaBoost.R2\n",
      "Inside Updated Sampling TrAdaBoost.R2\n",
      "RMSE List of STrAdaboostR2: [1.281865027657627, 1.8146741902949843, 1.8009572244842176, 3.2285298785186707, 1.4732504387053216, 1.6458883209356732, 1.3570674412195836, 1.476452589118703, 1.4631428949717606, 1.214905729724244, 1.564020050989506, 1.9459648913508276, 2.364200984987854, 1.2709122799837094, 1.2533304085942105, 1.5242285257850827, 1.9682612608108598, 1.3652531735150673, 1.9551119761893565, 1.2967832554203216]\n",
      "R^2 List of STrAdaboostR2: [0.6044125879736194, 0.5238982048102034, 0.5765556965122487, 0.21545170553772536, 0.6724827826291215, 0.20847020651518167, 0.5551988245117113, 0.2989985655816542, 0.2586406079880177, 0.4470291162026031, 0.4818783935446421, 0.3695579059563178, 0.46109945791578166, 0.5224123633871007, 0.5383588625308598, 0.5276104610188661, 0.557478110302173, 0.507050497573616, 0.3499127326725291, 0.42554891873719136]\n",
      "\n",
      "\n",
      "Mean, STDev of RMSE: 1.663240027162879 0.4794326504330879\n",
      "Mean, STDev of R^2: 0.4551023000950582 0.13183915061240706\n"
     ]
    }
   ],
   "source": [
    "#################################### STrAdaBoost.R2 Abalone ################################################################\n",
    "from two_TrAdaBoostR2 import TwoStageTrAdaBoostR2\n",
    "\n",
    "n_estimators = 100\n",
    "steps = 30\n",
    "fold = 10\n",
    "random_state = np.random.RandomState(1)\n",
    "\n",
    "r2scorelist_stradaboost_abalone = []\n",
    "rmselist_stradaboost_abalone = []\n",
    "\n",
    "kf = KFold(n_splits = 20)\n",
    "\n",
    "for train_idx, test_idx in kf.split(abalone_tgt_df):\n",
    "        \n",
    "    abalone_train_df_X = abalone_tgt_df.iloc[train_idx].loc[:, features_abalone]\n",
    "    abalone_test_df_X = abalone_tgt_df.iloc[test_idx][features_abalone]\n",
    "    abalone_train_df_y = abalone_tgt_df.iloc[train_idx].loc[:,target_abalone]\n",
    "    abalone_test_df_y = abalone_tgt_df.loc[test_idx][target_abalone] \n",
    "    \n",
    "    abalone_X_df = pd.concat([abalone_source_df_X, abalone_train_df_X], ignore_index=True)\n",
    "    abalone_y_df = pd.concat([abalone_source_df_y, abalone_train_df_y], ignore_index=True)\n",
    "\n",
    "    abalone_np_train_X = abalone_X_df.to_numpy()\n",
    "    abalone_np_train_y = abalone_y_df.to_numpy()\n",
    "\n",
    "    abalone_np_test_X = abalone_test_df_X.to_numpy()\n",
    "    abalone_np_test_y = abalone_test_df_y.to_numpy()\n",
    "\n",
    "    abalone_np_train_y_list = abalone_np_train_y.ravel()\n",
    "    abalone_np_test_y_list = abalone_np_test_y.ravel()\n",
    "    \n",
    "    sample_size = [len(abalone_source_df_X), len(abalone_train_df_X)]\n",
    "\n",
    "    model_stradaboost_abalone = TwoStageTrAdaBoostR2(DecisionTreeRegressor(max_depth=6),\n",
    "                        n_estimators = n_estimators, sample_size = sample_size,\n",
    "                        steps = steps, fold = fold, random_state = random_state)\n",
    "\n",
    "\n",
    "    model_stradaboost_abalone.fit(abalone_np_train_X, abalone_np_train_y_list)\n",
    "    y_pred_stradaboost_abalone = model_stradaboost_abalone.predict(abalone_np_test_X)\n",
    "\n",
    "    mse_stradaboost_abalone = sqrt(mean_squared_error(abalone_np_test_y, y_pred_stradaboost_abalone))\n",
    "    rmselist_stradaboost_abalone.append(mse_stradaboost_abalone)\n",
    "        \n",
    "    r2_score_stradaboost_abalone = pearsonr(abalone_np_test_y_list, y_pred_stradaboost_abalone)\n",
    "    r2_score_stradaboost_abalone = (r2_score_stradaboost_abalone[0])**2\n",
    "    r2scorelist_stradaboost_abalone.append(r2_score_stradaboost_abalone)\n",
    "\n",
    "print(\"RMSE List of STrAdaboostR2:\", rmselist_stradaboost_abalone)\n",
    "print(\"R^2 List of STrAdaboostR2:\", r2scorelist_stradaboost_abalone)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Mean, STDev of RMSE:\", statistics.mean(rmselist_stradaboost_abalone), statistics.stdev(rmselist_stradaboost_abalone))\n",
    "print(\"Mean, STDev of R^2:\", statistics.mean(r2scorelist_stradaboost_abalone), statistics.stdev(r2scorelist_stradaboost_abalone))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two-Stage TrAdaboost.R2\n",
      "-------------------------------------------\n",
      "RMSE List of TrAdaboost.R2: [2.3358154162536287, 2.6661149965277686, 2.9700755341610456, 3.2989531191802075, 2.2826498024306305, 2.0970523775620404, 2.0177380256371706, 1.8891152313622421, 2.752497388381207, 2.0542435141488022, 2.439907047558311, 2.835558420174797, 2.025763138898031, 1.9647481501725805, 2.344466033777315, 2.5560365813518255, 2.3600544608585694, 1.978096310462208, 2.646185453086457, 2.025808493314089]\n",
      "R^2 List of TrAdaboost.R2: [0.49319968046465007, 0.5379373907098207, 0.46260967123422536, 0.4867972870526036, 0.6080447463524427, 0.3023703738787406, 0.42381633136776686, 0.4574925001043787, 0.3289007157474186, 0.4268819720280001, 0.5229254826839679, 0.3240890792733061, 0.6913370705969084, 0.4550277813086461, 0.4128672587159948, 0.4264490187979004, 0.5020003908303805, 0.5097270723177003, 0.4088837015973851, 0.4647667318364844]\n",
      "\n",
      "\n",
      "Mean, Stdev of RMSE: 2.3770439747649466 0.3894959849588865\n",
      "Mean, Stdev of R^2: 0.46230621284493606 0.09186696319077892\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "########################### TwoStagetrAdaBoostR2 Abalone #######################################\n",
    "\n",
    "def get_estimator(**kwargs):\n",
    "    return DecisionTreeRegressor(max_depth = 6)\n",
    "\n",
    "kwargs_TwoTrAda = {'steps': 30,\n",
    "                    'fold': 10,\n",
    "                  'learning_rate': 0.1}\n",
    "\n",
    "#####################################################################################################################\n",
    "\n",
    "from adapt.instance_based import TrAdaBoost, TrAdaBoostR2, TwoStageTrAdaBoostR2\n",
    "\n",
    "print(\"Two-Stage TrAdaboost.R2\")\n",
    "print(\"-------------------------------------------\")\n",
    "\n",
    "r2scorelist_TwoTrAda_abalone = []\n",
    "rmselist_TwoTrAda_abalone = []\n",
    "\n",
    "kf = KFold(n_splits = 20)\n",
    "\n",
    "for train_idx, test_idx in kf.split(abalone_tgt_df):\n",
    "        \n",
    "    abalone_train_df_X = abalone_tgt_df.iloc[train_idx].loc[:, features_abalone]\n",
    "    abalone_test_df_X = abalone_tgt_df.iloc[test_idx][features_abalone]\n",
    "    abalone_train_df_y = abalone_tgt_df.iloc[train_idx].loc[:,target_abalone]\n",
    "    abalone_test_df_y = abalone_tgt_df.loc[test_idx][target_abalone] \n",
    "    \n",
    "    abalone_X_df = pd.concat([abalone_source_df_X, abalone_train_df_X], ignore_index=True)\n",
    "    abalone_y_df = pd.concat([abalone_source_df_y, abalone_train_df_y], ignore_index=True)\n",
    "\n",
    "    abalone_np_train_X = abalone_X_df.to_numpy()\n",
    "    abalone_np_train_y = abalone_y_df.to_numpy()\n",
    "\n",
    "    abalone_np_test_X = abalone_test_df_X.to_numpy()\n",
    "    abalone_np_test_y = abalone_test_df_y.to_numpy()\n",
    "\n",
    "    abalone_np_train_y_list = abalone_np_train_y.ravel()\n",
    "    abalone_np_test_y_list = abalone_np_test_y.ravel()\n",
    "\n",
    "    src_size_abalone = len(abalone_source_df_y)\n",
    "    tgt_size_abalone = len(abalone_train_df_y)\n",
    "    \n",
    "    src_idx_abalone = np.arange(start = 0, stop = (src_size_abalone - 1), step = 1)\n",
    "    tgt_idx_abalone = np.arange(start = src_size_abalone, stop = ((src_size_abalone + tgt_size_abalone)-1), step=1)\n",
    "\n",
    "\n",
    "    model_TwoTrAda_abalone = TwoStageTrAdaBoostR2(get_estimator = get_estimator, n_estimators = 100) \n",
    "    model_TwoTrAda_abalone.fit(abalone_np_train_X, abalone_np_train_y_list, src_idx_abalone, tgt_idx_abalone)\n",
    "\n",
    "    y_pred_TwoTrAda_abalone = model_TwoTrAda_abalone.predict(abalone_np_test_X)\n",
    "\n",
    "    mse_TwoTrAda_abalone = sqrt(mean_squared_error(abalone_np_test_y, y_pred_TwoTrAda_abalone))\n",
    "    rmselist_TwoTrAda_abalone.append(mse_TwoTrAda_abalone)\n",
    "        \n",
    "    r2_score_TwoTrAda_abalone = pearsonr(abalone_np_test_y_list, y_pred_TwoTrAda_abalone)\n",
    "    r2_score_TwoTrAda_abalone = (r2_score_TwoTrAda_abalone[0])**2\n",
    "    r2scorelist_TwoTrAda_abalone.append(r2_score_TwoTrAda_abalone)\n",
    "\n",
    "print(\"RMSE List of TrAdaboost.R2:\", rmselist_TwoTrAda_abalone)\n",
    "print(\"R^2 List of TrAdaboost.R2:\", r2scorelist_TwoTrAda_abalone)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "print(\"Mean, Stdev of RMSE:\", statistics.mean(rmselist_TwoTrAda_abalone), statistics.stdev(rmselist_TwoTrAda_abalone))\n",
    "print(\"Mean, Stdev of R^2:\", statistics.mean(r2scorelist_TwoTrAda_abalone), statistics.stdev(r2scorelist_TwoTrAda_abalone))\n",
    "\n",
    "\n",
    "print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaboost.R2 Transfer Learning\n",
      "-------------------------------------------\n",
      "RMSE List of Adaboost.R2 TL: [1.2504247494019327, 1.7657249668037305, 1.89569903377933, 3.2532755632102206, 1.4936185898283525, 1.4070485067715626, 1.2780691126409347, 1.2952120111369823, 1.2185132569760504, 1.1664952928318673, 1.4431519085733768, 1.9500910194428454, 2.363560140825891, 1.1378988286910905, 1.1343452679535926, 1.511106599638056, 1.8988885837677354, 1.1572423882973184, 1.8723490916485794, 1.0998007831649441]\n",
      "R^2 List of Adaboost.R2 TL: [0.5901314864496952, 0.5232221797175364, 0.5437723857569366, 0.23756770243906197, 0.6683812336475996, 0.23301984026064682, 0.6077878100647153, 0.352826224818253, 0.2830390415370179, 0.4396294459296463, 0.4683668644342236, 0.3649472972652102, 0.49633198407516443, 0.4752250341956109, 0.5698057437152155, 0.5273507252768682, 0.635721124383488, 0.5860177748898414, 0.3893848276174636, 0.4454354917910456]\n",
      "\n",
      "\n",
      "Mean, Stdev of RMSE: 1.5796257847692197 0.5290940312036921\n",
      "Mean, Stdev of R^2: 0.471898210913262 0.12851406705226057\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "########################### AdaBoostR2 Transfer Learning Abalone #####################################################\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "\n",
    "print(\"Adaboost.R2 Transfer Learning\")\n",
    "print(\"-------------------------------------------\")\n",
    "\n",
    "r2scorelist_AdaTr_abalone = []\n",
    "rmselist_AdaTr_abalone = []\n",
    "\n",
    "kf = KFold(n_splits = 20)\n",
    "\n",
    "for train_idx, test_idx in kf.split(abalone_tgt_df):\n",
    "        \n",
    "    abalone_train_df_X = abalone_tgt_df.iloc[train_idx].loc[:, features_abalone]\n",
    "    abalone_test_df_X = abalone_tgt_df.iloc[test_idx][features_abalone]\n",
    "    abalone_train_df_y = abalone_tgt_df.iloc[train_idx].loc[:,target_abalone]\n",
    "    abalone_test_df_y = abalone_tgt_df.loc[test_idx][target_abalone] \n",
    "    \n",
    "    abalone_X_df = pd.concat([abalone_source_df_X, abalone_train_df_X], ignore_index=True)\n",
    "    abalone_y_df = pd.concat([abalone_source_df_y, abalone_train_df_y], ignore_index=True)\n",
    "\n",
    "    abalone_np_train_X = abalone_X_df.to_numpy()\n",
    "    abalone_np_train_y = abalone_y_df.to_numpy()\n",
    "\n",
    "    abalone_np_test_X = abalone_test_df_X.to_numpy()\n",
    "    abalone_np_test_y = abalone_test_df_y.to_numpy()\n",
    "\n",
    "    abalone_np_train_y_list = abalone_np_train_y.ravel()\n",
    "    abalone_np_test_y_list = abalone_np_test_y.ravel()\n",
    "\n",
    "\n",
    "    model_AdaTr_abalone = AdaBoostRegressor(DecisionTreeRegressor(max_depth = 6), learning_rate = 0.1, n_estimators = 100) \n",
    "    model_AdaTr_abalone.fit(abalone_np_train_X, abalone_np_train_y_list)\n",
    "\n",
    "    y_pred_AdaTr_abalone = model_AdaTr_abalone.predict(abalone_np_test_X)\n",
    "\n",
    "    mse_AdaTr_abalone = sqrt(mean_squared_error(abalone_np_test_y, y_pred_AdaTr_abalone))\n",
    "    rmselist_AdaTr_abalone.append(mse_AdaTr_abalone)\n",
    "        \n",
    "    r2_score_AdaTr_abalone = pearsonr(abalone_np_test_y_list, y_pred_AdaTr_abalone)\n",
    "    r2_score_AdaTr_abalone = (r2_score_AdaTr_abalone[0])**2\n",
    "    r2scorelist_AdaTr_abalone.append(r2_score_AdaTr_abalone)\n",
    "\n",
    "print(\"RMSE List of Adaboost.R2 TL:\", rmselist_AdaTr_abalone)\n",
    "print(\"R^2 List of Adaboost.R2 TL:\", r2scorelist_AdaTr_abalone)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "print(\"Mean, Stdev of RMSE:\", statistics.mean(rmselist_AdaTr_abalone), statistics.stdev(rmselist_AdaTr_abalone))\n",
    "print(\"Mean, Stdev of R^2:\", statistics.mean(r2scorelist_AdaTr_abalone), statistics.stdev(r2scorelist_AdaTr_abalone))\n",
    "\n",
    "\n",
    "print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Regression Transfer Learning\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shrey/anaconda3/lib/python3.8/site-packages/sklearn/base.py:434: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/shrey/anaconda3/lib/python3.8/site-packages/sklearn/base.py:434: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/shrey/anaconda3/lib/python3.8/site-packages/sklearn/base.py:434: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/shrey/anaconda3/lib/python3.8/site-packages/sklearn/base.py:434: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/shrey/anaconda3/lib/python3.8/site-packages/sklearn/base.py:434: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/shrey/anaconda3/lib/python3.8/site-packages/sklearn/base.py:434: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/shrey/anaconda3/lib/python3.8/site-packages/sklearn/base.py:434: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/shrey/anaconda3/lib/python3.8/site-packages/sklearn/base.py:434: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/shrey/anaconda3/lib/python3.8/site-packages/sklearn/base.py:434: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/shrey/anaconda3/lib/python3.8/site-packages/sklearn/base.py:434: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/shrey/anaconda3/lib/python3.8/site-packages/sklearn/base.py:434: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/shrey/anaconda3/lib/python3.8/site-packages/sklearn/base.py:434: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/shrey/anaconda3/lib/python3.8/site-packages/sklearn/base.py:434: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/shrey/anaconda3/lib/python3.8/site-packages/sklearn/base.py:434: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/shrey/anaconda3/lib/python3.8/site-packages/sklearn/base.py:434: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/shrey/anaconda3/lib/python3.8/site-packages/sklearn/base.py:434: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/shrey/anaconda3/lib/python3.8/site-packages/sklearn/base.py:434: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/shrey/anaconda3/lib/python3.8/site-packages/sklearn/base.py:434: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/shrey/anaconda3/lib/python3.8/site-packages/sklearn/base.py:434: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE List of GBRTL: [1.396411605443678, 1.8378740199429298, 1.8271455594905333, 3.217428256994164, 1.6290472225941706, 1.2236063526759622, 1.2521234275778645, 1.373137309203999, 1.2843716646082113, 1.1118247260560632, 1.7826594807914964, 2.1438672848544416, 2.5464002094554448, 1.0332827703883227, 1.2116685836503465, 1.702678607615944, 2.0976420610431763, 1.1799921225691246, 1.8788661360109653, 1.1039443919080707]\n",
      "R^2 List of GBRTL: [0.5202927821512104, 0.5122182227076909, 0.5847241682437399, 0.23466672497860805, 0.6101592506920963, 0.22992309360030042, 0.49042684746186666, 0.21661178249367366, 0.23724000592729488, 0.4362241199348688, 0.34327071306640217, 0.30017811889421864, 0.37974289243959686, 0.47304264315281974, 0.5077155240416839, 0.4240262550703565, 0.5280310657185362, 0.469579216830239, 0.3863755039169853, 0.42588065671715275]\n",
      "\n",
      "\n",
      "Mean, Stdev of RMSE of GBRTL: 1.6416985896437455 0.554365305975001\n",
      "Mean, Stdev of R^2 of GBRTL: 0.41551647940196706 0.12131022441344184\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shrey/anaconda3/lib/python3.8/site-packages/sklearn/base.py:434: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "######################### Gradient Boosting Regression Transfer Learning Abalone #######################################\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "\n",
    "print(\"Gradient Boosting Regression Transfer Learning\")\n",
    "print(\"-------------------------------------------\")\n",
    "\n",
    "r2scorelist_GBRTL_abalone = []\n",
    "rmselist_GBRTL_abalone = []\n",
    "\n",
    "kf = KFold(n_splits = 20)\n",
    "\n",
    "for train_idx, test_idx in kf.split(abalone_tgt_df):\n",
    "        \n",
    "    abalone_train_df_X = abalone_tgt_df.iloc[train_idx].loc[:, features_abalone]\n",
    "    abalone_test_df_X = abalone_tgt_df.iloc[test_idx][features_abalone]\n",
    "    abalone_train_df_y = abalone_tgt_df.iloc[train_idx].loc[:,target_abalone]\n",
    "    abalone_test_df_y = abalone_tgt_df.loc[test_idx][target_abalone] \n",
    "    \n",
    "    abalone_X_df = pd.concat([abalone_source_df_X, abalone_train_df_X], ignore_index=True)\n",
    "    abalone_y_df = pd.concat([abalone_source_df_y, abalone_train_df_y], ignore_index=True)\n",
    "\n",
    "    abalone_np_train_X = abalone_X_df.to_numpy()\n",
    "    abalone_np_train_y = abalone_y_df.to_numpy()\n",
    "\n",
    "    abalone_np_test_X = abalone_test_df_X.to_numpy()\n",
    "    abalone_np_test_y = abalone_test_df_y.to_numpy()\n",
    "\n",
    "    abalone_np_train_y_list = abalone_np_train_y.ravel()\n",
    "    abalone_np_test_y_list = abalone_np_test_y.ravel()\n",
    "\n",
    "\n",
    "    model_GBRTL_abalone = GradientBoostingRegressor(learning_rate = 0.1, max_depth = 6, n_estimators = 100)\n",
    "    model_GBRTL_abalone.fit(abalone_np_train_X, abalone_np_train_y_list)\n",
    "\n",
    "    y_pred_GBRTL_abalone = model_GBRTL_abalone.predict(abalone_test_df_X) ##Using dataframe instead of the numpy matrix\n",
    "\n",
    "    mse_GBRTL_abalone = sqrt(mean_squared_error(abalone_np_test_y, y_pred_GBRTL_abalone))\n",
    "    rmselist_GBRTL_abalone.append(mse_GBRTL_abalone)\n",
    "        \n",
    "    r2_score_GBRTL_abalone = pearsonr(abalone_np_test_y_list, y_pred_GBRTL_abalone)\n",
    "    r2_score_GBRTL_abalone = (r2_score_GBRTL_abalone[0])**2\n",
    "    r2scorelist_GBRTL_abalone.append(r2_score_GBRTL_abalone)\n",
    "\n",
    "\n",
    "print(\"RMSE List of GBRTL:\", rmselist_GBRTL_abalone)\n",
    "print(\"R^2 List of GBRTL:\", r2scorelist_GBRTL_abalone)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "print(\"Mean, Stdev of RMSE of GBRTL:\", statistics.mean(rmselist_GBRTL_abalone), statistics.stdev(rmselist_GBRTL_abalone))\n",
    "print(\"Mean, Stdev of R^2 of GBRTL:\", statistics.mean(r2scorelist_GBRTL_abalone), statistics.stdev(r2scorelist_GBRTL_abalone))\n",
    "\n",
    "\n",
    "print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4774158366799477 0.1778150997140501\n",
      "1.4702303687021976 0.17970051016641017\n",
      "0.180939186766893 0.8609152412759344\n",
      "\n",
      "\n",
      "-0.5852548329307646 0.5745091033401768\n",
      "-0.36585924469666253 0.7239555216280504\n",
      "0.9064702983841878 0.3911716398994435\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "stats_StAd_rmse, pval_StAd_rmse = stats.ttest_ind(rmselist_stradaboost_abalone, rmselist_AdaTr_abalone)\n",
    "print(stats_StAd_rmse, pval_StAd_rmse)\n",
    "\n",
    "stats_StGb_rmse, pval_StGb_rmse = stats.ttest_ind(rmselist_stradaboost_abalone, rmselist_GBRTL_abalone)\n",
    "print(stats_StGb_rmse, pval_StGb_rmse)\n",
    "\n",
    "stats_StTr_rmse, pval_StTr_rmse = stats.ttest_ind(rmselist_stradaboost_abalone, rmselist_TwoTrAda_abalone)\n",
    "print(stats_StTr_rmse, pval_StTr_rmse)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "stats_StAd_r2, pval_StAd_r2 = stats.ttest_ind(r2scorelist_stradaboost_abalone, r2scorelist_AdaTr_abalone)\n",
    "print(stats_StAd_r2, pval_StAd_r2)\n",
    "\n",
    "stats_StGb_r2, pval_StGb_r2 = stats.ttest_ind(r2scorelist_stradaboost_abalone, r2scorelist_GBRTL_abalone)\n",
    "print(stats_StGb_r2, pval_StGb_r2)\n",
    "\n",
    "stats_StTr_r2, pval_StTr_r2 = stats.ttest_ind(r2scorelist_stradaboost_abalone, r2scorelist_TwoTrAda_abalone)\n",
    "print(stats_StTr_r2, pval_StTr_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3950437583222772 0.07475725214402565\n",
      "2.1242400051017136 0.10085945561990267\n",
      "0.32488683877032 0.7615486615910663\n",
      "\n",
      "\n",
      "-1.881842769362401 0.1329977130272984\n",
      "-1.2603856460659317 0.2760438976285486\n",
      "2.293616600098016 0.0835237480871644\n"
     ]
    }
   ],
   "source": [
    "#### Dependent(paired) t-test\n",
    "from scipy.stats import ttest_rel\n",
    "\n",
    "stats_StAd_rmse, pval_StAd_rmse = stats.ttest_rel(rmselist_stradaboost_abalone, rmselist_AdaTr_abalone)\n",
    "print(stats_StAd_rmse, pval_StAd_rmse)\n",
    "\n",
    "stats_StGb_rmse, pval_StGb_rmse = stats.ttest_rel(rmselist_stradaboost_abalone, rmselist_GBRTL_abalone)\n",
    "print(stats_StGb_rmse, pval_StGb_rmse)\n",
    "\n",
    "stats_StTr_rmse, pval_StTr_rmse = stats.ttest_rel(rmselist_stradaboost_abalone, rmselist_TwoTrAda_abalone)\n",
    "print(stats_StTr_rmse, pval_StTr_rmse)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "stats_StAd_r2, pval_StAd_r2 = stats.ttest_rel(r2scorelist_stradaboost_abalone, r2scorelist_AdaTr_abalone)\n",
    "print(stats_StAd_r2, pval_StAd_r2)\n",
    "\n",
    "stats_StGb_r2, pval_StGb_r2 = stats.ttest_rel(r2scorelist_stradaboost_abalone, r2scorelist_GBRTL_abalone)\n",
    "print(stats_StGb_r2, pval_StGb_r2)\n",
    "\n",
    "stats_StTr_r2, pval_StTr_r2 = stats.ttest_rel(r2scorelist_stradaboost_abalone, r2scorelist_TwoTrAda_abalone)\n",
    "print(stats_StTr_r2, pval_StTr_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kinematics Data\n",
      "(8192, 9)\n",
      "Target Set:  (2878, 8)\n",
      "Source Set 1:  (2396, 8)\n",
      "Source Set 2:  (2918, 8)\n",
      "Final Source Set:  (5314, 8)\n"
     ]
    }
   ],
   "source": [
    "################################################## Kinematics ######################################################################\n",
    "#### range: [0.04 - 1.45]\n",
    "#### Mid of correlation variable: theta7\n",
    "#### [0, 0.6] [0.6, 0.85], [0.6, 0.85]\n",
    "####################################################################################################################################\n",
    "target_var_Kinematics = ['y']\n",
    "colnames_Kinematics = ['theta1', 'theta2', 'theta3', 'theta4', 'theta5', 'theta6', 'theta7', 'theta8', 'y']\n",
    "KinematicsData_df = pd.read_csv('UCI_regression/Kinematics/kin8nm.data', header = None,  names = colnames_Kinematics)\n",
    "\n",
    "print(\"Kinematics Data\")\n",
    "print(KinematicsData_df.shape)\n",
    "\n",
    "############################################### Standardization ###############################################\n",
    "kinematics_cols = KinematicsData_df.columns.difference(['y'])\n",
    "\n",
    "ss = StandardScaler()\n",
    "KinematicsData_df[kinematics_cols] = ss.fit_transform(KinematicsData_df[kinematics_cols])\n",
    "\n",
    "\n",
    "########## Corr Kinematics ################\n",
    "# print(\"The correlation matrix is: \")\n",
    "# print(KinematicsData_df.corr().sort_values('y')['y'])\n",
    "\n",
    "##################### Splitting in 3 equal parts #######################################\n",
    "# print(KinematicsData_df.sort_values('theta7')['theta7'])\n",
    "\n",
    "drop_col_kinematics = ['theta7']\n",
    "\n",
    "kinematics_tgt_df = KinematicsData_df.loc[(KinematicsData_df['theta7'] <= -0.5)]\n",
    "kinematics_tgt_df = kinematics_tgt_df.drop(drop_col_kinematics, axis = 1)\n",
    "kinematics_tgt_df = kinematics_tgt_df.reset_index(drop=True)\n",
    "print(\"Target Set: \",kinematics_tgt_df.shape)\n",
    "\n",
    "\n",
    "kinematics_source1_df = KinematicsData_df.loc[(KinematicsData_df['theta7'] > -0.5) & (KinematicsData_df['theta7'] <= 0.5)]\n",
    "kinematics_source1_df = kinematics_source1_df.drop(drop_col_kinematics, axis = 1)\n",
    "kinematics_source1_df = kinematics_source1_df.reset_index(drop=True)\n",
    "print(\"Source Set 1: \",kinematics_source1_df.shape)\n",
    "\n",
    "\n",
    "kinematics_source2_df = KinematicsData_df.loc[(KinematicsData_df['theta7'] > 0.5)]\n",
    "kinematics_source2_df = kinematics_source2_df.drop(drop_col_kinematics, axis = 1)\n",
    "kinematics_source2_df = kinematics_source2_df.reset_index(drop=True)\n",
    "print(\"Source Set 2: \",kinematics_source2_df.shape)\n",
    "\n",
    "\n",
    "################################# Standardization #################################\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "kinematics_cols = kinematics_tgt_df.columns.difference(['y'])\n",
    "\n",
    "ss = StandardScaler()\n",
    "kinematics_tgt_df[kinematics_cols] = ss.fit_transform(kinematics_tgt_df[kinematics_cols])\n",
    "kinematics_source1_df[kinematics_cols] = ss.fit_transform(kinematics_source1_df[kinematics_cols])\n",
    "kinematics_source2_df[kinematics_cols] = ss.fit_transform(kinematics_source2_df[kinematics_cols])\n",
    "\n",
    "\n",
    "################################# Concatenating the source datasets #################################\n",
    "kinematics_source_df = pd.concat([kinematics_source1_df, kinematics_source2_df], ignore_index = True)\n",
    "kinematics_source_df = kinematics_source_df.reset_index(drop = True)\n",
    "print(\"Final Source Set: \",kinematics_source_df.shape)\n",
    "\n",
    "\n",
    "# #################### Splitting into features and target (Hide in STrAdaBoost.R2)####################\n",
    "# target_kinematics = ['y']\n",
    "\n",
    "# kinematics_source_df_y = kinematics_source_df[target_kinematics]\n",
    "# kinematics_source_df_X = kinematics_source_df.drop(target_kinematics, axis = 1)\n",
    "\n",
    "# features_kinematics = kinematics_source_df_X.columns\n",
    "# print(\"---------------------------\")\n",
    "###############################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataset extracted: \n",
      "(140, 8)\n",
      "----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "######################################## Variance Sampling: Phase 1 ###################################################\n",
    "kmeans = KMeans(n_clusters = 140, random_state=0).fit(kinematics_tgt_df)\n",
    "\n",
    "kinematics_alternate_df = kinematics_tgt_df.copy()\n",
    "kinematics_alternate_df_np = kinematics_tgt_df.to_numpy()\n",
    "\n",
    "idxlist = []\n",
    "kinematics_new_df_list = []\n",
    "\n",
    "for rowkm in kmeans.cluster_centers_:\n",
    "    mindist = -99\n",
    "    rowidx = 0\n",
    "    idx = 0\n",
    "    for row in kinematics_alternate_df_np:\n",
    "        dst = distance.euclidean(row, rowkm)\n",
    "\n",
    "        if(dst >= mindist):\n",
    "            mindist = dst\n",
    "            rowidx = idx\n",
    "            rowval = row\n",
    "\n",
    "        idx = idx + 1\n",
    "\n",
    "#     print(\"Row selected: \", rowidx) #Alternate_df.loc[rowidx,:]\\\n",
    "#     print(\"Min. distance: \", mindist)\n",
    "#     print(\"Matrix shape: \", kinematics_alternate_df_np.shape)\n",
    "    kinematics_new_df_list.append(rowval)\n",
    "    kinematics_alternate_df = np.delete(kinematics_alternate_df_np, rowidx, 0)\n",
    "    idxlist.append(rowidx)\n",
    "\n",
    "\n",
    "kinematics_new_df = pd.DataFrame(np.vstack(kinematics_new_df_list))\n",
    "\n",
    "print(\"Shape of dataset extracted: \")\n",
    "print(kinematics_new_df.shape)\n",
    "print(\"----------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------\n",
      "Shape of source before : (5314, 8)\n",
      "Shape of source after : (5173, 8)\n",
      "----------------------------------------------\n",
      "Shape of target before : (2878, 8)\n",
      "Shape of target after : (3018, 8)\n",
      "----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "##################################################### Variance Sampling: Phase 2 ################################################\n",
    "\n",
    "kinematics_alternate_source_df = kinematics_source_df[1:].copy()\n",
    "kinematics_alternate_source_df_np = kinematics_alternate_source_df.to_numpy()\n",
    "\n",
    "idxlist2 = []\n",
    "kinematics_final_df_list = []\n",
    "\n",
    "for row_nm in kinematics_new_df_list:\n",
    "    min_dist = -99\n",
    "    row_idx = 0\n",
    "    idx_val = 0\n",
    "    for row_alt in kinematics_alternate_source_df_np:\n",
    "        dst = distance.euclidean(row_alt, row_nm)\n",
    "        if(dst >= mindist):\n",
    "            min_dist = dst\n",
    "            row_idx = idx_val\n",
    "            row_val = row_alt\n",
    "\n",
    "        idx_val = idx_val + 1\n",
    "\n",
    "#     print(\"Row selected: \", row_idx) #Alternate_df.loc[rowidx,:]\\\n",
    "#     print(\"Min. distance: \", min_dist)\n",
    "#     print(\"Matrix shape: \", Elevators_alternate_source_df_np.shape)\n",
    "    kinematics_final_df_list.append(row_val)\n",
    "    kinematics_alternate_source_df_np = np.delete(kinematics_alternate_source_df_np, row_idx, 0)\n",
    "    idxlist2.append(row_idx)\n",
    "\n",
    "\n",
    "kinematics_final_df = pd.DataFrame(np.vstack(kinematics_final_df_list), columns= kinematics_source_df.columns)\n",
    "\n",
    "print(\"----------------------------------------------\")\n",
    "print(\"Shape of source before :\",kinematics_source_df.shape)\n",
    "kinematics_source_df = pd.DataFrame(np.vstack(kinematics_alternate_source_df_np), columns= kinematics_source_df.columns)\n",
    "print(\"Shape of source after :\", kinematics_source_df.shape)\n",
    "\n",
    "print(\"----------------------------------------------\")\n",
    "print(\"Shape of target before :\", kinematics_tgt_df.shape)\n",
    "kinematics_tgt_df = pd.concat([kinematics_tgt_df, kinematics_final_df], ignore_index=True)\n",
    "print(\"Shape of target after :\", kinematics_tgt_df.shape)\n",
    "\n",
    "print(\"----------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Set:  (3018, 8)\n",
      "Source Set:  (1330, 8)\n",
      "---------------------------\n"
     ]
    }
   ],
   "source": [
    "################################## Importance Sampling: Manhattan Distance ######################################################\n",
    "\n",
    "kinematics_source_df[\"ManDis\"] = \"\"\n",
    "\n",
    "kinematics_tgt_df_mean = []\n",
    "prow = kinematics_tgt_df.mean()\n",
    "kinematics_tgt_df_mean = [prow.theta1, prow.theta2, prow.theta3, prow.theta4, prow.theta5, prow.theta6, prow.theta8, prow.y]\n",
    "\n",
    "rowidx = 0\n",
    "\n",
    "for row in kinematics_source_df.itertuples():\n",
    "    row_list =[row.theta1, row.theta2, row.theta3, row.theta4, row.theta5, row.theta6, row.theta8, row.y]\n",
    "\n",
    "    man_dis = 0\n",
    "    for i in range(0, len(row_list)):\n",
    "        tempval = kinematics_tgt_df_mean[i] - row_list[i]\n",
    "        man_dis = man_dis + abs(tempval)\n",
    "\n",
    "    kinematics_source_df.loc[rowidx,\"ManDis\"] = man_dis\n",
    "    rowidx = rowidx + 1\n",
    "\n",
    "kinematics_source_df = kinematics_source_df.sort_values(by =['ManDis'])\n",
    "kinematics_source_df = kinematics_source_df.head(1330) \n",
    "kinematics_source_df = kinematics_source_df.drop(['ManDis'], axis =1)\n",
    "kinematics_source_df = kinematics_source_df.reset_index(drop=True)\n",
    "\n",
    "print(\"Target Set: \", kinematics_tgt_df.shape)\n",
    "print(\"Source Set: \", kinematics_source_df.shape)\n",
    "\n",
    "#################### Splitting into features and target ####################\n",
    "target_kinematics = ['y']\n",
    "\n",
    "kinematics_source_df_y = kinematics_source_df[target_kinematics]\n",
    "kinematics_source_df_X = kinematics_source_df.drop(target_kinematics, axis = 1)\n",
    "\n",
    "features_kinematics = kinematics_source_df_X.columns\n",
    "print(\"---------------------------\")\n",
    "###############################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside Updated Sampling TrAdaBoost.R2\n",
      "Inside Updated Sampling TrAdaBoost.R2\n",
      "Inside Updated Sampling TrAdaBoost.R2\n",
      "Inside Updated Sampling TrAdaBoost.R2\n",
      "Inside Updated Sampling TrAdaBoost.R2\n",
      "Inside Updated Sampling TrAdaBoost.R2\n",
      "Inside Updated Sampling TrAdaBoost.R2\n",
      "Inside Updated Sampling TrAdaBoost.R2\n",
      "Inside Updated Sampling TrAdaBoost.R2\n",
      "Inside Updated Sampling TrAdaBoost.R2\n",
      "Inside Updated Sampling TrAdaBoost.R2\n",
      "Inside Updated Sampling TrAdaBoost.R2\n",
      "Inside Updated Sampling TrAdaBoost.R2\n",
      "Inside Updated Sampling TrAdaBoost.R2\n",
      "Inside Updated Sampling TrAdaBoost.R2\n",
      "Inside Updated Sampling TrAdaBoost.R2\n",
      "Inside Updated Sampling TrAdaBoost.R2\n",
      "Inside Updated Sampling TrAdaBoost.R2\n",
      "Inside Updated Sampling TrAdaBoost.R2\n",
      "Inside Updated Sampling TrAdaBoost.R2\n",
      "RMSE List of STrAdaboostR2: 0.13609718934768414\n",
      "R^2 List of STrAdaboostR2: 0.724796005642048\n",
      "\n",
      "\n",
      "Mean, STDev of RMSE: 0.13609718934768414 0.008570356935774672\n",
      "Mean, STDev of R^2: 0.724796005642048 0.0723719858327517\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#################################### STrAdaBoost.R2 Active Learning Kinematics ################################################################\n",
    "from two_TrAdaBoostR2 import TwoStageTrAdaBoostR2\n",
    "\n",
    "n_estimators = 100\n",
    "steps = 30\n",
    "fold = 10\n",
    "random_state = np.random.RandomState(1)\n",
    "\n",
    "r2scorelist_stradaboost_kinematics = []\n",
    "rmselist_stradaboost_kinematics = []\n",
    "\n",
    "kf = KFold(n_splits = 20)\n",
    "\n",
    "for train_idx, test_idx in kf.split(kinematics_tgt_df):\n",
    "        \n",
    "    kinematics_train_df_X = kinematics_tgt_df.iloc[train_idx].loc[:, features_kinematics]\n",
    "    kinematics_test_df_X = kinematics_tgt_df.iloc[test_idx][features_kinematics]\n",
    "    kinematics_train_df_y = kinematics_tgt_df.iloc[train_idx].loc[:,target_kinematics]\n",
    "    kinematics_test_df_y = kinematics_tgt_df.loc[test_idx][target_kinematics] \n",
    "    \n",
    "    kinematics_X_df = pd.concat([kinematics_source_df_X, kinematics_train_df_X], ignore_index=True)\n",
    "    kinematics_y_df = pd.concat([kinematics_source_df_y, kinematics_train_df_y], ignore_index=True)\n",
    "\n",
    "    kinematics_np_train_X = kinematics_X_df.to_numpy()\n",
    "    kinematics_np_train_y = kinematics_y_df.to_numpy()\n",
    "\n",
    "    kinematics_np_test_X = kinematics_test_df_X.to_numpy()\n",
    "    kinematics_np_test_y = kinematics_test_df_y.to_numpy()\n",
    "\n",
    "    kinematics_np_train_y_list = kinematics_np_train_y.ravel()\n",
    "    kinematics_np_test_y_list = kinematics_np_test_y.ravel()\n",
    "    \n",
    "    \n",
    "    sample_size = [len(kinematics_source_df_X), len(kinematics_train_df_X)]\n",
    "\n",
    "    model_stradaboost_kinematics = TwoStageTrAdaBoostR2(DecisionTreeRegressor(max_depth=6),\n",
    "                        n_estimators = n_estimators, sample_size = sample_size,\n",
    "                        steps = steps, fold = fold, random_state = random_state)\n",
    "\n",
    "\n",
    "    model_stradaboost_kinematics.fit(kinematics_np_train_X, kinematics_np_train_y_list)\n",
    "    y_pred_stradaboost_kinematics = model_stradaboost_kinematics.predict(kinematics_np_test_X)\n",
    "\n",
    "\n",
    "    mse_stradaboost_kinematics = sqrt(mean_squared_error(kinematics_np_test_y, y_pred_stradaboost_kinematics))\n",
    "\n",
    "    rmselist_stradaboost_kinematics.append(mse_stradaboost_kinematics)\n",
    "        \n",
    "    r2_score_stradaboost_kinematics = pearsonr(kinematics_np_test_y_list, y_pred_stradaboost_kinematics)\n",
    "    r2_score_stradaboost_kinematics = (r2_score_stradaboost_kinematics[0])**2\n",
    "\n",
    "    r2scorelist_stradaboost_kinematics.append(r2_score_stradaboost_kinematics)\n",
    "\n",
    "    \n",
    "print(\"RMSE List of STrAdaboostR2:\", statistics.mean(rmselist_stradaboost_kinematics))\n",
    "print(\"R^2 List of STrAdaboostR2:\", statistics.mean(r2scorelist_stradaboost_kinematics))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "print(\"Mean, STDev of RMSE:\", statistics.mean(rmselist_stradaboost_kinematics), statistics.stdev(rmselist_stradaboost_kinematics))\n",
    "print(\"Mean, STDev of R^2:\", statistics.mean(r2scorelist_stradaboost_kinematics), statistics.stdev(r2scorelist_stradaboost_kinematics))\n",
    "\n",
    "\n",
    "print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two-Stage TrAdaboost.R2\n",
      "-------------------------------------------\n",
      "RMSE List of TrAdaboost.R2: [0.17127393955694623, 0.1669823459829515, 0.17217512838058419, 0.16807116672007408, 0.17662436844893892, 0.18322992269756577, 0.17708671995261255, 0.16720243538533466, 0.16972185591652802, 0.1794764312051885, 0.14985630557445656, 0.16337513788734326, 0.1650656035957635, 0.15649602793526374, 0.17227748084038497, 0.1602992884583097, 0.17564152304814457, 0.18968403888876234, 0.16677897526799754, 0.16003597223323784]\n",
      "R^2 List of TrAdaboostR2: [0.5535063206087691, 0.6632203635108883, 0.5754247900707788, 0.5655199718691947, 0.5288552633967446, 0.5771643746114014, 0.44668476227349024, 0.5324714539671573, 0.5390098563208533, 0.49612229355516463, 0.6296784300489926, 0.5750741847693349, 0.36045144880565116, 0.6585029517807146, 0.4473901173060256, 0.5280390341581853, 0.5584162072512125, 0.46412984004885527, 0.6133847371223513, 0.552189676932758]\n",
      "\n",
      "\n",
      "Mean, STDev of RMSE: 0.16956773339881942 0.0093664668885901\n",
      "Mean, STDev of R^2: 0.5432618039204262 0.07439481092959442\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "########################### TwoStagetrAdaBoostR2 Kinematics #######################################\n",
    "\n",
    "############# Transfer Learning specifications #########################\n",
    "\n",
    "\n",
    "def get_estimator(**kwargs):\n",
    "    return DecisionTreeRegressor(max_depth = 6)\n",
    "\n",
    "kwargs_TwoTrAda = {'steps': 30,\n",
    "                    'fold': 10,\n",
    "                  'learning_rate': 0.1}\n",
    "\n",
    "from adapt.instance_based import TrAdaBoost, TrAdaBoostR2, TwoStageTrAdaBoostR2\n",
    "\n",
    "print(\"Two-Stage TrAdaboost.R2\")\n",
    "print(\"-------------------------------------------\")\n",
    "\n",
    "r2scorelist_TwoTrAda_kinematics = []\n",
    "rmselist_TwoTrAda_kinematics = []\n",
    "\n",
    "kf = KFold(n_splits = 20)\n",
    "\n",
    "for train_idx, test_idx in kf.split(kinematics_tgt_df):\n",
    "        \n",
    "    kinematics_train_df_X = kinematics_tgt_df.iloc[train_idx].loc[:, features_kinematics]\n",
    "    kinematics_test_df_X = kinematics_tgt_df.iloc[test_idx][features_kinematics]\n",
    "    kinematics_train_df_y = kinematics_tgt_df.iloc[train_idx].loc[:,target_kinematics]\n",
    "    kinematics_test_df_y = kinematics_tgt_df.loc[test_idx][target_kinematics] \n",
    "    \n",
    "    kinematics_X_df = pd.concat([kinematics_source_df_X, kinematics_train_df_X], ignore_index=True)\n",
    "    kinematics_y_df = pd.concat([kinematics_source_df_y, kinematics_train_df_y], ignore_index=True)\n",
    "\n",
    "    kinematics_np_train_X = kinematics_X_df.to_numpy()\n",
    "    kinematics_np_train_y = kinematics_y_df.to_numpy()\n",
    "\n",
    "    kinematics_np_test_X = kinematics_test_df_X.to_numpy()\n",
    "    kinematics_np_test_y = kinematics_test_df_y.to_numpy()\n",
    "\n",
    "    kinematics_np_train_y_list = kinematics_np_train_y.ravel()\n",
    "    kinematics_np_test_y_list = kinematics_np_test_y.ravel()\n",
    "\n",
    "    src_size_kinematics = len(kinematics_source_df_y)\n",
    "    tgt_size_kinematics = len(kinematics_train_df_y)\n",
    "\n",
    "    src_idx_kinematics = np.arange(start = 0, stop = (src_size_kinematics - 1), step = 1)\n",
    "    tgt_idx_kinematics = np.arange(start = src_size_kinematics, stop = ((src_size_kinematics + tgt_size_kinematics)-1), step = 1)\n",
    "\n",
    "\n",
    "    model_TwoTrAda_kinematics = TwoStageTrAdaBoostR2(get_estimator = get_estimator, n_estimators = 100)\n",
    "    model_TwoTrAda_kinematics.fit(kinematics_np_train_X, kinematics_np_train_y_list, src_idx_kinematics, tgt_idx_kinematics)\n",
    "\n",
    "    y_pred_TwoTrAda_kinematics = model_TwoTrAda_kinematics.predict(kinematics_np_test_X)\n",
    "\n",
    "    mse_TwoTrAda_kinematics = sqrt(mean_squared_error(kinematics_np_test_y, y_pred_TwoTrAda_kinematics))\n",
    "    rmselist_TwoTrAda_kinematics.append(mse_TwoTrAda_kinematics)\n",
    "        \n",
    "    r2_score_TwoTrAda_kinematics = pearsonr(kinematics_np_test_y_list, y_pred_TwoTrAda_kinematics)\n",
    "    r2_score_TwoTrAda_kinematics = (r2_score_TwoTrAda_kinematics[0])**2\n",
    "    r2scorelist_TwoTrAda_kinematics.append(r2_score_TwoTrAda_kinematics)\n",
    "\n",
    "print(\"RMSE List of TrAdaboost.R2:\", rmselist_TwoTrAda_kinematics)\n",
    "print(\"R^2 List of TrAdaboostR2:\", r2scorelist_TwoTrAda_kinematics)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "print(\"Mean, STDev of RMSE:\", statistics.mean(rmselist_TwoTrAda_kinematics), statistics.stdev(rmselist_TwoTrAda_kinematics))\n",
    "print(\"Mean, STDev of R^2:\", statistics.mean(r2scorelist_TwoTrAda_kinematics), statistics.stdev(r2scorelist_TwoTrAda_kinematics))\n",
    "\n",
    "\n",
    "print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaboost.R2 Transfer Learning\n",
      "-------------------------------------------\n",
      "RMSE List of Adaboost.R2: [0.2154752685103698, 0.1985844838892281, 0.21146872987552065, 0.20143870658282897, 0.2084368802569673, 0.20509098740625809, 0.19590132265779048, 0.1984767295462005, 0.20066657556787076, 0.1940639339583811, 0.19573528041609659, 0.21254076626704194, 0.19260275381954683, 0.20024603801490737, 0.2127625151625833, 0.1946791338774877, 0.19844330806078467, 0.2246199493720246, 0.2070223979495842, 0.19051378455805573]\n",
      "R^2 List of Adaboost.R2: [0.36537822322200547, 0.515427104077821, 0.38411375147009996, 0.40826702567999223, 0.4074368190778562, 0.5766234982070354, 0.37387393704525607, 0.3875088393617786, 0.38634178126663865, 0.48767932699211114, 0.4059204909356948, 0.3471076650608209, 0.20297093095566596, 0.4439041165518097, 0.22565914366021045, 0.34539953611115465, 0.515785387970833, 0.25395486208595136, 0.4109043013447319, 0.4318167360165548]\n",
      "\n",
      "\n",
      "Mean, STDev of RMSE: 0.20293847728747644 0.008954844325299674\n",
      "Mean, STDev of R^2: 0.3938036738547011 0.09371349330463755\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "########################### AdaBoostR2 Transfer Learning Kinematics #####################################################\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "\n",
    "print(\"Adaboost.R2 Transfer Learning\")\n",
    "print(\"-------------------------------------------\")\n",
    "\n",
    "r2scorelist_AdaTL_kinematics = []\n",
    "rmselist_AdaTL_kinematics = []\n",
    "\n",
    "kf = KFold(n_splits = 20)\n",
    "\n",
    "for train_idx, test_idx in kf.split(kinematics_tgt_df):\n",
    "        \n",
    "    kinematics_train_df_X = kinematics_tgt_df.iloc[train_idx].loc[:, features_kinematics]\n",
    "    kinematics_test_df_X = kinematics_tgt_df.iloc[test_idx][features_kinematics]\n",
    "    kinematics_train_df_y = kinematics_tgt_df.iloc[train_idx].loc[:,target_kinematics]\n",
    "    kinematics_test_df_y = kinematics_tgt_df.loc[test_idx][target_kinematics] \n",
    "    \n",
    "    kinematics_X_df = pd.concat([kinematics_source_df_X, kinematics_train_df_X], ignore_index=True)\n",
    "    kinematics_y_df = pd.concat([kinematics_source_df_y, kinematics_train_df_y], ignore_index=True)\n",
    "\n",
    "    kinematics_np_train_X = kinematics_X_df.to_numpy()\n",
    "    kinematics_np_train_y = kinematics_y_df.to_numpy()\n",
    "\n",
    "    kinematics_np_test_X = kinematics_test_df_X.to_numpy()\n",
    "    kinematics_np_test_y = kinematics_test_df_y.to_numpy()\n",
    "\n",
    "    kinematics_np_train_y_list = kinematics_np_train_y.ravel()\n",
    "    kinematics_np_test_y_list = kinematics_np_test_y.ravel()\n",
    "\n",
    "    model_AdaTL_kinematics = AdaBoostRegressor(DecisionTreeRegressor(max_depth = 6), learning_rate = 0.1, n_estimators = 100) \n",
    "    model_AdaTL_kinematics.fit(kinematics_np_train_X, kinematics_np_train_y_list)\n",
    "\n",
    "    y_pred_AdaTL_kinematics = model_AdaTL_kinematics.predict(kinematics_np_test_X)\n",
    "\n",
    "    mse_AdaTL_kinematics = sqrt(mean_squared_error(kinematics_np_test_y, y_pred_AdaTL_kinematics))\n",
    "    rmselist_AdaTL_kinematics.append(mse_AdaTL_kinematics)\n",
    "        \n",
    "    r2_score_AdaTL_kinematics = pearsonr(kinematics_np_test_y_list, y_pred_AdaTL_kinematics)\n",
    "    r2_score_AdaTL_kinematics = (r2_score_AdaTL_kinematics[0])**2\n",
    "    r2scorelist_AdaTL_kinematics.append(r2_score_AdaTL_kinematics)\n",
    "\n",
    "print(\"RMSE List of Adaboost.R2:\", rmselist_AdaTL_kinematics)\n",
    "print(\"R^2 List of Adaboost.R2:\", r2scorelist_AdaTL_kinematics)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "print(\"Mean, STDev of RMSE:\", statistics.mean(rmselist_AdaTL_kinematics), statistics.stdev(rmselist_AdaTL_kinematics))\n",
    "print(\"Mean, STDev of R^2:\", statistics.mean(r2scorelist_AdaTL_kinematics), statistics.stdev(r2scorelist_AdaTL_kinematics))\n",
    "\n",
    "\n",
    "print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Regression Transfer Learning\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shrey/anaconda3/lib/python3.8/site-packages/sklearn/base.py:434: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/shrey/anaconda3/lib/python3.8/site-packages/sklearn/base.py:434: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/shrey/anaconda3/lib/python3.8/site-packages/sklearn/base.py:434: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/shrey/anaconda3/lib/python3.8/site-packages/sklearn/base.py:434: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/shrey/anaconda3/lib/python3.8/site-packages/sklearn/base.py:434: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/shrey/anaconda3/lib/python3.8/site-packages/sklearn/base.py:434: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/shrey/anaconda3/lib/python3.8/site-packages/sklearn/base.py:434: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/shrey/anaconda3/lib/python3.8/site-packages/sklearn/base.py:434: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/shrey/anaconda3/lib/python3.8/site-packages/sklearn/base.py:434: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/shrey/anaconda3/lib/python3.8/site-packages/sklearn/base.py:434: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/shrey/anaconda3/lib/python3.8/site-packages/sklearn/base.py:434: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/shrey/anaconda3/lib/python3.8/site-packages/sklearn/base.py:434: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/shrey/anaconda3/lib/python3.8/site-packages/sklearn/base.py:434: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/shrey/anaconda3/lib/python3.8/site-packages/sklearn/base.py:434: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/shrey/anaconda3/lib/python3.8/site-packages/sklearn/base.py:434: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/shrey/anaconda3/lib/python3.8/site-packages/sklearn/base.py:434: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/shrey/anaconda3/lib/python3.8/site-packages/sklearn/base.py:434: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/shrey/anaconda3/lib/python3.8/site-packages/sklearn/base.py:434: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/shrey/anaconda3/lib/python3.8/site-packages/sklearn/base.py:434: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE List of GBRTL: [0.20297238222001085, 0.1848060082047616, 0.20224217250963997, 0.18303929398079735, 0.2011556891615623, 0.19600130297585558, 0.19089314700841448, 0.19118682989768113, 0.195100757148304, 0.1730455086776183, 0.18937014323012513, 0.20898403152231088, 0.19671983116526634, 0.18648861736533234, 0.2032060476394821, 0.20387651982213575, 0.18700255898624019, 0.21573602371523867, 0.19667278803892851, 0.17198487208884486]\n",
      "R^2 List of GBRTL: [0.42467335025841574, 0.5648310321277084, 0.4229502742183758, 0.5042618696742892, 0.43054532168848236, 0.5429624837449435, 0.4088604238947674, 0.41530272672772856, 0.4104448645674725, 0.5500647787274874, 0.43958039663440185, 0.3666075916568952, 0.2442738102916208, 0.4949905916318364, 0.28992421680533204, 0.3281466615119518, 0.5293114951324919, 0.3242815636959829, 0.4648046415782203, 0.5053667617084855]\n",
      "\n",
      "\n",
      "Mean, STDev of RMSE: 0.1940242262679275 0.011172029103735728\n",
      "Mean, STDev of R^2: 0.4331092428138445 0.08960586177710513\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shrey/anaconda3/lib/python3.8/site-packages/sklearn/base.py:434: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "######################### Gradient Boosting Regression Transfer Learning Kinematics #######################################\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "\n",
    "print(\"Gradient Boosting Regression Transfer Learning\")\n",
    "print(\"-------------------------------------------\")\n",
    "\n",
    "r2scorelist_GBRTL_kinematics = []\n",
    "rmselist_GBRTL_kinematics = []\n",
    "\n",
    "kf = KFold(n_splits = 20)\n",
    "\n",
    "for train_idx, test_idx in kf.split(kinematics_tgt_df):\n",
    "        \n",
    "    kinematics_train_df_X = kinematics_tgt_df.iloc[train_idx].loc[:, features_kinematics]\n",
    "    kinematics_test_df_X = kinematics_tgt_df.iloc[test_idx][features_kinematics]\n",
    "    kinematics_train_df_y = kinematics_tgt_df.iloc[train_idx].loc[:,target_kinematics]\n",
    "    kinematics_test_df_y = kinematics_tgt_df.loc[test_idx][target_kinematics] \n",
    "    \n",
    "    kinematics_X_df = pd.concat([kinematics_source_df_X, kinematics_train_df_X], ignore_index=True)\n",
    "    kinematics_y_df = pd.concat([kinematics_source_df_y, kinematics_train_df_y], ignore_index=True)\n",
    "\n",
    "    kinematics_np_train_X = kinematics_X_df.to_numpy()\n",
    "    kinematics_np_train_y = kinematics_y_df.to_numpy()\n",
    "\n",
    "    kinematics_np_test_X = kinematics_test_df_X.to_numpy()\n",
    "    kinematics_np_test_y = kinematics_test_df_y.to_numpy()\n",
    "\n",
    "    kinematics_np_train_y_list = kinematics_np_train_y.ravel()\n",
    "    kinematics_np_test_y_list = kinematics_np_test_y.ravel()\n",
    "\n",
    "\n",
    "    model_GBRTL_kinematics = GradientBoostingRegressor(learning_rate = 0.1, max_depth = 6, n_estimators = 100)\n",
    "    model_GBRTL_kinematics.fit(kinematics_np_train_X, kinematics_np_train_y_list)\n",
    "\n",
    "    y_pred_GBRTL_kinematics = model_GBRTL_kinematics.predict(kinematics_test_df_X) ##Using dataframe instead of the numpy matrix\n",
    "\n",
    "    mse_GBRTL_kinematics = sqrt(mean_squared_error(kinematics_np_test_y, y_pred_GBRTL_kinematics))\n",
    "    rmselist_GBRTL_kinematics.append(mse_GBRTL_kinematics)\n",
    "        \n",
    "    r2_score_GBRTL_kinematics = pearsonr(kinematics_np_test_y_list, y_pred_GBRTL_kinematics)\n",
    "    r2_score_GBRTL_kinematics = (r2_score_GBRTL_kinematics[0])**2\n",
    "\n",
    "    r2scorelist_GBRTL_kinematics.append(r2_score_GBRTL_kinematics)\n",
    "\n",
    "\n",
    "print(\"RMSE List of GBRTL:\", rmselist_GBRTL_kinematics)\n",
    "print(\"R^2 List of GBRTL:\", r2scorelist_GBRTL_kinematics)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "    \n",
    "print(\"Mean, STDev of RMSE:\", statistics.mean(rmselist_GBRTL_kinematics), statistics.stdev(rmselist_GBRTL_kinematics))\n",
    "print(\"Mean, STDev of R^2:\", statistics.mean(r2scorelist_GBRTL_kinematics), statistics.stdev(r2scorelist_GBRTL_kinematics))\n",
    "\n",
    "\n",
    "print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.239211166142098 0.05549370834515708\n",
      "5.710744979214197 0.00044878897195905153\n",
      "12.933901052446585 1.2086974283329565e-06\n",
      "\n",
      "\n",
      "0.15020428465586927 0.8843214418707841\n",
      "-0.580453091569201 0.5775889224254931\n",
      "-5.759859685898271 0.0004241772800097817\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "stats_StAd_rmse, pval_StAd_rmse = stats.ttest_ind(rmselist_stradaboost_kinematics, rmselist_AdaTL_kinematics)\n",
    "print(stats_StAd_rmse, pval_StAd_rmse)\n",
    "\n",
    "stats_StGb_rmse, pval_StGb_rmse = stats.ttest_ind(rmselist_stradaboost_kinematics, rmselist_GBRTL_kinematics)\n",
    "print(stats_StGb_rmse, pval_StGb_rmse)\n",
    "\n",
    "stats_StTr_rmse, pval_StTr_rmse = stats.ttest_ind(rmselist_stradaboost_kinematics, rmselist_TwoTrAda_kinematics)\n",
    "print(stats_StTr_rmse, pval_StTr_rmse)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "stats_StAd_r2, pval_StAd_r2 = stats.ttest_ind(r2scorelist_stradaboost_kinematics, r2scorelist_AdaTL_kinematics)\n",
    "print(stats_StAd_r2, pval_StAd_r2)\n",
    "\n",
    "stats_StGb_r2, pval_StGb_r2 = stats.ttest_ind(r2scorelist_stradaboost_kinematics, r2scorelist_GBRTL_kinematics)\n",
    "print(stats_StGb_r2, pval_StGb_r2)\n",
    "\n",
    "stats_StTr_r2, pval_StTr_r2 = stats.ttest_ind(r2scorelist_stradaboost_kinematics, r2scorelist_TwoTrAda_kinematics)\n",
    "print(stats_StTr_r2, pval_StTr_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6296148561965667 0.02216745053260249\n",
      "6.519298221944031 0.0028583835971875094\n",
      "13.492049030039581 0.0001746222481559807\n",
      "\n",
      "\n",
      "0.7297095707949993 0.5060026819401974\n",
      "-4.064041932226505 0.015297728196215526\n",
      "-9.89590741109181 0.0005852307476539434\n"
     ]
    }
   ],
   "source": [
    "#### Dependent(paired) t-test\n",
    "from scipy.stats import ttest_rel\n",
    "\n",
    "\n",
    "stats_StAd_rmse, pval_StAd_rmse = stats.ttest_rel(rmselist_stradaboost_kinematics, rmselist_AdaTL_kinematics)\n",
    "print(stats_StAd_rmse, pval_StAd_rmse)\n",
    "\n",
    "stats_StGb_rmse, pval_StGb_rmse = stats.ttest_rel(rmselist_stradaboost_kinematics, rmselist_GBRTL_kinematics)\n",
    "print(stats_StGb_rmse, pval_StGb_rmse)\n",
    "\n",
    "stats_StTr_rmse, pval_StTr_rmse = stats.ttest_rel(rmselist_stradaboost_kinematics, rmselist_TwoTrAda_kinematics)\n",
    "print(stats_StTr_rmse, pval_StTr_rmse)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "stats_StAd_r2, pval_StAd_r2 = stats.ttest_rel(r2scorelist_stradaboost_kinematics, r2scorelist_AdaTL_kinematics)\n",
    "print(stats_StAd_r2, pval_StAd_r2)\n",
    "\n",
    "stats_StGb_r2, pval_StGb_r2 = stats.ttest_rel(r2scorelist_stradaboost_kinematics, r2scorelist_GBRTL_kinematics)\n",
    "print(stats_StGb_r2, pval_StGb_r2)\n",
    "\n",
    "stats_StTr_r2, pval_StTr_r2 = stats.ttest_rel(r2scorelist_stradaboost_kinematics, r2scorelist_TwoTrAda_kinematics)\n",
    "print(stats_StTr_r2, pval_StTr_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computer Activity Data\n",
      "(8192, 22)\n",
      "Target Set:  (2766, 21)\n",
      "Source Set 1:  (2539, 21)\n",
      "Source Set 2:  (2887, 21)\n",
      "Final Source Set:  (5426, 21)\n"
     ]
    }
   ],
   "source": [
    "################################### Computer Activity ###########################################\n",
    "colnames_CompAct = ['lread', 'lwrite', 'scall', 'sread', 'swrite', 'fork', 'exec', 'rchar', 'wchar', 'pgout', 'ppgout', \n",
    "                    'pgfree', 'pgscan', 'atch', 'pgin', 'ppgin', 'pflt', 'vflt', 'runqsz', 'freemem', 'freeswap', 'usr' ]\n",
    "CompActData_df = pd.read_csv('UCI_regression/ComputerActivity/cpu_act.data', header = None, names = colnames_CompAct)\n",
    "print(\"Computer Activity Data\")\n",
    "print(CompActData_df.shape)\n",
    "\n",
    "##################### Corr Computer Activity #####################\n",
    "# print(\"The correlation matrix is: \")\n",
    "# print(CompActData_df.corr().sort_values('usr')['usr'])\n",
    "\n",
    "##################### Splitting in 3 equal parts #######################################\n",
    "# print(CompActData_df.sort_values('pgin')['pgin'])\n",
    "\n",
    "drop_col_compact = ['pgin']\n",
    "\n",
    "compact_tgt_df = CompActData_df.loc[(CompActData_df['pgin'] <= 1.0)]\n",
    "compact_tgt_df = compact_tgt_df.drop(drop_col_compact, axis = 1)\n",
    "compact_tgt_df = compact_tgt_df.reset_index(drop=True)\n",
    "print(\"Target Set: \",compact_tgt_df.shape)\n",
    "\n",
    "\n",
    "compact_source1_df = CompActData_df.loc[(CompActData_df['pgin'] > 1.0) & (CompActData_df['pgin'] <= 6.0)]\n",
    "compact_source1_df = compact_source1_df.drop(drop_col_compact, axis = 1)\n",
    "compact_source1_df = compact_source1_df.reset_index(drop = True)\n",
    "print(\"Source Set 1: \",compact_source1_df.shape)\n",
    "\n",
    "\n",
    "compact_source2_df = CompActData_df.loc[(CompActData_df['pgin'] > 6.0)]\n",
    "compact_source2_df = compact_source2_df.drop(drop_col_compact, axis = 1)\n",
    "compact_source2_df = compact_source2_df.reset_index(drop = True)\n",
    "print(\"Source Set 2: \",compact_source2_df.shape)\n",
    "\n",
    "\n",
    "################################# Standardization #################################\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "compact_cols = compact_tgt_df.columns.difference(['usr'])\n",
    "\n",
    "ss = StandardScaler()\n",
    "compact_tgt_df[compact_cols] = ss.fit_transform(compact_tgt_df[compact_cols])\n",
    "compact_source1_df[compact_cols] = ss.fit_transform(compact_source1_df[compact_cols])\n",
    "compact_source2_df[compact_cols] = ss.fit_transform(compact_source2_df[compact_cols])\n",
    "\n",
    "\n",
    "##################### Concatenating the source datasets #####################\n",
    "compact_source_df = pd.concat([compact_source1_df, compact_source2_df], ignore_index = True)\n",
    "compact_source_df = compact_source_df.reset_index(drop = True)\n",
    "print(\"Final Source Set: \",compact_source_df.shape)\n",
    "\n",
    "\n",
    "# #################### Splitting into features and target (Hide for STrAdaBoost.R2)####################\n",
    "# target_compact = ['usr']\n",
    "\n",
    "# compact_source_df_y = compact_source_df[target_compact]\n",
    "# compact_source_df_X = compact_source_df.drop(target_compact, axis = 1)\n",
    "\n",
    "# features_compact = compact_source_df_X.columns\n",
    "# print(\"---------------------------\")\n",
    "###############################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset extracted is: \n",
      "(200, 21)\n",
      "------------------------------------------------------\n",
      "Shape of source before : (5426, 21)\n",
      "Shape of source after : (5226, 21)\n",
      "----------------------------------------------\n",
      "Shape of target before : (2766, 21)\n",
      "Shape of target after : (2766, 21)\n",
      "----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "######################################## Variance Sampling: Phase 1 ###################################################\n",
    "kmeans = KMeans(n_clusters = 200, random_state=0).fit(compact_tgt_df)\n",
    "\n",
    "compact_alternate_df = compact_tgt_df.copy()\n",
    "compact_alternate_df_np = compact_tgt_df.to_numpy()\n",
    "\n",
    "idxlist = []\n",
    "compact_new_df_list = []\n",
    "\n",
    "for rowkm in kmeans.cluster_centers_:\n",
    "    mindist = -99\n",
    "    rowidx = 0\n",
    "    idx = 0\n",
    "    for row in compact_alternate_df_np:\n",
    "        dst = distance.euclidean(row, rowkm)\n",
    "\n",
    "        if(dst >= mindist):\n",
    "            mindist = dst\n",
    "            rowidx = idx\n",
    "            rowval = row\n",
    "\n",
    "        idx = idx + 1\n",
    "\n",
    "#     print(\"Row selected: \", rowidx) #Alternate_df.loc[rowidx,:]\\\n",
    "#     print(\"Min. distance: \", mindist)\n",
    "#     print(\"Matrix shape: \", compact_alternate_df_np.shape)\n",
    "    compact_new_df_list.append(rowval)\n",
    "    compact_alternate_df = np.delete(compact_alternate_df_np, rowidx, 0)\n",
    "    idxlist.append(rowidx)\n",
    "\n",
    "\n",
    "compact_new_df = pd.DataFrame(np.vstack(compact_new_df_list))\n",
    "\n",
    "print(\"The dataset extracted is: \")\n",
    "print(compact_new_df.shape)\n",
    "print(\"------------------------------------------------------\")\n",
    "\n",
    "##################################################### Variance Sampling: Phase 2 ################################################\n",
    "\n",
    "compact_alternate_source_df = compact_source_df.copy()\n",
    "compact_alternate_source_df_np = compact_alternate_source_df.to_numpy()\n",
    "\n",
    "idxlist2 = []\n",
    "compact_final_df_list = []\n",
    "\n",
    "for row_nm in compact_new_df_list:\n",
    "    min_dist = -99\n",
    "    row_idx = 0\n",
    "    idx_val = 0\n",
    "    for row_alt in compact_alternate_source_df_np:\n",
    "        dst = distance.euclidean(row_alt, row_nm)\n",
    "        if(dst >= mindist):\n",
    "            min_dist = dst\n",
    "            row_idx = idx_val\n",
    "            row_val = row_alt\n",
    "\n",
    "        idx_val = idx_val + 1\n",
    "\n",
    "#     print(\"Row selected: \", row_idx) #Alternate_df.loc[rowidx,:]\\\n",
    "#     print(\"Min. distance: \", min_dist)\n",
    "#     print(\"Matrix shape: \", compact_alternate_source_df_np.shape)\n",
    "    compact_final_df_list.append(row_val)\n",
    "    compact_alternate_source_df_np = np.delete(compact_alternate_source_df_np, row_idx, 0)\n",
    "    idxlist2.append(row_idx)\n",
    "\n",
    "\n",
    "compact_final_df = pd.DataFrame(np.vstack(compact_final_df_list), columns= compact_alternate_source_df.columns)\n",
    "\n",
    "print(\"Shape of source before :\",compact_source_df.shape)\n",
    "compact_source_df = pd.DataFrame(np.vstack(compact_alternate_source_df_np), columns= compact_source_df.columns)\n",
    "print(\"Shape of source after :\", compact_source_df.shape)\n",
    "\n",
    "print(\"----------------------------------------------\")\n",
    "\n",
    "print(\"Shape of target before :\", compact_tgt_df.shape)\n",
    "Elevators_tgt_df = pd.concat([compact_tgt_df, compact_final_df], ignore_index=True)\n",
    "print(\"Shape of target after :\", compact_tgt_df.shape)\n",
    "\n",
    "print(\"----------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Set:  (2766, 21)\n",
      "Source Set:  (1356, 21)\n",
      "---------------------------\n"
     ]
    }
   ],
   "source": [
    "################################## Importance Sampling: Manhattan Distance ######################################################\n",
    "compact_source_df[\"ManDis\"] = \"\"\n",
    "\n",
    "compact_tgt_df_mean = []\n",
    "prow = compact_tgt_df.mean()\n",
    "compact_tgt_df_mean = [prow.lread, prow.lwrite, prow.scall, prow.sread, prow.swrite, prow.fork, prow.exec, prow.rchar,\n",
    "       prow.wchar, prow.pgout, prow.ppgout, prow.pgfree, prow.pgscan, prow.atch, prow.ppgin, prow.pflt, prow.vflt, prow.runqsz,\n",
    "        prow.freemem, prow.freeswap, prow.usr]\n",
    "\n",
    "rowidx = 0\n",
    "\n",
    "for row in compact_source_df.itertuples():\n",
    "    row_list =[row.lread, row.lwrite, row.scall, row.sread, row.swrite, row.fork, row.exec, row.rchar,\n",
    "       row.wchar, row.pgout, row.ppgout, row.pgfree, row.pgscan, row.atch, row.ppgin, row.pflt, row.vflt, row.runqsz,\n",
    "        row.freemem, row.freeswap, row.usr]\n",
    "\n",
    "    man_dis = 0\n",
    "    for i in range(0, len(row_list)):\n",
    "        tempval = compact_tgt_df_mean[i] - row_list[i]\n",
    "        man_dis = man_dis + abs(tempval)\n",
    "\n",
    "    compact_source_df.loc[rowidx,\"ManDis\"] = man_dis\n",
    "    rowidx = rowidx + 1\n",
    "\n",
    "\n",
    "# kinematics_source_df = kinematics_source_df.sort_values(by =['ManDis'])\n",
    "# kinematics_source_df = kinematics_source_df.head(1330) \n",
    "# kinematics_source_df = kinematics_source_df.drop(['ManDis'], axis =1)\n",
    "# kinematics_source_df = kinematics_source_df.reset_index(drop=True)\n",
    "\n",
    "# print(\"Target Set: \", kinematics_tgt_df.shape)\n",
    "# print(\"Source Set: \", kinematics_source_df.shape)\n",
    "\n",
    "compact_source_df = compact_source_df.sort_values(by =['ManDis'])\n",
    "compact_source_df = compact_source_df.head(1356) #### For Computer Activity 650 instances\n",
    "compact_source_df = compact_source_df.drop(['ManDis'], axis =1)\n",
    "compact_source_df = compact_source_df.reset_index(drop=True)\n",
    "\n",
    "print(\"Target Set: \", compact_tgt_df.shape)\n",
    "print(\"Source Set: \", compact_source_df.shape)\n",
    "\n",
    "\n",
    "#################### Splitting into features and target ####################\n",
    "target_compact = ['usr']\n",
    "\n",
    "compact_source_df_y = compact_source_df[target_compact]\n",
    "compact_source_df_X = compact_source_df.drop(target_compact, axis = 1)\n",
    "\n",
    "features_compact = compact_source_df_X.columns\n",
    "print(\"---------------------------\")\n",
    "###############################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STrAdaboost.R2 (50 iter)\n",
      "-------------------------------------------\n",
      "Inside Updated Sampling TrAdaBoost.R2\n",
      "Inside Updated Sampling TrAdaBoost.R2\n",
      "Inside Updated Sampling TrAdaBoost.R2\n",
      "Inside Updated Sampling TrAdaBoost.R2\n",
      "Inside Updated Sampling TrAdaBoost.R2\n",
      "Inside Updated Sampling TrAdaBoost.R2\n",
      "Inside Updated Sampling TrAdaBoost.R2\n",
      "Inside Updated Sampling TrAdaBoost.R2\n",
      "Inside Updated Sampling TrAdaBoost.R2\n",
      "Inside Updated Sampling TrAdaBoost.R2\n",
      "Inside Updated Sampling TrAdaBoost.R2\n",
      "Inside Updated Sampling TrAdaBoost.R2\n",
      "Inside Updated Sampling TrAdaBoost.R2\n",
      "Inside Updated Sampling TrAdaBoost.R2\n",
      "Inside Updated Sampling TrAdaBoost.R2\n",
      "Inside Updated Sampling TrAdaBoost.R2\n",
      "Inside Updated Sampling TrAdaBoost.R2\n",
      "Inside Updated Sampling TrAdaBoost.R2\n",
      "Inside Updated Sampling TrAdaBoost.R2\n",
      "Inside Updated Sampling TrAdaBoost.R2\n",
      "RMSE List of STrAdaboost.R2: [2.720968733669425, 2.4345699191658334, 2.541768848715657, 2.2897283428819843, 2.6311561694449166, 2.5474782829011366, 2.3713362266180478, 2.245324720375074, 2.7064397419167125, 2.5457767155934734, 2.4331589799616693, 2.4336123863428356, 2.479429052112151, 2.480031157095338, 2.7587248461567992, 2.5368703721179653, 3.186210403074402, 3.8986130143709534, 3.0084934897480884, 3.342109097790093]\n",
      "R^2 List of STrAdaboost.R2: [0.9615613526888211, 0.9514369774230375, 0.9746810024036147, 0.9839742669017011, 0.9822114278958173, 0.8913125681787651, 0.9837134288373766, 0.9811630185628939, 0.8948800215192373, 0.9640236614456981, 0.966562979687397, 0.8539824114795764, 0.9862857919847405, 0.9648646965895236, 0.8452241197559179, 0.9750563391645007, 0.8882501150024834, 0.6530140444598472, 0.7443139630580329, 0.7861097124065605]\n",
      "\n",
      "\n",
      "Mean, STDev of RMSE: 2.6795900250026277 0.4021200251716997\n",
      "Mean, STDev of R^2: 0.9116310949722771 0.09349857326087074\n"
     ]
    }
   ],
   "source": [
    "#################################### STrAdaBoost.R2 compAct ################################################################\n",
    "from two_TrAdaBoostR2 import TwoStageTrAdaBoostR2\n",
    "\n",
    "n_estimators = 100\n",
    "steps = 30\n",
    "fold = 10\n",
    "random_state = np.random.RandomState(1)\n",
    "\n",
    "r2scorelist_stradaboost_compact = []\n",
    "rmselist_stradaboost_compact = []\n",
    "\n",
    "print(\"STrAdaboost.R2 (50 iter)\")\n",
    "print(\"-------------------------------------------\")\n",
    "\n",
    "kf = KFold(n_splits = 20)\n",
    "\n",
    "for train_idx, test_idx in kf.split(compact_tgt_df):\n",
    "        \n",
    "    compact_train_df_X = compact_tgt_df.iloc[train_idx].loc[:, features_compact]\n",
    "    compact_test_df_X = compact_tgt_df.iloc[test_idx][features_compact]\n",
    "    compact_train_df_y = compact_tgt_df.iloc[train_idx].loc[:,target_compact]\n",
    "    compact_test_df_y = compact_tgt_df.loc[test_idx][target_compact] \n",
    "    \n",
    "    compact_X_df = pd.concat([compact_source_df_X, compact_train_df_X], ignore_index=True)\n",
    "    compact_y_df = pd.concat([compact_source_df_y, compact_train_df_y], ignore_index=True)\n",
    "\n",
    "    compact_np_train_X = compact_X_df.to_numpy()\n",
    "    compact_np_train_y = compact_y_df.to_numpy()\n",
    "\n",
    "    compact_np_test_X = compact_test_df_X.to_numpy()\n",
    "    compact_np_test_y = compact_test_df_y.to_numpy()\n",
    "\n",
    "    compact_np_train_y_list = compact_np_train_y.ravel()\n",
    "    compact_np_test_y_list = compact_np_test_y.ravel()\n",
    "    \n",
    "    sample_size = [len(compact_source_df_X), len(compact_train_df_X)]\n",
    "\n",
    "    model_stradaboost_compact = TwoStageTrAdaBoostR2(DecisionTreeRegressor(max_depth=6),\n",
    "                        n_estimators = n_estimators, sample_size = sample_size,\n",
    "                        steps = steps, fold = fold, random_state = random_state)\n",
    "\n",
    "\n",
    "    model_stradaboost_compact.fit(compact_np_train_X, compact_np_train_y_list)\n",
    "    y_pred_stradaboost_compact = model_stradaboost_compact.predict(compact_np_test_X)\n",
    "    \n",
    "    mse_stradaboost_compact = sqrt(mean_squared_error(compact_np_test_y, y_pred_stradaboost_compact))\n",
    "    rmselist_stradaboost_compact.append(mse_stradaboost_compact)\n",
    "        \n",
    "    r2_score_stradaboost_compact = pearsonr(compact_np_test_y_list, y_pred_stradaboost_compact)\n",
    "    r2_score_stradaboost_compact = (r2_score_stradaboost_compact[0])**2\n",
    "    r2scorelist_stradaboost_compact.append(r2_score_stradaboost_compact)\n",
    "        \n",
    "\n",
    "print(\"RMSE List of STrAdaboost.R2:\", rmselist_stradaboost_compact)\n",
    "print(\"R^2 List of STrAdaboost.R2:\", r2scorelist_stradaboost_compact)\n",
    "\n",
    "print(\"\\n\")\n",
    "        \n",
    "print(\"Mean, STDev of RMSE:\", statistics.mean(rmselist_stradaboost_compact), statistics.stdev(rmselist_stradaboost_compact))\n",
    "print(\"Mean, STDev of R^2:\", statistics.mean(r2scorelist_stradaboost_compact), statistics.stdev(r2scorelist_stradaboost_compact))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE List of TrAdaboost.R2: [2.850245727000064, 2.7005677832721795, 2.423316286110016, 2.3887951407597274, 2.4424424368999618, 2.712540816344094, 2.6922048694063783, 2.4355915511440815, 2.273891051582728, 2.455530672645627, 2.791109298242016, 3.0911190772016943, 2.452726097045739, 2.4119787667080628, 2.7179839042457594, 2.7054580881290557, 2.6361848259425584, 2.715932861277004, 2.646284569087001, 2.320419939336392]\n",
      "R^2 List of TrAdaboost.R2: [0.9558602992700392, 0.931005062205565, 0.9444198699483178, 0.9767082601341441, 0.9821184016071886, 0.9787744231494595, 0.8357842797316652, 0.9680639432406996, 0.9724770595594833, 0.9748248196263483, 0.8432705081475351, 0.9211716882582086, 0.9661358344788633, 0.9474209043007024, 0.7855117137776909, 0.9814564031631141, 0.9710649039181786, 0.8450924978885074, 0.9341749920943405, 0.9798138826886302]\n",
      "\n",
      "\n",
      "Mean, STDev of RMSE: 2.593216188119007 0.20554703753186954\n",
      "Mean, STDev of R^2: 0.9347574873594341 0.058924264017501\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "########################### TwoStagetrAdaBoostR2 compAct #######################################\n",
    "\n",
    "############# Transfer Learning specifications #########################\n",
    "\n",
    "def get_estimator(**kwargs):\n",
    "    return DecisionTreeRegressor(max_depth = 6)\n",
    "\n",
    "kwargs_TwoTrAda = {'steps': 30,\n",
    "                    'fold': 10,\n",
    "                  'learning_rate': 0.1}\n",
    "\n",
    "#####################################################################################################################\n",
    "\n",
    "from adapt.instance_based import TrAdaBoost, TrAdaBoostR2, TwoStageTrAdaBoostR2\n",
    "\n",
    "print(\"Two-Stage TrAdaboost.R2\")\n",
    "print(\"-------------------------------------------\")\n",
    "\n",
    "r2scorelist_TwoTrAda_compact = []\n",
    "rmselist_TwoTrAda_compact = []\n",
    "\n",
    "kf = KFold(n_splits = 20)\n",
    "\n",
    "for train_idx, test_idx in kf.split(compact_tgt_df):\n",
    "        \n",
    "    compact_train_df_X = compact_tgt_df.iloc[train_idx].loc[:, features_compact]\n",
    "    compact_test_df_X = compact_tgt_df.iloc[test_idx][features_compact]\n",
    "    compact_train_df_y = compact_tgt_df.iloc[train_idx].loc[:,target_compact]\n",
    "    compact_test_df_y = compact_tgt_df.loc[test_idx][target_compact] \n",
    "    \n",
    "    compact_X_df = pd.concat([compact_source_df_X, compact_train_df_X], ignore_index=True)\n",
    "    compact_y_df = pd.concat([compact_source_df_y, compact_train_df_y], ignore_index=True)\n",
    "\n",
    "    compact_np_train_X = compact_X_df.to_numpy()\n",
    "    compact_np_train_y = compact_y_df.to_numpy()\n",
    "\n",
    "    compact_np_test_X = compact_test_df_X.to_numpy()\n",
    "    compact_np_test_y = compact_test_df_y.to_numpy()\n",
    "\n",
    "    compact_np_train_y_list = compact_np_train_y.ravel()\n",
    "    compact_np_test_y_list = compact_np_test_y.ravel()\n",
    "\n",
    "    src_size_compact = len(compact_source_df_y)\n",
    "    tgt_size_compact = len(compact_train_df_y)\n",
    "    \n",
    "    src_idx_compact = np.arange(start=0, stop=(src_size_compact - 1), step=1)\n",
    "    tgt_idx_compact = np.arange(start=src_size_compact, stop=((src_size_compact + tgt_size_compact)-1), step=1)\n",
    "\n",
    "    model_TwoTrAda_compact = TwoStageTrAdaBoostR2(get_estimator = get_estimator, n_estimators = 100)\n",
    "    model_TwoTrAda_compact.fit(compact_np_train_X, compact_np_train_y_list, src_idx_compact, tgt_idx_compact)\n",
    "\n",
    "    y_pred_TwoTrAda_compact = model_TwoTrAda_compact.predict(compact_np_test_X)\n",
    "\n",
    "    mse_TwoTrAda_compact = sqrt(mean_squared_error(compact_np_test_y, y_pred_TwoTrAda_compact))\n",
    "\n",
    "    rmselist_TwoTrAda_compact.append(mse_TwoTrAda_compact)\n",
    "        \n",
    "    r2_score_TwoTrAda_compact = pearsonr(compact_np_test_y_list, y_pred_TwoTrAda_compact)\n",
    "    r2_score_TwoTrAda_compact = (r2_score_TwoTrAda_compact[0])**2\n",
    "\n",
    "    r2scorelist_TwoTrAda_compact.append(r2_score_TwoTrAda_compact)\n",
    "\n",
    "\n",
    "print(\"RMSE List of TrAdaboost.R2:\", rmselist_TwoTrAda_compact)\n",
    "print(\"R^2 List of TrAdaboost.R2:\", r2scorelist_TwoTrAda_compact)\n",
    "\n",
    "print(\"\\n\")\n",
    "        \n",
    "print(\"Mean, STDev of RMSE:\", statistics.mean(rmselist_TwoTrAda_compact), statistics.stdev(rmselist_TwoTrAda_compact))\n",
    "print(\"Mean, STDev of R^2:\", statistics.mean(r2scorelist_TwoTrAda_compact), statistics.stdev(r2scorelist_TwoTrAda_compact))\n",
    "\n",
    "\n",
    "print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaboost.R2 Transfer Learning\n",
      "-------------------------------------------\n",
      "RMSE List of Adaboost.R2: [2.7497607287843904, 2.5208526259990593, 2.4948268134683627, 2.577512548409647, 2.32702382173679, 2.3216455841878125, 2.4181591995786937, 2.334494866576656, 2.3519531779585354, 2.60371142986263, 2.631725106318242, 2.8575501136246153, 2.54642011764177, 2.305717861419587, 2.722873833819519, 2.6456650763593865, 2.6419954948055775, 2.7446544956999404, 2.7618707632104353, 2.6573848223593988]\n",
      "R^2 List of Adaboost.R2: [0.9628507856331731, 0.9476199758914179, 0.9444863522653001, 0.9739300893739561, 0.9862879199834731, 0.9853015927797946, 0.8881935735983749, 0.9726899441917263, 0.9752572746248802, 0.9735256726643179, 0.8733316440291593, 0.9347874453456327, 0.9669435715720194, 0.9524811342596715, 0.8268690783935556, 0.9833335382643411, 0.9725179154073612, 0.8344019712614956, 0.9428754162773287, 0.9786248389406393]\n",
      "\n",
      "\n",
      "Mean, STDev of RMSE: 2.5607899240910523 0.17056972694345388\n",
      "Mean, STDev of R^2: 0.943815486737881 0.04894320718774135\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "########################### AdaBoostR2 Transfer Learning compAct #####################################################\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "#earlystopping = EarlyStopping(monitor=\"mean_squared_error\", patience=40, verbose=1, mode='auto')\n",
    "#epochs = 100, batch_size = 5, callbacks=[earlystopping])\n",
    "\n",
    "print(\"Adaboost.R2 Transfer Learning\")\n",
    "print(\"-------------------------------------------\")\n",
    "\n",
    "r2scorelist_AdaTL_compact = []\n",
    "rmselist_AdaTL_compact = []\n",
    "\n",
    "kf = KFold(n_splits = 20)\n",
    "\n",
    "for train_idx, test_idx in kf.split(compact_tgt_df):\n",
    "        \n",
    "    compact_train_df_X = compact_tgt_df.iloc[train_idx].loc[:, features_compact]\n",
    "    compact_test_df_X = compact_tgt_df.iloc[test_idx][features_compact]\n",
    "    compact_train_df_y = compact_tgt_df.iloc[train_idx].loc[:,target_compact]\n",
    "    compact_test_df_y = compact_tgt_df.loc[test_idx][target_compact] \n",
    "    \n",
    "    compact_X_df = pd.concat([compact_source_df_X, compact_train_df_X], ignore_index=True)\n",
    "    compact_y_df = pd.concat([compact_source_df_y, compact_train_df_y], ignore_index=True)\n",
    "\n",
    "    compact_np_train_X = compact_X_df.to_numpy()\n",
    "    compact_np_train_y = compact_y_df.to_numpy()\n",
    "\n",
    "    compact_np_test_X = compact_test_df_X.to_numpy()\n",
    "    compact_np_test_y = compact_test_df_y.to_numpy()\n",
    "\n",
    "    compact_np_train_y_list = compact_np_train_y.ravel()\n",
    "    compact_np_test_y_list = compact_np_test_y.ravel()\n",
    "\n",
    "    model_AdaTL_compact = AdaBoostRegressor(DecisionTreeRegressor(max_depth = 6), learning_rate = 0.1, n_estimators = 100) \n",
    "    model_AdaTL_compact.fit(compact_np_train_X, compact_np_train_y_list) \n",
    "\n",
    "    y_pred_AdaTL_compact = model_AdaTL_compact.predict(compact_np_test_X)\n",
    "\n",
    "    mse_AdaTL_compact = sqrt(mean_squared_error(compact_np_test_y, y_pred_AdaTL_compact))\n",
    "    rmselist_AdaTL_compact.append(mse_AdaTL_compact)\n",
    "        \n",
    "    r2_score_AdaTL_compact = pearsonr(compact_np_test_y_list, y_pred_AdaTL_compact)\n",
    "    r2_score_AdaTL_compact = (r2_score_AdaTL_compact[0])**2\n",
    "    r2scorelist_AdaTL_compact.append(r2_score_AdaTL_compact)\n",
    "\n",
    "\n",
    "print(\"RMSE List of Adaboost.R2:\", rmselist_AdaTL_compact)\n",
    "print(\"R^2 List of Adaboost.R2:\", r2scorelist_AdaTL_compact)\n",
    "\n",
    "print(\"\\n\")\n",
    "        \n",
    "print(\"Mean, STDev of RMSE:\", statistics.mean(rmselist_AdaTL_compact), statistics.stdev(rmselist_AdaTL_compact))\n",
    "print(\"Mean, STDev of R^2:\", statistics.mean(r2scorelist_AdaTL_compact), statistics.stdev(r2scorelist_AdaTL_compact))\n",
    "\n",
    "print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### Gradient Boosting Regression Transfer Learning compAct #######################################\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "\n",
    "print(\"Gradient Boosting Regression Transfer Learning\")\n",
    "print(\"-------------------------------------------\")\n",
    "\n",
    "r2scorelist_GBRTL_compact = []\n",
    "rmselist_GBRTL_compact = []\n",
    "\n",
    "kf = KFold(n_splits = 20)\n",
    "\n",
    "for train_idx, test_idx in kf.split(compact_tgt_df):\n",
    "        \n",
    "    compact_train_df_X = compact_tgt_df.iloc[train_idx].loc[:, features_compact]\n",
    "    compact_test_df_X = compact_tgt_df.iloc[test_idx][features_compact]\n",
    "    compact_train_df_y = compact_tgt_df.iloc[train_idx].loc[:,target_compact]\n",
    "    compact_test_df_y = compact_tgt_df.loc[test_idx][target_compact] \n",
    "    \n",
    "    compact_X_df = pd.concat([compact_source_df_X, compact_train_df_X], ignore_index=True)\n",
    "    compact_y_df = pd.concat([compact_source_df_y, compact_train_df_y], ignore_index=True)\n",
    "\n",
    "    compact_np_train_X = compact_X_df.to_numpy()\n",
    "    compact_np_train_y = compact_y_df.to_numpy()\n",
    "\n",
    "    compact_np_test_X = compact_test_df_X.to_numpy()\n",
    "    compact_np_test_y = compact_test_df_y.to_numpy()\n",
    "\n",
    "    compact_np_train_y_list = compact_np_train_y.ravel()\n",
    "    compact_np_test_y_list = compact_np_test_y.ravel()\n",
    "\n",
    "\n",
    "    model_GBRTL_compact = GradientBoostingRegressor(learning_rate = 0.1, max_depth = 6, n_estimators = 100) #, subsample=0.5)\n",
    "    model_GBRTL_compact.fit(compact_np_train_X, compact_np_train_y_list)\n",
    "\n",
    "    y_pred_GBRTL_compact = model_GBRTL_compact.predict(compact_test_df_X) ##Using dataframe instead of the numpy matrix\n",
    "\n",
    "    mse_GBRTL_compact = sqrt(mean_squared_error(compact_np_test_y, y_pred_GBRTL_compact))\n",
    "    rmselist_GBRTL_compact.append(mse_GBRTL_compact)\n",
    "        \n",
    "    r2_score_GBRTL_compact = pearsonr(compact_np_test_y_list, y_pred_GBRTL_compact)\n",
    "    r2_score_GBRTL_compact = (r2_score_GBRTL_compact[0])**2\n",
    "    r2scorelist_GBRTL_compact.append(r2_score_GBRTL_compact)\n",
    "\n",
    "\n",
    "print(\"RMSE List of GBRTL:\", rmselist_GBRTL_compact)\n",
    "print(\"R^2 List of GBRTL:\", r2scorelist_GBRTL_compact)\n",
    "\n",
    "print(\"\\n\")\n",
    "        \n",
    "print(\"Mean, STDev of RMSE:\", statistics.mean(rmselist_GBRTL_compact), statistics.stdev(rmselist_GBRTL_compact))\n",
    "print(\"Mean, STDev of R^2:\", statistics.mean(r2scorelist_GBRTL_compact), statistics.stdev(r2scorelist_GBRTL_compact))\n",
    "\n",
    "print(\"-------------------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
