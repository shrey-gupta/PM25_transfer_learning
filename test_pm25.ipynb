{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repositories uploaded!!\n"
     ]
    }
   ],
   "source": [
    "# from two_TrAdaBoostR2 import TwoStageTrAdaBoostR2 ##STrAdaBoost.R2\n",
    "# from TwoStageTrAdaBoostR2 import TwoStageTrAdaBoostR2 ##two-stage TrAdaBoost.R2\n",
    "\n",
    "import pandas as pd\n",
    "import sys\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from keras.layers import Input, Dense, Activation, Conv2D, Dropout, Flatten\n",
    "from keras import optimizers, utils, initializers, regularizers\n",
    "import keras.backend as K\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import StandardScaler #Importing the StandardScaler\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.stats.stats import pearsonr\n",
    "from math import sqrt\n",
    "\n",
    "#Geo plotting libraries\n",
    "import geopandas as gdp\n",
    "from matplotlib.colors import ListedColormap\n",
    "# import geoplot as glpt\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn import linear_model\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.lines as mlines\n",
    "\n",
    "import statistics\n",
    "\n",
    "from scipy.stats import rv_continuous\n",
    "from scipy.stats import *\n",
    "\n",
    "from statistics import mean\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial import distance\n",
    "\n",
    "\n",
    "######### Instance Transfer repositories ####################\n",
    "from adapt.instance_based import TwoStageTrAdaBoostR2\n",
    "\n",
    "print(\"Repositories uploaded!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second Upload Completed!!\n"
     ]
    }
   ],
   "source": [
    "from adapt.instance_based import TrAdaBoost, TrAdaBoostR2, TwoStageTrAdaBoostR2\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from adapt.instance_based import KMM\n",
    "\n",
    "print(\"Second Upload Completed!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Importing libraries for map plots #######\n",
    "\n",
    "import geopandas as gpd\n",
    "import json\n",
    "import altair as alt\n",
    "import pandas as pd\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import glob\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib import cm\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Reading the US 2018 dataset\n",
    "US_new_source = pd.read_csv('US_data/new_US_dataset/source_test.csv')\n",
    "US_new_target = pd.read_csv('US_data/new_US_dataset/target_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'Date', 'PM25', 'Region', 'CF_CLDTT', 'CF_PS', 'CF_Q', 'CF_Q10M',\n",
       "       'CF_Q2M', 'CF_RH', 'CF_SLP', 'CF_T', 'CF_T10M', 'CF_T2M', 'CF_TPREC',\n",
       "       'CF_TROPPB', 'CF_TS', 'CF_U', 'CF_U10M', 'CF_U2M', 'CF_V', 'CF_V10M',\n",
       "       'CF_V2M', 'CF_ZL', 'CF_ZPBL', 'CF_PM25', 'CF_CO', 'CF_NO2', 'CF_O3',\n",
       "       'CF_SO2', 'LC_Shrubs', 'LC_Herbaceous', 'LC_Agriculture', 'LC_Urban',\n",
       "       'LC_Bare', 'LC_Snow', 'LC_Water', 'LC_Wetland', 'LC_Lichen',\n",
       "       'LC_Closed_Forest', 'LC_Open_Forest', 'LC_Ocean', 'Elevation',\n",
       "       'Dist_Primary', 'Dist_Secondary', 'Pop', 'Lon', 'Lat'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "US_new_target.shape, US_new_source.shape \n",
    "US_new_target.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CF_CLDTT', 'CF_PS', 'CF_Q', 'CF_Q10M', 'CF_Q2M', 'CF_RH', 'CF_SLP',\n",
       "       'CF_T', 'CF_T10M', 'CF_T2M', 'CF_TPREC', 'CF_TROPPB', 'CF_TS', 'CF_U',\n",
       "       'CF_U10M', 'CF_U2M', 'CF_V', 'CF_V10M', 'CF_V2M', 'CF_ZL', 'CF_ZPBL',\n",
       "       'CF_PM25', 'CF_CO', 'CF_NO2', 'CF_O3', 'CF_SO2', 'LC_Shrubs',\n",
       "       'LC_Herbaceous', 'LC_Agriculture', 'LC_Urban', 'LC_Bare', 'LC_Snow',\n",
       "       'LC_Water', 'LC_Wetland', 'LC_Lichen', 'LC_Closed_Forest',\n",
       "       'LC_Open_Forest', 'LC_Ocean', 'Elevation', 'Dist_Primary',\n",
       "       'Dist_Secondary', 'Pop'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######## Split into features and output ########\n",
    "output_US = ['PM25']\n",
    "\n",
    "US_y_source_df = US_new_source['PM25']\n",
    "US_X_source_df = US_new_source.drop(output_US, axis = 1)\n",
    "\n",
    "US_y_target_df = US_new_target['PM25']\n",
    "US_X_target_df = US_new_target.drop(output_US, axis = 1)\n",
    "\n",
    "\n",
    "######## Drop the unwanted features ########\n",
    "drop_features = ['ID', 'Date', 'Region', 'Lon', 'Lat']\n",
    "\n",
    "US_X_source_df = US_X_source_df.drop(drop_features, axis = 1)\n",
    "US_X_target_df = US_X_target_df.drop(drop_features, axis = 1)\n",
    "\n",
    "US_X_source_df.shape, US_X_target_df.shape\n",
    "US_X_source_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Normalize the dataset ########\n",
    "from sklearn import preprocessing\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "X_target = US_X_target_df.values \n",
    "X_target_scaled = min_max_scaler.fit_transform(X_target)\n",
    "US_X_target_df = pd.DataFrame(X_target_scaled)\n",
    "\n",
    "X_source = US_X_source_df.values \n",
    "X_source_scaled = min_max_scaler.fit_transform(X_source)\n",
    "US_X_source_df = pd.DataFrame(X_source_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------\n"
     ]
    }
   ],
   "source": [
    "######## Split into Train-Test ########\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "US_X_target_df, US_X_test_df, US_y_target_df, US_y_test_df = train_test_split(US_X_target_df, US_y_target_df, test_size = 0.30)\n",
    "\n",
    "######## Merging the datasets ########\n",
    "US_X_df = pd.concat([US_X_target_df, US_X_source_df], ignore_index=True)\n",
    "US_y_df = pd.concat([US_y_target_df, US_y_source_df], ignore_index=True)\n",
    "\n",
    "US_np_train_X = US_X_df.to_numpy()\n",
    "US_np_train_y = US_y_df.to_numpy()\n",
    "\n",
    "US_np_test_X = US_X_test_df.to_numpy()\n",
    "US_np_test_y = US_y_test_df.to_numpy()\n",
    "\n",
    "US_np_train_y_list = US_np_train_y.ravel()\n",
    "US_np_test_y_list = US_np_test_y.ravel()\n",
    "\n",
    "src_size_US = len(US_y_source_df)\n",
    "tgt_size_US = len(US_y_target_df)\n",
    "\n",
    "print(\"---------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOTRBoost.R2 AS\n",
      "-------------------------------------------\n",
      "Inside STrAdaBoost.R2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-9271f5d99c06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m                           random_state = random_state)\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mmodel_lotrboost_us\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mUS_np_train_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUS_np_train_y_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0my_pred_us\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_lotrboost_us\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mUS_np_test_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/PM25/PM_25_TL/two_TrAdaBoostR2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    257\u001b[0m             \u001b[0;31m## The sample weights were calculate above. They are 1/n+m. Where n: no. of source instances, m: no. of target instances.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m             \u001b[0;31m## Fitting the model means making it learn. This basically means for 'Boosting', to provide weights to the instances.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m             \u001b[0;31m## Add this models to the list of models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_weight_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m         \u001b[0;31m# Fit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1007\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate_estimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_weight_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0miboost\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0;31m# Boosting step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             sample_weight, estimator_weight, estimator_error = self._boost(\n\u001b[0m\u001b[1;32m    131\u001b[0m                 \u001b[0miboost\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_weight_boosting.py\u001b[0m in \u001b[0;36m_boost\u001b[0;34m(self, iboost, X, y, sample_weight, random_state)\u001b[0m\n\u001b[1;32m   1066\u001b[0m         \u001b[0mX_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_safe_indexing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbootstrap_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m         \u001b[0my_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_safe_indexing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbootstrap_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1068\u001b[0;31m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1069\u001b[0m         \u001b[0my_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m   1240\u001b[0m         \"\"\"\n\u001b[1;32m   1241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1242\u001b[0;31m         super().fit(\n\u001b[0m\u001b[1;32m   1243\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1244\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    373\u001b[0m                                            min_impurity_split)\n\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 375\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "####################### LOTRBoost.R2 #########################################################\n",
    "from two_TrAdaBoostR2 import TwoStageTrAdaBoostR2\n",
    "\n",
    "sample_size = [len(US_X_target_df), len(US_X_source_df)]\n",
    "# sample_size = [153, 900]\n",
    "\n",
    "n_estimators = 50\n",
    "steps = 20\n",
    "fold = 10\n",
    "random_state = np.random.RandomState(1)\n",
    "\n",
    "print(\"LOTRBoost.R2 AS\")\n",
    "print(\"-------------------------------------------\")\n",
    "\n",
    "r2scorelist_stradaboost_us = []\n",
    "rmselist_stradaboost_us = []\n",
    "\n",
    "\n",
    "for x in range(0, 10):\n",
    "\n",
    "    model_lotrboost_us = TwoStageTrAdaBoostR2(DecisionTreeRegressor(max_depth=6),\n",
    "                          n_estimators = n_estimators, sample_size = sample_size,\n",
    "                          steps = steps, fold = fold,\n",
    "                          random_state = random_state)\n",
    "\n",
    "    model_lotrboost_us.fit(US_np_train_X, US_np_train_y_list)\n",
    "    y_pred_us = model_lotrboost_us.predict(US_np_test_X) \n",
    "\n",
    "    rmse_lotrboost_us = sqrt(mean_squared_error(US_np_test_y_list, y_pred_lotrboost_US))\n",
    "    rmselist_lotrboost_us.append(rmse_lotrboost_US)\n",
    "    print(\"RMSE:\", rmse_lotrboost_us)\n",
    "\n",
    "    r2_score_lotrboost_us = pearsonr(US_np_test_y_list, y_pred_lotrboost_US)\n",
    "    r2_score_lotrboost_us = (r2_score_lotrboost_us[0])**2\n",
    "    r2scorelist_lotrboost_us.append(r2_score_lotrboost_us)\n",
    "    print(\"R-squared Score:\", r2_score_lotrboost_us)\n",
    "    \n",
    "    \n",
    "print(\"Mean RMSE of LOTRBoost:\", statistics.mean(rmselist_lotrboost_us))\n",
    "print(\"Mean R-squared of LOTRBoost:\", statistics.mean(r2scorelist_lotrboost_us))\n",
    "print(\"\\n\")\n",
    "print(\"RMSE of LOTRBoost:\", rmselist_lotrboost_us)\n",
    "print(\"R-squared of LOTRBoost:\", r2scorelist_lotrboost_us)\n",
    "\n",
    "\n",
    "print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
