{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done uploading repositories\n",
      "Second Upload Completed!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shrey/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py:4166: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().replace(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 10) (50000, 10)\n",
      "Target:  (400, 9)\n",
      "Test:  (49600, 9)\n",
      "Source:  (2500, 9)\n",
      "Adaboost.R2 Transfer Learning (M + H, L)\n",
      "-------------------------------------------\n",
      "STrAdaBoost\n",
      "Inside STrAdaBoost.R2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shrey/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_split.py:293: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STrAdaBoost\n",
      "Inside STrAdaBoost.R2\n",
      "STrAdaBoost\n",
      "Inside STrAdaBoost.R2\n",
      "STrAdaBoost\n",
      "Inside STrAdaBoost.R2\n",
      "STrAdaBoost\n",
      "Inside STrAdaBoost.R2\n",
      "STrAdaBoost\n",
      "Inside STrAdaBoost.R2\n",
      "STrAdaBoost\n",
      "Inside STrAdaBoost.R2\n",
      "STrAdaBoost\n",
      "Inside STrAdaBoost.R2\n",
      "STrAdaBoost\n",
      "Inside STrAdaBoost.R2\n",
      "STrAdaBoost\n",
      "Inside STrAdaBoost.R2\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "######################## Header files ################################################\n",
    "# from two_TrAdaBoostR2 import TwoStageTrAdaBoostR2 ##For STrAdaBoost.R2\n",
    "# from TwoStageTrAdaBoostR2 import TwoStageTrAdaBoostR2 ## For two-stage TrAdaBoost.R2\n",
    "\n",
    "import pandas as pd\n",
    "import sys\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from keras.layers import Input, Dense, Activation, Conv2D, Dropout, Flatten\n",
    "from keras import optimizers, utils, initializers, regularizers\n",
    "import keras.backend as K\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import StandardScaler #Importing the StandardScaler\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.stats.stats import pearsonr\n",
    "from math import sqrt\n",
    "\n",
    "#Geo plotting libraries\n",
    "#import geopandas as gdp\n",
    "#from matplotlib.colors import ListedColormap\n",
    "#import geoplot as glpt\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn import linear_model\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.lines as mlines\n",
    "import folium\n",
    "import glob\n",
    "\n",
    "import statistics\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial import distance\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "from adapt.instance_based import (TrAdaBoost, TrAdaBoostR2, TwoStageTrAdaBoostR2)\n",
    "\n",
    "\n",
    "print(\"Done uploading repositories\")\n",
    "\n",
    "from adapt.instance_based import TrAdaBoost, TrAdaBoostR2, TwoStageTrAdaBoostR2\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from adapt.instance_based import KMM\n",
    "\n",
    "print(\"Second Upload Completed!!\")\n",
    "\n",
    "################# UCI Beijing dataset (2013 - 2017) #####################################################################\n",
    "############### Spacio-temporal dataset. (multi-year and multi-terrain) #################################################\n",
    "\n",
    "# beijing_aqi_df = pd.read_csv('AQI_datasets/Beijing_AQI/PRSA_Data_20130301-20170228/PRSA_Data_Aotizhongxin_20130301-20170228.csv')\n",
    "# drop_index = ['No']\n",
    "# beijing_aqi_df = beijing_aqi_df.drop(drop_index, axis=1)\n",
    "# #print(beijing_aqi_df.head())\n",
    "# print(beijing_aqi_df.isnull().sum())\n",
    "\n",
    "#AQI_datasets/Beijing_AQI/PRSA_Data_20130301-20170228\n",
    "path_beijing = r'AQI_datasets/Beijing_AQI/PRSA_Data_20130301-20170228/' ## Path for all the files\n",
    "allFiles = glob.glob(path_beijing + \"/*.csv\")\n",
    "beijing_aqi_df = pd.DataFrame()\n",
    "list_beijing = []\n",
    "\n",
    "for file_ in allFiles:\n",
    "    temp_df = pd.read_csv(file_, index_col = None, header=0)\n",
    "    list_beijing.append(temp_df)\n",
    "beijing_aqi_df = pd.concat(list_beijing)\n",
    "\n",
    "cols = ['No', 'year', 'month', 'day', 'hour', 'PM2.5', 'PM10', 'SO2', 'NO2', 'CO', 'O3', 'TEMP',\n",
    "       'PRES', 'DEWP', 'RAIN', 'wd', 'WSPM', 'station']\n",
    "\n",
    "beijing_aqi_df = beijing_aqi_df[cols]\n",
    "\n",
    "drop_index = ['No']\n",
    "beijing_aqi_df = beijing_aqi_df.drop(drop_index, axis=1)\n",
    "\n",
    "beijing_aqi_df = beijing_aqi_df.sort_values(['station', 'year'])\n",
    "\n",
    "######################### Seperate the dataset into predictors and target variable. ############\n",
    "\n",
    "predictors = ['year', 'month', 'SO2', 'NO2', 'CO', 'TEMP', 'PRES', 'DEWP', 'RAIN', 'wd', 'WSPM', 'station', 'O3']\n",
    "beijing_df = beijing_aqi_df[predictors]\n",
    "wd_codes = {'N':1, 'E': 2, 'W': 3, 'S': 4, 'NE': 5, 'NW': 6, 'SE': 7, 'SW': 8, 'NNE': 9, 'NNW': 10, 'SSE': 11,\n",
    "            'SSW': 12, 'WNW': 13, 'WSW': 14, 'ENE': 15, 'ESE': 16 }\n",
    "\n",
    "\n",
    "beijing_df.replace(wd_codes, inplace=True)\n",
    "beijing_df.wd.value_counts()\n",
    "beijing_df = beijing_df.dropna()\n",
    "\n",
    "######################### Drop stations ########################################\n",
    "drop_stations = ['station']\n",
    "beijing_df = beijing_df.drop(drop_stations, axis=1)\n",
    "\n",
    "################ Splitting the dataset by the year #############################\n",
    "beijing_df.year.value_counts()\n",
    "beijing_df_target = beijing_df[beijing_df['year'].isin([2014, 2015])]\n",
    "beijing_df_source = beijing_df[beijing_df['year'].isin([2016])]\n",
    "\n",
    "drop_cols = ['year', 'month']\n",
    "beijing_df_target = beijing_df_target.drop(drop_cols, axis = 1)\n",
    "beijing_df_source = beijing_df_source.drop(drop_cols, axis = 1)\n",
    "\n",
    "beijing_df_target = beijing_df_target.reset_index(drop=True)\n",
    "beijing_df_source = beijing_df_source.reset_index(drop=True)\n",
    "\n",
    "#### Select first 10k source and first 50k train instances\n",
    "beijing_df_source = beijing_df_source.iloc[:10000]\n",
    "beijing_df_target = beijing_df_target.iloc[:50000]\n",
    "\n",
    "print(beijing_df_source.shape, beijing_df_target.shape)\n",
    "beijing_df_target\n",
    "\n",
    "########################## Standardize the dataset. ############################\n",
    "\n",
    "cols_to_norm = ['SO2', 'NO2', 'CO', 'TEMP', 'PRES', 'DEWP', 'RAIN', 'WSPM']\n",
    "\n",
    "ss = StandardScaler()\n",
    "# beijing_df_target[cols_to_norm] = ss.fit_transform(beijing_df_target[cols_to_norm])\n",
    "# beijing_df_source[cols_to_norm] = ss.fit_transform(beijing_df_source[cols_to_norm])\n",
    "\n",
    "##################### Splitting the dataset into train and test set ################\n",
    "\n",
    "target_beijing_col = ['O3']\n",
    "beijingAQ_train_df_y = beijing_df_target[target_beijing_col]\n",
    "beijingAQ_train_df_X = beijing_df_target.drop(target_beijing_col, axis =1)\n",
    "\n",
    "\n",
    "beijingAQ_source_df_y = beijing_df_source[target_beijing_col]\n",
    "beijingAQ_source_df_X = beijing_df_source.drop(target_beijing_col, axis =1)\n",
    "\n",
    "\n",
    "################## Split into target and test dataset ###################\n",
    "def TimeSeriesTrainTestSplit(X, y, test_size):\n",
    "\n",
    "        test_index = int(len(X)*(1-test_size))\n",
    "\n",
    "        X_train = X.iloc[:test_index]\n",
    "        y_train = y.iloc[:test_index]\n",
    "        X_test = X.iloc[test_index:]\n",
    "        y_test = y.iloc[test_index:]\n",
    "        return X_train, y_train, X_test, y_test\n",
    "\n",
    "beijingAQ_test_df_X, beijingAQ_test_df_y, beijingAQ_tgt_df_X, beijingAQ_tgt_df_y = TimeSeriesTrainTestSplit(beijingAQ_train_df_X, beijingAQ_train_df_y, 0.008)\n",
    "\n",
    "# print(beijingAQ_tgt_df_X.shape)\n",
    "# print(beijingAQ_test_df_X.shape)\n",
    "\n",
    "beijingAQ_tgt_df = pd.concat([beijingAQ_tgt_df_X, beijingAQ_tgt_df_y], axis=1, sort= False)\n",
    "beijingAQ_tgt_df = beijingAQ_tgt_df.reset_index(drop=True)\n",
    "\n",
    "beijingAQ_test_df = pd.concat([beijingAQ_test_df_X, beijingAQ_test_df_y], axis=1, sort= False)\n",
    "beijingAQ_test_df = beijingAQ_test_df.reset_index(drop=True)\n",
    "\n",
    "beijingAQ_source_df = pd.concat([beijingAQ_source_df_X, beijingAQ_source_df_y], axis=1, sort= False)\n",
    "beijingAQ_source_df = beijingAQ_source_df.reset_index(drop=True)\n",
    "\n",
    "beijingAQ_source_df.shape\n",
    "\n",
    "\n",
    "################################## Importance Sampling ######################################################\n",
    "beijingAQ_source_df[\"ManDis\"] = \"\"\n",
    "\n",
    "beijingAQ_tgt_df_mean = []\n",
    "beijingAQ_tgt_df_mean = beijingAQ_tgt_df.mean().tolist()\n",
    "\n",
    "rowidx = 0\n",
    "\n",
    "for row in beijingAQ_source_df.itertuples():\n",
    "    row_list =[row.SO2, row.NO2, row.CO, row.TEMP, row.PRES, row.DEWP, row.RAIN, row.wd, row.WSPM, row.O3]\n",
    "    \n",
    "    man_dis = 0\n",
    "    for i in range(0, len(row_list)):\n",
    "        tempval = beijingAQ_tgt_df_mean[i] - row_list[i]\n",
    "        man_dis = man_dis + abs(tempval)\n",
    "    \n",
    "#     print(\"Mandis Value:\", man_dis)\n",
    "    beijingAQ_source_df.loc[rowidx,\"ManDis\"] = man_dis\n",
    "    rowidx = rowidx + 1\n",
    "\n",
    "beijingAQ_source_df = beijingAQ_source_df.sort_values(by =['ManDis'])\n",
    "beijingAQ_source_df = beijingAQ_source_df.head(2500) \n",
    "beijingAQ_source_df = beijingAQ_source_df.drop(['ManDis'], axis =1)\n",
    "beijingAQ_source_df = beijingAQ_source_df.reset_index(drop=True)\n",
    "\n",
    "############################ Split again into target and features ############################\n",
    "\n",
    "target_column_beijingAQ = ['O3']\n",
    "\n",
    "beijingAQ_tgt_df_y = beijingAQ_tgt_df[target_column_beijingAQ]\n",
    "beijingAQ_tgt_df_X = beijingAQ_tgt_df.drop(target_column_beijingAQ, axis = 1)\n",
    "\n",
    "beijingAQ_test_df_y = beijingAQ_test_df[target_column_beijingAQ]\n",
    "beijingAQ_test_df_X = beijingAQ_test_df.drop(target_column_beijingAQ, axis = 1)\n",
    "\n",
    "beijingAQ_source_df_y = beijingAQ_source_df[target_column_beijingAQ]\n",
    "beijingAQ_source_df_X = beijingAQ_source_df.drop(target_column_beijingAQ, axis = 1)\n",
    "\n",
    "\n",
    "columns_beijingAQ = beijingAQ_tgt_df_X.columns\n",
    "beijingAQ_tgt_df_X[columns_beijingAQ] = ss.fit_transform(beijingAQ_tgt_df_X[columns_beijingAQ])\n",
    "beijingAQ_test_df_X[columns_beijingAQ] = ss.fit_transform(beijingAQ_test_df_X[columns_beijingAQ])\n",
    "beijingAQ_source_df_X[columns_beijingAQ] = ss.fit_transform(beijingAQ_source_df_X[columns_beijingAQ])\n",
    "\n",
    "# beijingAQ_source_df_X = beijingAQ_source_df\n",
    "\n",
    "print(\"Target: \",beijingAQ_tgt_df_X.shape)\n",
    "print(\"Test: \",beijingAQ_test_df_X.shape)\n",
    "print(\"Source: \",beijingAQ_source_df_X.shape)\n",
    "\n",
    "\n",
    "############### Merging the datasets ##########################################\n",
    "beijingAQ_X_df = pd.concat([beijingAQ_tgt_df_X, beijingAQ_source_df_X], ignore_index=True)\n",
    "beijingAQ_y_df = pd.concat([beijingAQ_tgt_df_y, beijingAQ_source_df_y], ignore_index=True)\n",
    "\n",
    "beijingAQ_np_train_X = beijingAQ_X_df.to_numpy()\n",
    "beijingAQ_np_train_y = beijingAQ_y_df.to_numpy()\n",
    "\n",
    "beijingAQ_np_test_X = beijingAQ_test_df_X.to_numpy()\n",
    "beijingAQ_np_test_y = beijingAQ_test_df_y.to_numpy()\n",
    "\n",
    "beijingAQ_np_train_y_list = beijingAQ_np_train_y.ravel()\n",
    "beijingAQ_np_test_y_list = beijingAQ_np_test_y.ravel()\n",
    "\n",
    "src_size_beijingAQ = len(beijingAQ_source_df_y)\n",
    "tgt_size_beijingAQ = len(beijingAQ_tgt_df_y)\n",
    "\n",
    "src_idx = np.arange(start=0, stop=(src_size_beijingAQ - 1), step=1)\n",
    "tgt_idx = np.arange(start=src_size_beijingAQ, stop=((src_size_beijingAQ + tgt_size_beijingAQ) - 1), step=1)\n",
    "\n",
    "\n",
    "########################### Transfer Learning Italian AQ #####################################################\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "def get_estimator(**kwargs):\n",
    "    return DecisionTreeRegressor(max_depth = 6)\n",
    "\n",
    "kwargs_TwoTrAda = {'steps': 30,\n",
    "                    'fold': 10,\n",
    "                  'learning_rate': 0.1}\n",
    "\n",
    "\n",
    "\n",
    "print(\"Adaboost.R2 Transfer Learning (M + H, L)\")\n",
    "print(\"-------------------------------------------\")\n",
    "\n",
    "r2scorelist_AdaTL_beijingAQ = []\n",
    "rmselist_AdaTL_beijingAQ = []\n",
    "\n",
    "r2scorelist_Ada_beijingAQ = []\n",
    "rmselist_Ada_beijingAQ = []\n",
    "\n",
    "r2scorelist_KMM_beijingAQ = []\n",
    "rmselist_KMM_beijingAQ = []\n",
    "\n",
    "r2scorelist_GBRTL_beijingAQ = []\n",
    "rmselist_GBRTL_beijingAQ = []\n",
    "\n",
    "r2scorelist_GBR_beijingAQ = []\n",
    "rmselist_GBR_beijingAQ = []\n",
    "\n",
    "r2scorelist_TwoTrAda_beijingAQ = []\n",
    "rmselist_TwoTrAda_beijingAQ = []\n",
    "\n",
    "r2scorelist_stradaboost_beijingAQ = []\n",
    "rmselist_stradaboost_beijingAQ = []\n",
    "\n",
    "\n",
    "kfold = KFold(n_splits = 10, random_state = 42, shuffle=False)\n",
    "\n",
    "for x in range(0, 10):\n",
    "    ################### STrAdaBoost ###################\n",
    "    print(\"STrAdaBoost\")\n",
    "    from two_TrAdaBoostR2 import TwoStageTrAdaBoostR2\n",
    "\n",
    "    sample_size = [len(beijingAQ_tgt_df_X), len(beijingAQ_source_df_X)]\n",
    "    n_estimators = 100\n",
    "    steps = 30\n",
    "    fold = 10\n",
    "    random_state = np.random.RandomState(1)\n",
    "\n",
    "\n",
    "    model_stradaboost_beijingAQ = TwoStageTrAdaBoostR2(DecisionTreeRegressor(max_depth = 6),\n",
    "                        n_estimators = n_estimators, sample_size = sample_size,\n",
    "                        steps = steps, fold = fold, random_state = random_state)\n",
    "\n",
    "\n",
    "    model_stradaboost_beijingAQ.fit(beijingAQ_np_train_X, beijingAQ_np_train_y_list)\n",
    "    y_pred_stradaboost_beijingAQ = model_stradaboost_beijingAQ.predict(beijingAQ_np_test_X)\n",
    "\n",
    "\n",
    "    mse_stradaboost_beijingAQ = sqrt(mean_squared_error(beijingAQ_np_test_y, y_pred_stradaboost_beijingAQ))\n",
    "    rmselist_stradaboost_beijingAQ.append(mse_stradaboost_beijingAQ)\n",
    "\n",
    "    r2_score_stradaboost_beijingAQ = pearsonr(beijingAQ_np_test_y_list, y_pred_stradaboost_beijingAQ)\n",
    "    r2_score_stradaboost_beijingAQ = (r2_score_stradaboost_beijingAQ[0])**2\n",
    "    r2scorelist_stradaboost_beijingAQ.append(r2_score_stradaboost_beijingAQ)\n",
    "\n",
    "\n",
    "\n",
    "with open('beijingAQ_rmse_stradaboost.txt', 'w') as beijingAQ_handle_rmse:\n",
    "    beijingAQ_handle_rmse.write(\"\\n\\nSTrAdaBoost Active Sampling:\\n \")\n",
    "    beijingAQ_handle_rmse.writelines(\"%s\\n\" % ele for ele in rmselist_stradaboost_beijingAQ)\n",
    "\n",
    "\n",
    "with open('beijingAQ_r2_stradaboost.txt', 'w') as beijingAQ_handle_r2:\n",
    "    beijingAQ_handle_r2.write(\"\\n\\nSTrAdaBoost Active Sampling:\\n \")\n",
    "    beijingAQ_handle_r2.writelines(\"%s\\n\" % ele for ele in r2scorelist_stradaboost_beijingAQ)\n",
    "\n",
    "\n",
    "print(\"-------------------------------------------\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
